1
00:00:00,500 --> 00:00:03,500
Recording now.

2
00:00:08,500 --> 00:00:11,500
We're recording and there's this.

3
00:00:12,500 --> 00:00:14,500
Yeah, OK, so welcome to.

4
00:00:16,500 --> 00:00:18,500
Yeah, it feels like a turn of pictures, it's been two weeks,

5
00:00:18,500 --> 00:00:21,500
but somehow continuation broke.

6
00:00:22,500 --> 00:00:25,500
And welcome to the next lecture of deep learning for NLP.

7
00:00:25,500 --> 00:00:27,500
So this is my last lecture today.

8
00:00:27,500 --> 00:00:30,500
Next week, I'm going to hand over to Martin Tutek,

9
00:00:30,500 --> 00:00:32,500
who's going to take over the second part of the semester.

10
00:00:33,500 --> 00:00:36,500
And then the last lecture, one lecture, maybe,

11
00:00:36,500 --> 00:00:41,500
will be a guest talk on the topic of ethics in NLP by Thomas.

12
00:00:41,500 --> 00:00:44,500
So where were we last time, by the way?

13
00:00:44,500 --> 00:00:45,500
OK, what was it?

14
00:00:45,500 --> 00:00:47,500
Word embeddings training, right?

15
00:00:48,500 --> 00:00:51,500
Anyone remembers what we did last time?

16
00:00:51,500 --> 00:00:53,500
OK, word embeddings.

17
00:00:53,500 --> 00:00:55,500
So today we're going to be talking about record neural networks,

18
00:00:55,500 --> 00:00:59,500
which is super fancy name for things that are actually quite fancy.

19
00:00:59,500 --> 00:01:05,500
So the point is so far, so we are talking about the NLP,

20
00:01:05,500 --> 00:01:09,500
so multilayer perceptron and we squeeze some input in there.

21
00:01:09,500 --> 00:01:14,500
And for text with different lengths, we just said, let's use continuous back-off words, right?

22
00:01:14,500 --> 00:01:19,500
Cram everything into one bag and just use everything and don't care about de-ordering,

23
00:01:19,500 --> 00:01:19,500
don't care about it.

24
00:01:20,500 --> 00:01:24,500
But actually, yeah, so this is what we did so far.

25
00:01:25,500 --> 00:01:30,500
And yeah, so we, as I said, we took as a concatenation of the vectors.

26
00:01:30,500 --> 00:01:34,500
So, for example, previous three words when we were talking about language modeling.

27
00:01:34,500 --> 00:01:36,500
So predict the next word given the past three words.

28
00:01:36,500 --> 00:01:38,500
So we concatenated the word vectors.

29
00:01:38,500 --> 00:01:38,500
OK, good.

30
00:01:39,500 --> 00:01:41,500
We then, what did we do?

31
00:01:41,500 --> 00:01:46,500
We also used the averaging and we also used the Markov property, right?

32
00:01:46,500 --> 00:01:48,500
So we said, what is the Markov property again?

33
00:01:48,500 --> 00:01:54,500
Like previous two, three words maximum because of the explosion of the parameters

34
00:01:54,500 --> 00:01:59,500
or of the impracticality of learning the parameters.

35
00:02:00,500 --> 00:02:07,500
What we really want in natural language processing, what we really, really want is working with sequence of inputs.

36
00:02:08,500 --> 00:02:09,500
Right.

37
00:02:10,500 --> 00:02:18,500
And we want to, for example, so give me a task where we have a variable length input and fixed size output.

38
00:02:19,500 --> 00:02:21,500
What could be, Gesundheit, what could be the task?

39
00:02:22,500 --> 00:02:26,500
Like variable length inputs and fixed size output.

40
00:02:27,500 --> 00:02:29,500
Sentiment classification, for instance, right?

41
00:02:29,500 --> 00:02:34,500
Because we have these different movie reviews, if you remember lecture number one, which was back in winter.

42
00:02:34,500 --> 00:02:35,500
Feels like that.

43
00:02:36,500 --> 00:02:41,500
We had this very short movie reviews like, oh, this is bad movie.

44
00:02:41,500 --> 00:02:45,500
And we have this super lengthy, you know, thousand words reviews.

45
00:02:45,500 --> 00:02:51,500
And we want to somehow use all of them regardless of their length and predict maybe whether it's positive or negative.

46
00:02:51,500 --> 00:02:52,500
So this is one example.

47
00:02:52,500 --> 00:02:54,500
What do we really care about in NLP?

48
00:02:55,500 --> 00:03:00,500
How we were dealing with that so far was just, yeah, kind of make a simplification and hope like it will work.

49
00:03:00,500 --> 00:03:03,500
It works to some extent, but there's better approaches as we will talk today about.

50
00:03:04,500 --> 00:03:07,500
So this is a motivation for recurrent neural networks.

51
00:03:07,500 --> 00:03:10,500
And we start with a little bit of abstraction.

52
00:03:10,500 --> 00:03:15,500
What recurrent neural networks are actually could be what's this like the interface of working with them.

53
00:03:15,500 --> 00:03:18,500
And then we talk about particle implementation, implementations of recurrent neural network.

54
00:03:19,500 --> 00:03:20,500
So.

55
00:03:21,500 --> 00:03:24,500
Let's have a sequence of.

56
00:03:25,500 --> 00:03:34,500
And input vectors, OK, so what's coming in is not just words like or elements or items we're putting in vectors.

57
00:03:34,500 --> 00:03:35,500
All right.

58
00:03:35,500 --> 00:03:37,500
So what these vectors could be.

59
00:03:38,500 --> 00:03:46,500
Well, they have same dimensions, so the n dimension of input, each of these vectors and what these vectors could contain.

60
00:03:46,500 --> 00:03:53,500
So typically they would be a word embedding the token, but it could be any arbitrary input like one of encoding and so on.

61
00:03:53,500 --> 00:04:07,500
So if we have a sentence, the cat said and so on, what we are actually putting into our network is a sequence of vectors.

62
00:04:09,500 --> 00:04:12,500
And they have the same dimensionality, the n.

63
00:04:13,500 --> 00:04:14,500
Right.

64
00:04:16,500 --> 00:04:22,500
So and we have n of them, right, so this is times n, any arbitrary length.

65
00:04:22,500 --> 00:04:26,500
And we want our model to work for any n of what we choose.

66
00:04:26,500 --> 00:04:29,500
So nothing fixed, but some like depending on the inputs.

67
00:04:29,500 --> 00:04:30,500
OK.

68
00:04:31,500 --> 00:04:40,500
And what is in here, as I said, could be a yeah, typically we're using word embeddings, which are somehow pre-trained on as what was that one word2vec for instance, right.

69
00:04:42,500 --> 00:04:43,500
OK.

70
00:04:44,500 --> 00:04:46,500
Any questions so far?

71
00:04:47,500 --> 00:04:49,500
Very straightforward, we're putting in vectors.

72
00:04:50,500 --> 00:04:51,500
What is the output of that?

73
00:04:52,500 --> 00:04:54,500
Is some vector as well.

74
00:04:54,500 --> 00:04:56,500
So of dimension d out.

75
00:04:58,500 --> 00:05:07,500
And well, we're now pretending we're plugging out like we're kind of transforming the sequence of inputs into one output, as we will see later, it's much more powerful than that.

76
00:05:08,500 --> 00:05:16,500
Then basically our RNN, so Recurrent Neural Network, is just a function from the input, a sequence of vectors to one vector to the output.

77
00:05:17,500 --> 00:05:19,500
Spread, it can't be any simpler, actually.

78
00:05:20,500 --> 00:05:30,500
Right, so it's just sequence of embeddings or sequence of vectors in and vector out.

79
00:05:30,500 --> 00:05:32,500
OK, any questions?

80
00:05:32,500 --> 00:05:36,500
So this is like the main interface to RNN is this.

81
00:05:38,500 --> 00:05:39,500
OK, all good.

82
00:05:39,500 --> 00:05:40,500
So.

83
00:05:42,500 --> 00:05:45,500
If you think about it, like what is the end, what does it mean?

84
00:05:45,500 --> 00:05:53,500
Well, actually, these RNNs, this kind of simple interface is actually returning not only one output, but it's returning a sequence of outputs.

85
00:05:55,500 --> 00:06:03,500
Because, for instance, if we have like n is equals to three, so our input sequence is x1, x2, x3, so three vectors.

86
00:06:03,500 --> 00:06:04,500
Right.

87
00:06:04,500 --> 00:06:05,500
And we're returning here.

88
00:06:05,500 --> 00:06:06,500
It's a typo.

89
00:06:06,500 --> 00:06:08,500
It should be y3.

90
00:06:08,500 --> 00:06:12,500
It's returning, you know, RNN of these three inputs.

91
00:06:12,500 --> 00:06:17,500
OK, but actually the sequence also contains the subsequence x1, x2.

92
00:06:17,500 --> 00:06:18,500
Right.

93
00:06:19,500 --> 00:06:23,500
So this is also, so this is three again, I'm sorry.

94
00:06:24,500 --> 00:06:30,500
So this RNN function, it takes a subsequence of x1 and x2 and also should return y2.

95
00:06:31,500 --> 00:06:34,500
OK, because this is subsequence is part of this larger sequence.

96
00:06:35,500 --> 00:06:44,500
And if you take it to the extreme, then y1 would be RNN of x1.

97
00:06:45,500 --> 00:06:48,500
Yeah, because we're just, this is length one.

98
00:06:48,500 --> 00:06:48,500
Right.

99
00:06:48,500 --> 00:06:50,500
I mean, we're putting just one vector in and one vector out.

100
00:06:50,500 --> 00:06:55,500
So this is what we're getting for free, basically, by our definition.

101
00:06:57,500 --> 00:07:04,500
So this means that at each of the steps, one, two or three, the RNN is outputting a vector yi.

102
00:07:05,500 --> 00:07:06,500
Right.

103
00:07:07,500 --> 00:07:08,500
Everybody's with me.

104
00:07:08,500 --> 00:07:08,500
Why is it so?

105
00:07:08,500 --> 00:07:11,500
Because we define it in a way, basically.

106
00:07:11,500 --> 00:07:15,500
And we said like, yeah, take everything in up until sequence of n.

107
00:07:15,500 --> 00:07:18,500
And then at the end step, you know, spit something out.

108
00:07:19,500 --> 00:07:22,500
And obviously we have n sequence, you know, we have different subsequences.

109
00:07:22,500 --> 00:07:24,500
So basically at each step it's spitting something out.

110
00:07:24,500 --> 00:07:25,500
Right.

111
00:07:26,500 --> 00:07:26,500
Any question?

112
00:07:30,500 --> 00:07:31,500
You might be asking why?

113
00:07:32,500 --> 00:07:34,500
Like it's kind of weird.

114
00:07:34,500 --> 00:07:34,500
Yeah.

115
00:07:35,500 --> 00:07:37,500
It's a way of defining things.

116
00:07:37,500 --> 00:07:42,500
So there might be advantages of this definition.

117
00:07:42,500 --> 00:07:45,500
So let's call the sequence outputting function RNN star.

118
00:07:45,500 --> 00:07:46,500
Right.

119
00:07:46,500 --> 00:07:57,500
So we kind of, you know, we distinguish between something, the function which takes a sequence of vectors and spitting out a vector and sequence of vectors in and sequence of vectors out.

120
00:07:57,500 --> 00:07:57,500
Right.

121
00:07:57,500 --> 00:08:04,500
So the only thing which is important here is that it takes n vector in and spits out n vectors out.

122
00:08:04,500 --> 00:08:07,500
So the numbers, the n is the same.

123
00:08:07,500 --> 00:08:15,500
So we cannot, with this definition of RNN, we cannot do sequence of length n in and sequence of length n out.

124
00:08:15,500 --> 00:08:16,500
That's not what we're doing.

125
00:08:16,500 --> 00:08:19,500
Basically we say for each input, there could be one output.

126
00:08:19,500 --> 00:08:19,500
Because in that.

127
00:08:22,500 --> 00:08:22,500
Okay.

128
00:08:22,500 --> 00:08:22,500
So recap.

129
00:08:22,500 --> 00:08:23,500
Okay.

130
00:08:23,500 --> 00:08:30,500
So for a sequence of input vectors, x1 to i, we have this output vector for the whole sequence.

131
00:08:30,500 --> 00:08:37,500
And we have this vector of, we have this sequence of vectors output for the same input.

132
00:08:38,500 --> 00:08:38,500
So.

133
00:08:41,500 --> 00:08:45,500
So without knowing what the, so the question is like, yeah, what is this function actually?

134
00:08:45,500 --> 00:08:47,500
Like, you know, what is in there?

135
00:08:47,500 --> 00:08:51,500
Well, before actually going deeper, what is RNN function should be implementing?

136
00:08:51,500 --> 00:08:54,500
What, what could be the advantage of such a function?

137
00:08:54,500 --> 00:08:58,500
So it takes one to n and output something.

138
00:08:58,500 --> 00:09:00,500
What is the advantage?

139
00:09:02,500 --> 00:09:04,500
You know, what's, is it, is it any good?

140
00:09:04,500 --> 00:09:08,500
Like what, what could be, what could be in, for example, yes.

141
00:09:08,500 --> 00:09:09,500
Yeah.

142
00:09:09,500 --> 00:09:09,500
Okay.

143
00:09:09,500 --> 00:09:23,500
It does capture the context exactly, but remember the language models we had before, like Ngram language model, it captures the context as well.

144
00:09:23,500 --> 00:09:23,500
Right.

145
00:09:24,500 --> 00:09:28,500
So this is not the only advantage, but there's, sorry, I think you were first.

146
00:09:30,500 --> 00:09:31,500
It can remember something.

147
00:09:31,500 --> 00:09:32,500
Yeah.

148
00:09:32,500 --> 00:09:34,500
And gram language model remembers something as well.

149
00:09:34,500 --> 00:09:34,500
So.

150
00:09:35,500 --> 00:09:36,500
But you're right.

151
00:09:36,500 --> 00:09:36,500
You're on, you're on track.

152
00:09:36,500 --> 00:09:37,500
Yes.

153
00:09:40,500 --> 00:09:43,500
It remember variable length context because we're changing n.

154
00:09:43,500 --> 00:09:43,500
Yes.

155
00:09:44,500 --> 00:09:46,500
If we change n in Ngrams, it can also, but yeah.

156
00:09:46,500 --> 00:09:46,500
Okay.

157
00:09:46,500 --> 00:09:47,500
I got you.

158
00:09:48,000 --> 00:09:49,200
Something substantial.

159
00:09:50,300 --> 00:09:50,600
Yes.

160
00:09:57,500 --> 00:09:59,500
Oh, let's have documents.

161
00:09:59,500 --> 00:10:06,500
Let's have the cat sat and bad movie.

162
00:10:06,500 --> 00:10:08,500
So these are two documents, right?

163
00:10:08,500 --> 00:10:11,388
So here we have n equals three

164
00:10:11,412 --> 00:10:12,800
and here we have n equals two.

165
00:10:12,800 --> 00:10:17,888
And then basically if we plot, we can plug
either of those things into this function

166
00:10:17,888 --> 00:10:23,576
RNN and it will spit either one vector
or your three vectors or your two vectors.

167
00:10:24,500 --> 00:10:26,500
Does it answer your question or not?

168
00:10:26,500 --> 00:10:29,500
Or is it something where I just made a typo here?

169
00:10:29,500 --> 00:10:30,500
Well, basically yes.

170
00:10:30,500 --> 00:10:34,500
For each position, it outputs one, one, one.

171
00:10:35,500 --> 00:10:35,500
Yeah.

172
00:10:35,500 --> 00:10:41,488
Audience: So, so at the end, we have, we have
all three wise or we have, or is it like, yeah,

173
00:10:41,512 --> 00:10:47,500
the last one, basically this one, the last one,
this one is the last one of those sequence.

174
00:10:47,500 --> 00:10:48,500
I can, let me just, okay.

175
00:10:48,500 --> 00:10:51,500
I didn't want to draw any pictures here, but let me just write a picture.

176
00:10:51,500 --> 00:10:53,500
Maybe it will, you know, I have much cooler pictures later on.

177
00:10:54,500 --> 00:10:58,500
So, X1 X2 X3.

178
00:10:59,500 --> 00:11:04,500
And we say, let's plug it into something here.

179
00:11:05,500 --> 00:11:06,500
I don't want to spoil here to something.

180
00:11:06,500 --> 00:11:15,500
So let's, let's make it like something here and it will spit Y1, Y2, Y3.

181
00:11:17,500 --> 00:11:18,500
It's a very abstract, right?

182
00:11:18,500 --> 00:11:20,488
Because this is
something, this is going, this

183
00:11:20,512 --> 00:11:22,500
is going to be different
and more complicated.

184
00:11:23,500 --> 00:11:23,600
Okay.

185
00:11:25,500 --> 00:11:25,600
Okay.

186
00:11:25,600 --> 00:11:27,500
So the advantages of the RNN.

187
00:11:27,500 --> 00:11:27,500
Yeah.

188
00:11:27,500 --> 00:11:30,191
So we are in context
and we have different

189
00:11:30,215 --> 00:11:33,500
lengths, so this is good,
but we have something.

190
00:11:33,500 --> 00:11:38,500
Maybe, you know, if, if you compare it with N-Gram language models

191
00:11:38,500 --> 00:11:44,500
n-gram language model we had sort of a simplification, it even had a name.

192
00:11:45,500 --> 00:11:48,988
The simplification in
N-Gram language models,

193
00:11:49,012 --> 00:11:52,500
we kind
of, you know, get, got

194
00:11:52,500 --> 00:11:57,500
rid of it in a, in bird embeddings, but still we had some limits.

195
00:11:57,500 --> 00:12:01,500
So, okay.

196
00:12:01,500 --> 00:12:02,500
Maybe I'm asking wrong way.

197
00:12:03,500 --> 00:12:04,500
It takes the whole input.

198
00:12:05,500 --> 00:12:05,500
It takes it whole.

199
00:12:05,500 --> 00:12:07,500
We have the Markov property.

200
00:12:07,500 --> 00:12:12,500
So the name was Markov property because we are limiting to like K equals to three, like

201
00:12:12,500 --> 00:12:15,500
K previous words or the window and so on.

202
00:12:15,500 --> 00:12:20,500
But now this, this thing takes the whole input sequence and produces,

203
00:12:20,500 --> 00:12:21,500
you know, whatever is coming out.

204
00:12:21,500 --> 00:12:23,500
So basically it's conditioning on the whole input.

205
00:12:24,500 --> 00:12:27,500
So we are actually overcoming the Markov property.

206
00:12:27,500 --> 00:12:29,000
 We don't have to limit the context.

207
00:12:29,500 --> 00:12:29,900
Okay.

208
00:12:31,500 --> 00:12:32,500
Sounds great.

209
00:12:33,500 --> 00:12:39,500
So, so each output takes the entire history one to this position i

210
00:12:39,500 --> 00:12:41,500
Without the Markov property.

211
00:12:42,500 --> 00:12:44,500
So the question is like, what, what are we going to do with this, with this?

212
00:12:44,500 --> 00:12:45,500
Why?

213
00:12:45,500 --> 00:12:47,500
So there's a vector coming out of the sequence.

214
00:12:47,500 --> 00:12:49,500
So what can we do with that?

215
00:12:50,500 --> 00:12:54,500
And typically we use it for, as we were talking before for some further predictions.

216
00:12:54,500 --> 00:12:59,500
So for example, we can, we can plug it into softmax for, you know, for a

217
00:12:59,500 --> 00:13:01,500
classification, multi-class classification.

218
00:13:01,500 --> 00:13:06,500
We could plug it into through some, some MLP, some multi-layer perceptron,

219
00:13:06,500 --> 00:13:11,500
maybe one extra layer, some projection into whatever two, two dimensional

220
00:13:11,500 --> 00:13:14,500
vector or one dimension and put a sigmoid out of it if we do on top of it,

221
00:13:14,500 --> 00:13:17,500
if we do binary predictions and stuff like that.

222
00:13:17,500 --> 00:13:18,500
Okay.

223
00:13:18,500 --> 00:13:22,500
So it's basically taking an input, making a transformation of arbitrary

224
00:13:22,500 --> 00:13:27,500
length input into either one vector or a sequence of vectors of the same length.

225
00:13:28,500 --> 00:13:29,500
Okay.

226
00:13:30,500 --> 00:13:32,500
So how, how does this work?

227
00:13:32,500 --> 00:13:35,500
Like, how can this actually work?

228
00:13:35,500 --> 00:13:39,500
Like take any sequence of inputs and producing any sequence of outputs.

229
00:13:40,500 --> 00:13:42,500
So what, what could, what could be in there?

230
00:13:42,500 --> 00:13:45,500
Like there must be something happening because how can you design such a

231
00:13:45,500 --> 00:13:49,500
function that no matter how long the input you pass in, it will, you know,

232
00:13:49,500 --> 00:13:50,500
it will kind of adapt to that.

233
00:13:51,500 --> 00:13:54,988
So there is one underlying
mechanism here and

234
00:13:55,012 --> 00:13:58,500
the backbone idea,
and it's called states.

235
00:13:59,500 --> 00:14:03,500
Because we need to pass the information between these, like discrete

236
00:14:03,500 --> 00:14:05,500
states and time as you wish.

237
00:14:05,500 --> 00:14:08,500
So from one time point to another time point and so on.

238
00:14:08,500 --> 00:14:11,500
So we're passing information from one position to the next.

239
00:14:12,500 --> 00:14:22,500
So from the, let's say we're at position Y and let's, the cat and we're at position.

240
00:14:22,500 --> 00:14:23,500
So this is one, this is two.

241
00:14:24,500 --> 00:14:28,500
And here we have this, our tiny RNN thing.

242
00:14:29,500 --> 00:14:31,500
And here we have also RNN thing.

243
00:14:32,500 --> 00:14:35,500
And from coming to, to the next step.

244
00:14:35,500 --> 00:14:39,500
So you're outputting Y one, you're outputting Y two.

245
00:14:39,500 --> 00:14:44,500
And we're basically saving some sort of state, which we're

246
00:14:44,500 --> 00:14:46,500
passing to, to the next state here.

247
00:14:47,500 --> 00:14:49,500
So we're, we're doing something in here.

248
00:14:50,500 --> 00:14:56,500
Save it into some state, which we, you know, this S S I, and we're passing

249
00:14:56,500 --> 00:14:58,500
it to the next, next state in time.

250
00:14:59,500 --> 00:15:03,500
So this is how we're keeping the information for arbitrary length input.

251
00:15:03,500 --> 00:15:08,500
So we're saying, well, something will be stored somewhere and we just pass it by.

252
00:15:08,500 --> 00:15:10,500
What is in this state?

253
00:15:10,500 --> 00:15:11,500
It's an open question.

254
00:15:11,500 --> 00:15:13,500
Like this is a part of these architectures.

255
00:15:13,500 --> 00:15:14,500
Okay.

256
00:15:14,500 --> 00:15:20,500
But we are using a state vector to store something about the, you know, the

257
00:15:20,500 --> 00:15:23,500
history, about the context from the very beginning.

258
00:15:23,500 --> 00:15:24,200
Okay.

259
00:15:24,500 --> 00:15:25,300
Any question?

260
00:15:38,500 --> 00:15:43,500
So we can then recursively define these RNNs, which means that at each step

261
00:15:43,500 --> 00:15:47,500
Y, what we have, we have the current input vector.

262
00:15:47,500 --> 00:15:50,500
So this is maybe the word embedding coming in and we have the

263
00:15:50,500 --> 00:15:51,500
vector of the previous state.

264
00:15:52,500 --> 00:15:54,500
So S I minus one.

265
00:15:55,500 --> 00:15:57,500
Now you might be asking if we're at the very beginning.

266
00:15:57,500 --> 00:16:03,500
So we have X one, the, and it's going into our tiny little thing here.

267
00:16:04,500 --> 00:16:07,500
What is the previous, you know, what is S what is S zero?

268
00:16:07,500 --> 00:16:09,500
What is the previous one?

269
00:16:09,500 --> 00:16:09,500
Right.

270
00:16:09,500 --> 00:16:11,000
Because we always need a previous one.

271
00:16:11,500 --> 00:16:15,500
So this is the initial initial state vector.

272
00:16:15,500 --> 00:16:18,500
And typically, I mean, it's often omitted in the definitions and typically

273
00:16:18,500 --> 00:16:21,500
it could be fill in zeros, maybe.

274
00:16:21,500 --> 00:16:25,500
There could be some special architectures where you need to initialize with maybe

275
00:16:25,500 --> 00:16:28,500
old ones or some random initialization, but typically it would be zero.

276
00:16:28,500 --> 00:16:28,500
Okay.

277
00:16:30,500 --> 00:16:35,500
So at each step, we take the previous state and the current inputs and

278
00:16:35,500 --> 00:16:39,500
compute the current state as I.

279
00:16:41,500 --> 00:16:42,500
And how are we going to compute it?

280
00:16:42,500 --> 00:16:45,500
It's this beautiful function R as recurrent.

281
00:16:46,500 --> 00:16:49,500
And this is just basically an interface, right?

282
00:16:49,500 --> 00:16:51,500
So we're not saying what are we actually computing in there?

283
00:16:51,500 --> 00:16:52,500
We'll be, we'll do something.

284
00:16:52,500 --> 00:16:55,500
And it really depends on the implementation on the actual

285
00:16:55,500 --> 00:16:56,500
architecture of this RNNs.

286
00:16:57,500 --> 00:17:01,500
So we're specified at R later, but conceptually we're just taking current

287
00:17:01,500 --> 00:17:04,500
word and the previous state and produce the next state and go on.

288
00:17:05,500 --> 00:17:06,500
Okay.

289
00:17:06,500 --> 00:17:07,500
Everybody's with me.

290
00:17:08,500 --> 00:17:09,500
Sounds great.

291
00:17:11,500 --> 00:17:15,500
Also, so we have, we have the next state, but there's still something missing

292
00:17:15,500 --> 00:17:16,500
because we want to output something here, right?

293
00:17:16,500 --> 00:17:18,500
So this is the output of each step.

294
00:17:18,500 --> 00:17:24,500
So to compute the current outputs, again, we have, what do we have again?

295
00:17:24,500 --> 00:17:25,500
Each step we have the X_i.

296
00:17:25,500 --> 00:17:26,500
So this is the inputs word.

297
00:17:27,500 --> 00:17:29,500
We have the previous state as i - 1.

298
00:17:29,500 --> 00:17:31,500
So this is the same as in the previous slide.

299
00:17:32,500 --> 00:17:35,500
We already computed the next stage.

300
00:17:36,500 --> 00:17:43,500
Oh, and now the output will be basically taking the next state and running

301
00:17:43,500 --> 00:17:45,500
some functional on the, on, on top of that.

302
00:17:45,500 --> 00:17:52,500
So again, here we have say X one it's the, Oh, this is the word.

303
00:17:53,500 --> 00:17:54,800
Oh, this is a bad example.

304
00:17:54,800 --> 00:17:55,500
I'm sorry.

305
00:17:55,500 --> 00:17:57,500
Let's make it generic.

306
00:17:57,500 --> 00:18:03,500
So we have x_i it's coming into our RNN thing here.

307
00:18:04,500 --> 00:18:12,500
We have the previous state as i-1, and this is spitting out S_i, and

308
00:18:12,500 --> 00:18:17,500
what we're saying here, we're taking this, this new hidden state and run it

309
00:18:17,500 --> 00:18:23,500
through the old function to compute the, the outputs.

310
00:18:23,500 --> 00:18:23,500
Okay.

311
00:18:25,500 --> 00:18:26,500
Some, some sort of transformation here.

312
00:18:26,500 --> 00:18:30,500
What exactly all could be with the final layer as well as with the R function.

313
00:18:30,500 --> 00:18:31,500
So we have two functions.

314
00:18:31,500 --> 00:18:36,500
One is taking the previous state and input and code computes the output

315
00:18:36,500 --> 00:18:41,500
or the next state, and the second function is taking the current state

316
00:18:41,500 --> 00:18:42,900
and producing some output value.

317
00:18:43,500 --> 00:18:44,500
Okay.

318
00:18:44,500 --> 00:18:47,500
So two functions, any questions?

319
00:18:52,500 --> 00:18:53,500
Good.

320
00:18:53,800 --> 00:19:00,800
So to sum up at each step of these, of the sequence of the inputs, we have

321
00:19:00,800 --> 00:19:06,100
the current input x_i  and the previous state, and we compute two things.

322
00:19:06,100 --> 00:19:11,500
So this is the current state using this R function, which takes into account

323
00:19:11,500 --> 00:19:14,100
the previous state and the input and the output.

324
00:19:16,000 --> 00:19:20,000
What is important here that these functions R and O

325
00:19:20,000 --> 00:19:22,200
they can have different parameters as well.

326
00:19:22,200 --> 00:19:24,200
Obviously they will be parametriced in a way

327
00:19:24,200 --> 00:19:27,900
And these parameters and the functions will be the same for each of the

328
00:19:27,900 --> 00:19:31,400
positions, because for each of the position, we will, we will have the

329
00:19:31,400 --> 00:19:35,800
same function R and same function O taking our input and previous

330
00:19:35,800 --> 00:19:37,400
state and spitting out something.

331
00:19:37,500 --> 00:19:40,600
And these things have parameters.

332
00:19:40,600 --> 00:19:44,400
The parameters will remain the same all over the whole sequence.

333
00:19:44,600 --> 00:19:46,500
So they will be like shared parameters.

334
00:19:46,600 --> 00:19:49,800
So here's the summary of that RNN thing.

335
00:19:50,320 --> 00:19:54,320
So yeah, basically repeating was written here, but this is a simple

336
00:19:54,320 --> 00:19:57,020
kind of recurrent definition of RNN.

337
00:19:57,320 --> 00:20:04,320
I think it's time for some visualization of this abstract RNN, which is,

338
00:20:04,320 --> 00:20:07,320
yeah, you find in the literature basically most of the time.

339
00:20:07,320 --> 00:20:10,720
So again, this is the input we have at point Y.

340
00:20:11,320 --> 00:20:12,620
This is the previous.

341
00:20:12,880 --> 00:20:15,080
So this is a, this is a constant.

342
00:20:15,880 --> 00:20:17,880
We have the previous output.

343
00:20:17,880 --> 00:20:18,880
So this is some sort of vector, right?

344
00:20:18,880 --> 00:20:21,880
Because the states is a, is a vector.

345
00:20:23,880 --> 00:20:28,880
And there are two functions, R and O, which are producing s_i and y_i.

346
00:20:28,880 --> 00:20:30,880
So the output and the s_i's, right?

347
00:20:31,880 --> 00:20:36,880
And what is important here that if there's any parameters here, which we

348
00:20:36,880 --> 00:20:41,880
denote by this very fancy word theta, sorry, letter theta, the parameters

349
00:20:41,880 --> 00:20:44,880
will be shared for all, all steps.

350
00:20:44,880 --> 00:20:47,180
So at each step, the parameter parameters will be the same.

351
00:20:48,880 --> 00:20:49,880
What is this thing?

352
00:20:50,880 --> 00:20:52,880
Why is, why is there this an arrow?

353
00:20:59,880 --> 00:21:02,280
Yeah, because the recursion, exactly.

354
00:21:02,360 --> 00:21:05,060
Because the next step will be, I mean, this step will be previous

355
00:21:05,080 --> 00:21:06,460
step of the next one.

356
00:21:06,760 --> 00:21:07,760
Okay.

357
00:21:07,760 --> 00:21:07,760
Yeah.

358
00:21:13,760 --> 00:21:14,760
Yes.

359
00:21:14,860 --> 00:21:17,860
Because we want to, we want to predict something.

360
00:21:17,860 --> 00:21:20,560
We want to transform the state.

361
00:21:20,560 --> 00:21:23,560
So maybe the state could be the same as the output, right?

362
00:21:23,560 --> 00:21:26,560
So this could be same as this, but maybe not.

363
00:21:26,560 --> 00:21:30,560
Maybe you want to keep some information, but output something else.

364
00:21:31,560 --> 00:21:34,560
We will see later, we will see that later because you maybe want to keep

365
00:21:34,560 --> 00:21:37,560
like a large memory, but just output like a vector for predictions.

366
00:21:38,054 --> 00:21:39,560
And they will be different.

367
00:21:40,371 --> 00:21:41,560
Sometimes they're the same.

368
00:21:41,560 --> 00:21:43,014
That's yeah.
That's a great point.

369
00:21:44,560 --> 00:21:45,560
Any other question?

370
00:21:47,560 --> 00:21:51,560
Yeah. So this, this is like, you know,
there's this loop and it's kind of ugly.

371
00:21:52,560 --> 00:21:57,284
If you want to implement the thing, how,
how do you, how do you implement this thing?

372
00:21:57,309 --> 00:21:58,943
Because we're, we're talking about

373
00:21:58,968 --> 00:22:02,560
 so what is our big one idea of doing NLP,

374
00:22:02,560 --> 00:22:04,560
doing deep learning and NLP?

375
00:22:04,560 --> 00:22:06,560
What, what, what is it?

376
00:22:06,560 --> 00:22:10,560
We have one core idea of doing deep neural networks where we compute all this

377
00:22:10,560 --> 00:22:12,560
bloody thing, gradients and stuff like that.

378
00:22:12,979 --> 00:22:13,765
What is that?

379
00:22:14,146 --> 00:22:14,820
What was that?

380
00:22:15,560 --> 00:22:18,560
One abstraction, like the most powerful abstraction we're actually

381
00:22:18,560 --> 00:22:20,560
working all the time with, underlying.

382
00:22:23,037 --> 00:22:25,236
The perceptron is an architecture sort of.

383
00:22:25,386 --> 00:22:25,671
Yeah.

384
00:22:25,696 --> 00:22:29,560
I mean, it's a network, you have layers, but even like deeper, like the

385
00:22:29,560 --> 00:22:32,560
mathematical kind of concept there, or it's not even mathematical, it's

386
00:22:32,560 --> 00:22:33,560
computer science.

387
00:22:33,560 --> 00:22:33,560
Yes.

388
00:22:34,560 --> 00:22:36,560
You were first, I guess.

389
00:22:36,560 --> 00:22:36,560
I don't know.

390
00:22:38,560 --> 00:22:38,560
Yeah.

391
00:22:38,560 --> 00:22:40,560
And vector propagation running on something.

392
00:22:40,560 --> 00:22:42,560
So there is some underlying structure.

393
00:22:43,560 --> 00:22:44,560
That's the computational graph.

394
00:22:44,560 --> 00:22:47,560
The computational graph is where you run your forward propagation,

395
00:22:47,560 --> 00:22:49,560
deprecation and stuff like that.

396
00:22:49,560 --> 00:22:52,560
And everything, everything should be competition graph, but here.

397
00:22:53,560 --> 00:22:54,560
Like I'm using the same color.

398
00:22:54,560 --> 00:22:55,560
So over and over.

399
00:22:55,560 --> 00:22:58,391
So these are trainable parameters or trainable weights

400
00:22:58,416 --> 00:22:59,560
like the green lights, right?

401
00:22:59,560 --> 00:23:01,726
The green, green things are trainable.

402
00:23:02,154 --> 00:23:03,560
This is constant.

403
00:23:05,560 --> 00:23:09,935
And yeah, I'm using this as a sort of like intermediate gray things.

404
00:23:10,560 --> 00:23:13,560
Like this is a differential function.

405
00:23:17,560 --> 00:23:18,560
Yeah.

406
00:23:18,560 --> 00:23:18,560
Right.

407
00:23:18,560 --> 00:23:21,560
So computational graph, there is computational graph under

408
00:23:21,560 --> 00:23:22,560
everything we're talking about.

409
00:23:23,229 --> 00:23:26,872
How do you compute this kind of loop in computational graph?

410
00:23:33,930 --> 00:23:34,922
Yes.

411
00:23:45,851 --> 00:23:46,270
Oh, okay.

412
00:23:46,295 --> 00:23:50,342
So you would say you would, you wouldn't have a loop in your computational graph.

413
00:23:50,367 --> 00:23:53,170
So you would, you wouldn't wire it as a loop because how can you

414
00:23:53,250 --> 00:23:54,551
do back propagation on a loop?

415
00:23:55,012 --> 00:23:55,475
Yeah.

416
00:23:55,514 --> 00:23:56,578
You can't obviously.

417
00:23:56,603 --> 00:23:58,560
So you would say let's have for each step, I'm going to say, let's

418
00:23:58,560 --> 00:24:00,804
have for each step, one of those.

419
00:24:01,558 --> 00:24:02,057
Okay.

420
00:24:02,082 --> 00:24:02,304
Yeah.

421
00:24:02,329 --> 00:24:03,560
That's a great idea.

422
00:24:03,560 --> 00:24:04,569
It's actually how are we doing them?

423
00:24:04,594 --> 00:24:04,976
Exactly.

424
00:24:05,245 --> 00:24:06,404
We're calling it unrolling.

425
00:24:07,063 --> 00:24:12,560
So we take this, we take this abstract thing and then for each

426
00:24:12,560 --> 00:24:14,560
position, one, two, three, or four.

427
00:24:14,560 --> 00:24:17,560
So here I have N equals to four.

428
00:24:17,560 --> 00:24:23,412
I'm basically making sure that for each input, there is this kind of the same

429
00:24:23,437 --> 00:24:27,560
thing and the same function, R and O, R and O, R and O, R and O, right?

430
00:24:27,560 --> 00:24:32,560
So we're coming from this one with the loop and unrolling without a loop.

431
00:24:32,560 --> 00:24:33,560
This is a computational graph.

432
00:24:33,560 --> 00:24:35,560
I could run the propagation on.

433
00:24:35,976 --> 00:24:36,389
Right.

434
00:24:36,414 --> 00:24:41,560
Because yeah, as usual, you go from, you know, the forward pass and somewhere

435
00:24:41,560 --> 00:24:44,560
here, you get back with the losses and gradients and all good.

436
00:24:44,560 --> 00:24:45,560
You can do that.

437
00:24:47,166 --> 00:24:48,560
Here's the interesting part.

438
00:24:48,560 --> 00:24:49,928
I mean, I really want to highlight that.

439
00:24:50,560 --> 00:24:56,560
That the parameters of these R and O function, if there's any, they are

440
00:24:56,560 --> 00:24:58,968
shared and are the same for all the positions.

441
00:24:59,190 --> 00:24:59,992
So this is important.

442
00:25:00,017 --> 00:25:02,560
We're sharing parameters, right?

443
00:25:02,560 --> 00:25:03,560
We're sharing parameters.

444
00:25:03,560 --> 00:25:07,560
So each of these units or how you want to call it, like a recursive

445
00:25:07,560 --> 00:25:09,841
kind of functions are the same.

446
00:25:10,052 --> 00:25:11,301
If we train them.

447
00:25:11,444 --> 00:25:15,560
So if you, can you train this with, you know, having paid off

448
00:25:15,560 --> 00:25:16,560
like to all these things?

449
00:25:16,560 --> 00:25:17,560
Can you train that?

450
00:25:19,198 --> 00:25:22,889
Could you, could you train this network or do you need some special hacks?

451
00:25:23,560 --> 00:25:30,560
Yeah, I think we need some methods to combine the training

452
00:25:30,560 --> 00:25:34,560
directions that we need to take for all the steps.

453
00:25:35,129 --> 00:25:35,560
Okay.

454
00:25:35,560 --> 00:25:37,560
What, what could, what could that be?

455
00:25:39,560 --> 00:25:41,560
I think there's even fancy name for that.

456
00:25:42,560 --> 00:25:44,367
And I think we had it before.

457
00:25:44,392 --> 00:25:49,560
Even I, even I can even claim like we can do it right away with what we, what we know.

458
00:25:50,560 --> 00:25:56,560
So this is going to be a multivariate chain rule because you're going to

459
00:25:56,560 --> 00:25:58,560
sum up all the gradients here.

460
00:25:58,560 --> 00:26:01,560
And we had it already in a, you know, we had the share parameters in work

461
00:26:01,560 --> 00:26:05,560
to back or in, in word embeddings where he had just one, a matrix, which was

462
00:26:05,560 --> 00:26:08,560
large and kind of each of the embeddings was pointing there.

463
00:26:08,560 --> 00:26:09,560
We run it the same way.

464
00:26:09,560 --> 00:26:11,560
This there's no difference basically.

465
00:26:11,560 --> 00:26:11,560
Right.

466
00:26:11,560 --> 00:26:11,560
Yeah.

467
00:26:11,560 --> 00:26:12,560
I mean, you were right.

468
00:26:12,560 --> 00:26:16,560
So we would, we might need to tackle it differently, but we don't have to,

469
00:26:16,560 --> 00:26:18,560
because this is a graph we can work with.

470
00:26:18,560 --> 00:26:21,500
We can absolutely run stochastic gradient.

471
00:26:21,525 --> 00:26:24,560
And also, sorry, we can absolutely run a back propagation on this graph and it

472
00:26:24,560 --> 00:26:28,560
will work because we have, you know, chain of rule chain for multivariables

473
00:26:28,560 --> 00:26:31,152
and plus and stuff like that from calculus.

474
00:26:31,560 --> 00:26:32,560
So this is easy to train.

475
00:26:32,560 --> 00:26:37,560
I mean, basically once you, yeah, this is hard to work with the recursion, but

476
00:26:37,560 --> 00:26:44,560
once you unroll it, you can just run your whatever back propagation on that and

477
00:26:44,560 --> 00:26:47,560
train all the parameters you want to train, because there is nothing special about it.

478
00:26:47,560 --> 00:26:52,061
So the input still didn't have, for example, any role on the y_1?

479
00:26:52,625 --> 00:26:53,096
No.

480
00:26:53,771 --> 00:26:54,091
No.

481
00:26:56,194 --> 00:26:57,067
Yeah

482
00:26:57,137 --> 00:26:57,608
Okay.

483
00:26:57,734 --> 00:26:58,392
That's a great one.

484
00:26:58,455 --> 00:27:05,560
So the question was that the x_2 has no role in the y_1.

485
00:27:05,560 --> 00:27:08,070
So we're doing left to right.

486
00:27:09,375 --> 00:27:10,994
And maybe it's not a good thing.

487
00:27:11,019 --> 00:27:11,780
Maybe it's a good thing.

488
00:27:11,883 --> 00:27:16,414
So what is the advantage of doing left to right for systing in language?

489
00:27:18,730 --> 00:27:20,455
We're left to right.

490
00:27:20,480 --> 00:27:21,631
Maybe we think left to right.

491
00:27:21,656 --> 00:27:22,933
Time flies left to right.

492
00:27:23,322 --> 00:27:25,846
We're thinking we're using the past for creating the future.

493
00:27:25,885 --> 00:27:27,797
Maybe this is good.

494
00:27:28,583 --> 00:27:30,009
What is the disadvantage?

495
00:27:35,011 --> 00:27:37,448
You don't capture the whole context because there could be interest, because

496
00:27:37,646 --> 00:27:42,078
for this prediction, maybe if you're running, I don't know, thing, thinking

497
00:27:42,205 --> 00:27:45,400
of a task, like part of speech tech or entity recognition, okay.

498
00:27:45,488 --> 00:27:47,067
Maybe entity recognition.

499
00:27:48,210 --> 00:27:51,560
And here you still don't know all the information from the rest of the

500
00:27:51,560 --> 00:27:55,560
sentence because there's some long-range dependency.

501
00:27:55,560 --> 00:28:01,560
And maybe this, maybe recognition is the bad example, but maybe parsing.

502
00:28:01,560 --> 00:28:04,630
I mean, something very, you know, syntax, something very strange where

503
00:28:04,655 --> 00:28:06,361
you need a full sentence.

504
00:28:06,386 --> 00:28:11,901
So yeah, then you may think like, oh, maybe I can do something more

505
00:28:11,926 --> 00:28:12,718
fancier.

506
00:28:12,774 --> 00:28:13,694
And we'll do that.

507
00:28:13,726 --> 00:28:15,742
I'll come to that.

508
00:28:15,767 --> 00:28:19,464
This is the simplest left to right, but we might have something more

509
00:28:19,567 --> 00:28:20,099
complicated.

510
00:28:20,298 --> 00:28:20,956
Okay.

511
00:28:21,480 --> 00:28:22,861
Any other questions?

512
00:28:24,972 --> 00:28:25,551
Okay, great.

513
00:28:25,576 --> 00:28:27,796
So this is our computational graph.

514
00:28:27,821 --> 00:28:30,590
We can run left to right, find a frequent
thing, and we can train parameters.

515
00:28:30,653 --> 00:28:32,566
We still don't know what is in R and O.

516
00:28:32,735 --> 00:28:34,505
Yeah, so some fancy functions maybe

517
00:28:34,984 --> 00:28:36,595
It will be simpler than that

518
00:28:36,682 --> 00:28:37,610
So we'll come to that.

519
00:28:38,007 --> 00:28:40,010
So what can we actually do with such a network?

520
00:28:40,802 --> 00:28:41,192
Right?

521
00:28:41,256 --> 00:28:42,870
So what could be the task?

522
00:28:42,895 --> 00:28:46,078
So we have conceptually two things we can do.

523
00:28:46,586 --> 00:28:51,800
And the first one is that we have
this RNN as an acceptor on encoder.

524
00:28:51,967 --> 00:28:55,308
So basically here we have our graph again.

525
00:28:55,419 --> 00:29:01,784
And we run all these inputs and,
you know, have this final output here.

526
00:29:01,832 --> 00:29:03,450
So at the last step, we have this output.

527
00:29:03,475 --> 00:29:06,887
Which we plug into either softmax, if this has number of classes,

528
00:29:06,927 --> 00:29:10,506
if this have, if this is number of classes, is y

529
00:29:10,556 --> 00:29:14,215
y, for the output vectors have number of classes

530
00:29:14,587 --> 00:29:17,103
We might run softmax on top of that to get probability distribution

531
00:29:17,128 --> 00:29:21,802
and then maybe predict a thing if this is multiclass classification.

532
00:29:21,897 --> 00:29:22,560
Right?

533
00:29:22,560 --> 00:29:26,560
Or we can run it through much deeper whatever we want.

534
00:29:26,560 --> 00:29:30,560
Like plug it into maybe another network or some other parts, MLP,

535
00:29:30,560 --> 00:29:31,560
whatever.

536
00:29:31,560 --> 00:29:38,085
The point is that the loss for all these, well, all these parameters,

537
00:29:38,110 --> 00:29:41,672
but basically the loss which gets back propagated through all the

538
00:29:41,697 --> 00:29:45,728
gradients back to the beginning comes for only from the last output

539
00:29:45,753 --> 00:29:46,237
here.

540
00:29:46,262 --> 00:29:47,560
So this is what matters.

541
00:29:47,560 --> 00:29:51,560
I mean, this kind of representation, what's coming out of here,

542
00:29:51,560 --> 00:29:55,744
takes them, you know, has any value basically.

543
00:29:55,769 --> 00:29:58,284
And all these outputs are actually ignored.

544
00:29:58,309 --> 00:29:59,997
We don't care what's coming out of here.

545
00:30:00,022 --> 00:30:04,751
Because we only care about, for example, here, encoding a sentence

546
00:30:04,776 --> 00:30:08,500
into a vector which carries the full sentiment of the sentence,

547
00:30:08,525 --> 00:30:12,198
which we can use for sentiment, basically binary prediction of

548
00:30:12,246 --> 00:30:13,611
sentiment.

549
00:30:13,984 --> 00:30:14,254
Right?

550
00:30:14,279 --> 00:30:18,786
So we want to encode the whole sequence into one vector.

551
00:30:18,825 --> 00:30:20,579
And then run something on the vector.

552
00:30:20,604 --> 00:30:23,619
It's not different from continuous back of words.

553
00:30:23,996 --> 00:30:27,463
But for that, we kind of, you know, took each word's embeddings and

554
00:30:27,488 --> 00:30:28,690
made an average.

555
00:30:28,715 --> 00:30:30,715
And that was our representation.

556
00:30:30,901 --> 00:30:35,390
Here, it can go much simpler because there is an ordering of these

557
00:30:35,415 --> 00:30:36,415
things.

558
00:30:37,032 --> 00:30:37,583
Right?

559
00:30:37,608 --> 00:30:41,377
It matters which word comes first and which comes second.

560
00:30:41,402 --> 00:30:43,289
But the output will be the same.

561
00:30:43,314 --> 00:30:46,008
We have the vector representation of the whole input, and we can

562
00:30:46,033 --> 00:30:48,365
run it for some task.

563
00:30:48,390 --> 00:30:53,774
And the lost here will be backpropagated through the last point,

564
00:30:53,799 --> 00:30:55,560
through the whole network.

565
00:30:55,560 --> 00:30:56,560
Okay?

566
00:30:56,560 --> 00:31:01,560
Any questions?

567
00:31:01,560 --> 00:31:03,560
Great.

568
00:31:03,560 --> 00:31:05,560
So this is one example.

569
00:31:05,560 --> 00:31:10,560
The other, we can run RMMs as a so-called transducer, which is

570
00:31:10,560 --> 00:31:15,560
super fancy for, I don't know, what's like, where it's coming from,

571
00:31:15,560 --> 00:31:19,295
finite state machines, maybe, finite state automata?

572
00:31:19,763 --> 00:31:20,560
I don't know.

573
00:31:21,470 --> 00:31:23,375
But anyway, the idea is very simple.

574
00:31:23,400 --> 00:31:28,190
So in the previous case, we had the full input going through the

575
00:31:28,215 --> 00:31:31,293
network, and we had the last output, where we had a loss.

576
00:31:31,318 --> 00:31:36,866
But now, we're actually doing something called a sequence tagging,

577
00:31:36,891 --> 00:31:37,962
or this is a task.

578
00:31:37,987 --> 00:31:42,380
Typically, sequence tagging is done by this sort of architectures

579
00:31:42,405 --> 00:31:48,368
where we, at each of the positions,
we're spreading out vector, right?

580
00:31:48,560 --> 00:31:53,934
And then, we have a supervised
signal for each of these position vectors.

581
00:31:54,356 --> 00:32:01,260
So what could be a task that we can model with this architecture?

582
00:32:02,560 --> 00:32:07,465
What can you model with such an architecture, where on each input

583
00:32:07,490 --> 00:32:09,123
position, you have some output?

584
00:32:09,837 --> 00:32:11,241
What could such a task be?

585
00:32:11,266 --> 00:32:15,560
I think we had actually one in the
very beginning, in the first lecture, yes.

586
00:32:16,447 --> 00:32:19,560
It could be part of speech tagging, exactly, because then for each

587
00:32:19,560 --> 00:32:29,560
word here, like the cat said on and so on, we have the labels.

588
00:32:29,560 --> 00:32:32,560
So here, it could be determiner.

589
00:32:32,560 --> 00:32:35,016
This could be noun.

590
00:32:35,041 --> 00:32:36,984
This could be verb.

591
00:32:37,009 --> 00:32:40,246
And this could be so on, right?

592
00:32:40,271 --> 00:32:43,560
So we would basically, for each word, we would spit maybe one of the

593
00:32:43,560 --> 00:32:44,731
classes, right?

594
00:32:44,756 --> 00:32:45,595
Part of speech tagging.

595
00:32:45,620 --> 00:32:48,217
We didn't talk about part of speech tagging, to be honest, because

596
00:32:48,242 --> 00:32:49,973
it's just too low level.

597
00:32:49,998 --> 00:32:55,560
But we had something else where we had a sequence tagging problem.

598
00:32:55,560 --> 00:32:56,848
What was that?

599
00:32:56,873 --> 00:33:02,560
Where you spit out one of several classes for each token.

600
00:33:02,560 --> 00:33:05,560
Anyone remembers that?

601
00:33:05,560 --> 00:33:10,560
It also had a fancy name.

602
00:33:10,560 --> 00:33:11,692
Exactly.

603
00:33:11,717 --> 00:33:13,057
Name and recognition.

604
00:33:13,234 --> 00:33:16,783
So for each word, we would have, okay, so the cat said there are some

605
00:33:16,808 --> 00:33:20,630
entities, but maybe...

606
00:33:20,964 --> 00:33:22,179
uhm

607
00:33:23,560 --> 00:33:31,502
John Snow was killed?

608
00:33:31,527 --> 00:33:32,560
Was it legal?

609
00:33:32,560 --> 00:33:34,560
I don't know.

610
00:33:34,710 --> 00:33:35,570
That's a good question.

611
00:33:35,595 --> 00:33:37,062
Okay, something, right?

612
00:33:37,266 --> 00:33:42,377
And here, the labels would be, sorry, the labels could be depending on

613
00:33:42,402 --> 00:33:45,415
your text set, how you define it, but I would say this is beginning of

614
00:33:45,900 --> 00:33:53,342
person, this is inside of person, and this is O, and this is maybe also

615
00:33:53,367 --> 00:33:54,560
O, right?

616
00:33:54,560 --> 00:33:57,790
So we're spitting a tag for each of the token sequences.

617
00:33:57,815 --> 00:34:01,546
And this is exactly what you're going to model with record neural

618
00:34:01,571 --> 00:34:07,560
networks, as a transducer where you
output or you have a loss on each of that.

619
00:34:07,560 --> 00:34:08,419
So what is the loss here?

620
00:34:08,444 --> 00:34:14,560
Yes, I mean, this is a one-hot encoding label.

621
00:34:14,560 --> 00:34:17,657
And this is a vector.

622
00:34:17,682 --> 00:34:21,252
And you have, for example, okay, so what is a loss?

623
00:34:21,277 --> 00:34:21,887
Typical loss.

624
00:34:21,990 --> 00:34:27,560
I mean, the only loss you should remember from declaring for NLP.

625
00:34:27,560 --> 00:34:29,260
Crossentropilos

626
00:34:29,285 --> 00:34:29,998
Thank you.

627
00:34:30,109 --> 00:34:32,506
So he would run Crossentropilos

628
00:34:34,347 --> 00:34:39,338
And the question is, okay, so if we don't have this sum over here, so we

629
00:34:39,363 --> 00:34:45,081
have a function, computational graph, with multiple outputs, but we

630
00:34:45,106 --> 00:34:49,876
cannot run minimizing a function on multiple outputs, right?

631
00:34:49,901 --> 00:34:53,560
So we need to have one output only, which we want to minimize, the loss.

632
00:34:53,560 --> 00:34:56,560
There's only single loss we want to minimize.

633
00:34:56,560 --> 00:35:01,560
So that's why we typically take all these local sort of losses and plug

634
00:35:01,560 --> 00:35:03,560
into a sum.

635
00:35:03,560 --> 00:35:07,560
So this is the final loss.

636
00:35:07,560 --> 00:35:12,560
So it's sum of these crossentropies for each of the position.

637
00:35:12,560 --> 00:35:15,560
Does that make any sense?

638
00:35:15,560 --> 00:35:17,560
Why are we doing this?

639
00:35:17,560 --> 00:35:22,560
Because we can only minimize function with one output, right?

640
00:35:22,560 --> 00:35:27,560
We have billions, gazillions, dimensions, but we only minimize one value.

641
00:35:27,560 --> 00:35:31,560
Because I know how to make a compare two values and say which one is

642
00:35:31,560 --> 00:35:33,560
smaller than the other.

643
00:35:33,560 --> 00:35:35,560
I can't do it with two values, right?

644
00:35:35,560 --> 00:35:37,375
So we just need one.

645
00:35:37,400 --> 00:35:38,986
Okay?

646
00:35:39,011 --> 00:35:43,560
Any questions?

647
00:35:43,560 --> 00:35:44,748
Great.

648
00:35:44,773 --> 00:35:45,176
Okay.

649
00:35:45,201 --> 00:35:46,327
So we have this.

650
00:35:46,352 --> 00:35:52,890
So we can do this either taking last state and then running classifier on

651
00:35:52,915 --> 00:35:57,199
top of that or doing something very fancy or taking each input, sorry,

652
00:35:57,224 --> 00:35:59,560
each position and running something fancy on top of it and just summing

653
00:35:59,560 --> 00:36:01,560
up for the loss.

654
00:36:01,560 --> 00:36:02,945
We can train this as well.

655
00:36:02,970 --> 00:36:03,826
Yes, you have a question?

656
00:36:03,851 --> 00:36:08,560
Now we identify the thing on the right  [...] like another thing

657
00:36:08,560 --> 00:36:11,384
I mean, how can you start backpropagating in the middle?

658
00:36:11,873 --> 00:36:14,776
So what do you think, I mean, how can you start backpropagating in the middle.

659
00:36:14,801 --> 00:36:16,992
So what do you need to compute for the backpropagation?

660
00:36:17,153 --> 00:36:19,327
What you're interested in?

661
00:36:22,703 --> 00:36:28,739
Right, so this would be your, let's call it E as an error.

662
00:36:29,723 --> 00:36:33,814
And what you're interested in is a partial derivative of your error or

663
00:36:33,839 --> 00:36:40,016
your loss with respect to all these parameters.

664
00:36:46,012 --> 00:36:47,671
And so on. Right?
This is what you want.

665
00:36:47,696 --> 00:36:49,325
You're updating the parameters here.

666
00:36:49,579 --> 00:36:51,784
And this is your final value.

667
00:36:51,809 --> 00:36:54,811
This is like your output of the loss
and you want to minimize this function.

668
00:36:55,192 --> 00:36:56,793
So you have to start from here.

669
00:36:57,746 --> 00:37:00,836
I mean, well, you're starting like the forward pass, you're starting

670
00:37:00,892 --> 00:37:02,313
from the input values.

671
00:37:02,702 --> 00:37:05,575
You kind of compute all these intermediate values.

672
00:37:05,868 --> 00:37:08,027
And then you say, okay, this is my output.

673
00:37:08,052 --> 00:37:10,560
And I'm starting with this part.

674
00:37:10,560 --> 00:37:13,471
Partial derivative of E to E equals to 1.

675
00:37:13,606 --> 00:37:17,505
And then I'm going to the children of this node and doing this local

676
00:37:17,530 --> 00:37:19,712
derivative and so on.

677
00:37:19,737 --> 00:37:22,562
Basically the very same stuff we did in the second lecture.

678
00:37:22,962 --> 00:37:27,560
But you start from this single loss,
like this error of loss or how you call it.

679
00:37:27,560 --> 00:37:32,050
So you go from the beginning to the
top and then here you're backpropagating.

680
00:37:32,075 --> 00:37:37,096
Of course, I mean, the topology here of this network
is kind of complicated because, well, it's not.

681
00:37:37,151 --> 00:37:38,174
It's only deep.

682
00:37:38,199 --> 00:37:39,857
Basically it's a deep network.

683
00:37:40,111 --> 00:37:42,062
But you can't do anything wrong.

684
00:37:42,095 --> 00:37:45,895
I mean, if you apply the rules of, you know, computing from the last

685
00:37:45,920 --> 00:37:49,960
node to the children and then back to the children and to the children

686
00:37:49,985 --> 00:37:50,706
and to the children, you will finish with the full loop.

687
00:37:50,731 --> 00:37:52,864
children, you will finish with the full gradient.

688
00:37:53,560 --> 00:37:54,785
Does that make sense?

689
00:37:54,810 --> 00:37:55,698
Okay, cool.

690
00:37:55,723 --> 00:37:57,723
Any other questions?

691
00:37:59,560 --> 00:38:02,417
So I'm coming back to your question actually here.

692
00:38:02,442 --> 00:38:06,942
How to make it, you know, can we look into the future and use the

693
00:38:06,967 --> 00:38:08,656
future for some current predictions?

694
00:38:09,077 --> 00:38:14,451
Right? So how would you, for
example, what would it be like, so now you

695
00:38:14,498 --> 00:38:16,710
know this network.

696
00:38:16,735 --> 00:38:19,796
What would be like the most simple solution for this problem?

697
00:38:19,868 --> 00:38:23,692
Where you say, yeah, for representing the sentence, I want to take

698
00:38:24,240 --> 00:38:26,866
like left to right and right to left.

699
00:38:26,891 --> 00:38:29,668
What could be like the most simple solution you could use?

700
00:38:34,835 --> 00:38:38,105
You want to have a vector representing the whole sentence and this

701
00:38:38,130 --> 00:38:41,560
vector should have the representation which is learned left to right

702
00:38:41,560 --> 00:38:43,074
and right to left.

703
00:38:43,099 --> 00:38:45,560
How could you solve it?

704
00:38:45,560 --> 00:38:47,344
Running twice.

705
00:38:47,369 --> 00:38:49,560
And then?

706
00:38:49,560 --> 00:38:50,138
And then up.

707
00:38:50,163 --> 00:38:50,725
Okay.

708
00:38:50,750 --> 00:38:53,577
So you would like sum them up or something else?

709
00:38:54,758 --> 00:38:56,583
Maybe the average.
Okay.

710
00:38:56,608 --> 00:38:59,719
Yeah. That's not really far
away from what we would do.

711
00:38:59,766 --> 00:39:04,118
So we say run one RNN from left to right.

712
00:39:04,143 --> 00:39:05,560
We call this a forward.

713
00:39:05,560 --> 00:39:07,560
And one, another RNN from right to left.

714
00:39:07,560 --> 00:39:09,560
We call it a backward.

715
00:39:09,932 --> 00:39:11,972
This doesn't have to
do anything with

716
00:39:11,997 --> 00:39:14,495
forward propagation and backward
propagation. Nothing at all.

717
00:39:14,520 --> 00:39:16,520
It's just left to right and right to left.

718
00:39:16,559 --> 00:39:19,241
These are false friends.

719
00:39:19,352 --> 00:39:21,804
And then? Typically we concatenate them, right.

720
00:39:21,884 --> 00:39:25,176
We take one vector from one direction and concatenate the vector from

721
00:39:25,201 --> 00:39:26,298
the other direction.

722
00:39:26,805 --> 00:39:27,827
So what does it mean?

723
00:39:28,385 --> 00:39:35,535
If I have the cat sat, I'm running one RNN like that.

724
00:39:37,186 --> 00:39:39,186
And here comes one vector.

725
00:39:39,218 --> 00:39:48,453
And then I'm running, let's say, one RNN from the right-hand side.

726
00:39:48,478 --> 00:39:50,675
It's really hard to draw from the right-hand side.

727
00:39:50,700 --> 00:39:52,413
And I'm getting another vector.

728
00:39:52,469 --> 00:39:57,390
So let's call it y_f like forward and y_b as backward.

729
00:39:57,415 --> 00:40:03,014
Then I'm going to take these two vectors and just concatenate.

730
00:40:03,044 --> 00:40:03,871
That's it.

731
00:40:06,117 --> 00:40:08,117
How can I create a loss?

732
00:40:08,142 --> 00:40:09,723
Yeah, and the loss will be the same.

733
00:40:09,748 --> 00:40:12,560
Basically, I would sum up the losses from one side and the other.

734
00:40:12,560 --> 00:40:15,188
No, I don't have to sum up the losses.

735
00:40:15,213 --> 00:40:19,563
Then I will run my classification on this node.

736
00:40:19,588 --> 00:40:22,617
And everything will work out because it's just a computational graph.

737
00:40:22,680 --> 00:40:25,752
So there will be nothing very special about this case.

738
00:40:27,609 --> 00:40:28,560
Does that make sense?

739
00:40:30,262 --> 00:40:30,745
Okay.

740
00:40:30,770 --> 00:40:31,889
So basically we're saying

741
00:40:31,914 --> 00:40:34,373
We can look into the future for predicting the present.

742
00:40:34,398 --> 00:40:35,349
Yeah, fair enough.

743
00:40:35,374 --> 00:40:35,891
We can do that.

744
00:40:35,916 --> 00:40:37,440
We did it already with Word2Vec.

745
00:40:38,560 --> 00:40:39,321
Okay.

746
00:40:39,464 --> 00:40:43,093
So the big question now remains, what is happening inside this

747
00:40:43,118 --> 00:40:44,149
RNN?

748
00:40:44,325 --> 00:40:47,772
What are these functions doing that it could work like that?

749
00:40:47,797 --> 00:40:50,100
Theoretically, we talk about the interface.

750
00:40:50,125 --> 00:40:53,247
At each point, input in, previous state and some output and

751
00:40:53,291 --> 00:40:55,520
something very fancy and next state.

752
00:40:55,560 --> 00:40:56,741
Okay.

753
00:40:56,766 --> 00:40:57,963
What is in there?

754
00:40:57,988 --> 00:41:02,813
What could be the simplest network architecture?

755
00:41:02,838 --> 00:41:04,229
We call it simple RNN.

756
00:41:04,528 --> 00:41:06,528
I think it comes from the 1990s.

757
00:41:07,258 --> 00:41:16,428
Is that here, excuse me, we have a, so for computing the next

758
00:41:16,507 --> 00:41:21,560
state, s_i from the previous state, s_i - 1, and the current

759
00:41:21,560 --> 00:41:24,762
input, we just say, okay, let's take the previous state and

760
00:41:24,787 --> 00:41:28,292
multiply by some weights, some linear projection.

761
00:41:28,317 --> 00:41:34,925
Take the input, also do some linear
projection, add some, what is this?

762
00:41:36,682 --> 00:41:38,360
Bias.
We add some bias.

763
00:41:39,376 --> 00:41:44,560
So this is just linear mapping from the previous state and the current state.

764
00:41:44,560 --> 00:41:47,656
And this is some nonlinear function.

765
00:41:47,681 --> 00:41:48,356
Nothing more.

766
00:41:48,381 --> 00:41:52,324
So just basically linear transformation and some nonlinear

767
00:41:52,349 --> 00:41:54,094
transformation on top of that.

768
00:41:54,119 --> 00:41:55,284
Okay.

769
00:41:55,309 --> 00:41:56,134
Any questions?

770
00:42:00,796 --> 00:42:05,426
So we're saying take the previous state, apply some linear

771
00:42:05,451 --> 00:42:08,942
projection, take the current state, apply some linear projection

772
00:42:08,967 --> 00:42:13,520
and sum them up and run it through some nonlinearity because

773
00:42:13,545 --> 00:42:16,324
it's better to have nonlinearities because if everything is

774
00:42:16,349 --> 00:42:17,560
linear, the rest is linear.

775
00:42:17,560 --> 00:42:19,560
So the whole thing is linear.

776
00:42:19,560 --> 00:42:23,560
So there's some typical nonlinearities like ReLU or TomH.

777
00:42:23,560 --> 00:42:27,560
Then for the outputs, we're just basically saying, well, this is

778
00:42:27,560 --> 00:42:28,344
the state.

779
00:42:28,369 --> 00:42:30,661
So there's no fancy function.

780
00:42:30,686 --> 00:42:32,337
We just say this is SI.

781
00:42:32,362 --> 00:42:34,206
You were asking before why we need this.

782
00:42:34,231 --> 00:42:34,515
Yeah.

783
00:42:34,540 --> 00:42:37,480
So in the simplest case, we're just spitting out the state and

784
00:42:37,505 --> 00:42:38,695
it's fine enough.

785
00:42:39,560 --> 00:42:40,044
Okay.

786
00:42:40,081 --> 00:42:42,069
So this is very simple.

787
00:42:42,093 --> 00:42:45,779
What are the... So the devil is in the details

788
00:42:45,804 --> 00:42:48,949
These three matrices, I mean, two matrices and the bias are

789
00:42:48,974 --> 00:42:49,999
obviously learnable.

790
00:42:50,024 --> 00:42:53,001
So this is what you learn during backpropagation.

791
00:42:53,057 --> 00:42:55,904
But given this is a computational graph, you can learn it

792
00:42:55,929 --> 00:42:59,238
basically by backpropagation in a very nice way.

793
00:42:59,263 --> 00:43:00,905
So here are some dimensions.

794
00:43:00,960 --> 00:43:02,897
I'm not going to go through the details right now, but everything

795
00:43:02,922 --> 00:43:07,474
has to kind of work out in terms
of multiplying vector and matrix.

796
00:43:07,560 --> 00:43:08,810
This is still the same.

797
00:43:08,835 --> 00:43:11,354
So you have to pay attention to the dimensions of those things.

798
00:43:11,926 --> 00:43:15,560
And the nonlinear function as I said it's the tanhRO.

799
00:43:16,087 --> 00:43:16,968
Great.

800
00:43:16,993 --> 00:43:22,433
The problem here with this kind of approach, the simple

801
00:43:22,458 --> 00:43:25,560
recurrent neural networks, is so-called vanishing gradient.

802
00:43:25,560 --> 00:43:31,560
Because this is in fact like a very deep network and you're

803
00:43:31,560 --> 00:43:34,364
multiplying with the matrix W over and over.

804
00:43:34,389 --> 00:43:39,771
So if you're running from here and the network is really deep,

805
00:43:39,796 --> 00:43:47,190
so here you're multiplying lots of gradients and they become

806
00:43:47,215 --> 00:43:48,605
extremely close to zero.

807
00:43:48,670 --> 00:43:50,319
So it's called vanishing.

808
00:43:50,344 --> 00:43:53,044
So they vanish, they come to zero.

809
00:43:53,069 --> 00:43:55,318
And the point is if you have zero gradient, what happens?

810
00:43:55,343 --> 00:43:57,560
You don't update anything.

811
00:43:57,560 --> 00:44:02,873
So updating information from the beginning of the network gets

812
00:44:02,898 --> 00:44:06,182
harder because the signal is just far away.

813
00:44:06,207 --> 00:44:08,035
So the signal is at the end of the network.

814
00:44:08,060 --> 00:44:11,589
You have lots of some meaningful gradient.

815
00:44:11,614 --> 00:44:14,189
But when you're multiplying over and over, small numbers, small,

816
00:44:14,214 --> 00:44:17,220
small numbers, and then you end up with zero.

817
00:44:17,245 --> 00:44:22,546
And that makes it hard for RNNs to train basically and to capture

818
00:44:22,571 --> 00:44:23,934
long-range dependencies.

819
00:44:23,959 --> 00:44:28,100
So this is nice and simple, but a vanishing gradient is an issue.

820
00:44:28,125 --> 00:44:33,747
Because you want to keep information from
the very beginning for the whole sequence.

821
00:44:33,772 --> 00:44:38,092
the problem is if you have long-range dependencies problems,

822
00:44:38,394 --> 00:44:42,560
you have to have a solution which can solve long-range dependencies.

823
00:44:42,560 --> 00:44:44,560
And the simple RNNs, they can't do that.

824
00:44:44,560 --> 00:44:46,408
So you need something much better.

825
00:44:46,433 --> 00:44:50,560
So vanishing gradient, we call this vanilla RNN as well.

826
00:44:50,560 --> 00:44:52,351
Vanilla RNN.

827
00:44:52,376 --> 00:44:56,560
So simple RNN, vanilla RNN, just nothing fancy.

828
00:44:57,560 --> 00:44:59,560
Just these two functions basically.

829
00:44:59,585 --> 00:45:02,257
Linear projection and nonlinear rectangular.

830
00:45:02,560 --> 00:45:04,560
So we need to do something better.

831
00:45:04,560 --> 00:45:07,164
And now we come to this so-called gated arc.

832
00:45:07,189 --> 00:45:08,560
Any questions on that?

833
00:45:08,560 --> 00:45:10,560
Vanishing gradient.

834
00:45:10,560 --> 00:45:12,560
Yes.

835
00:45:14,560 --> 00:45:16,491
Why the gradient vanishes?

836
00:45:16,769 --> 00:45:17,824
Uhm

837
00:45:18,078 --> 00:45:21,927
Because the network is, well, because of the depth of the network.

838
00:45:22,173 --> 00:45:28,641
Like in the deep networks, you have the
issue of propagating the gradients from the,

839
00:45:28,704 --> 00:45:31,778
The gradients from the, so the signal comes from your output node.

840
00:45:32,560 --> 00:45:36,560
And then you're multiplying by this local derivatives and so on.

841
00:45:36,560 --> 00:45:45,344
And the more multiplications you have, the closer in some cases to zero it can go.

842
00:45:45,369 --> 00:45:48,560
And then you have basically very, very few updates in there.

843
00:45:48,560 --> 00:45:50,778
So what you would typically do is maybe training.

844
00:45:50,803 --> 00:45:54,560
So it was like back in the days of early training of RNNs,

845
00:45:54,560 --> 00:45:58,560
sorry, of deep nets was freezing layers and training like half of the network.

846
00:45:58,560 --> 00:46:02,560
And then training second half of the network and so on to overcome this.

847
00:46:02,560 --> 00:46:06,560
And in RNN is an example of this deep network.

848
00:46:06,560 --> 00:46:06,972
So this is an issue.

849
00:46:06,997 --> 00:46:07,575
Yes.

850
00:46:20,794 --> 00:46:23,861
The point is, yeah, you can exactly, you can have both.

851
00:46:23,886 --> 00:46:25,379
I mean, this is the thing.

852
00:46:25,404 --> 00:46:27,404
You have tanh, right?

853
00:46:27,429 --> 00:46:33,429
So in tanh, your gradient goes to zero on both parts.

854
00:46:33,454 --> 00:46:35,454
Right? I mean..

855
00:46:35,996 --> 00:46:37,939
You have something in the middle.

856
00:46:37,964 --> 00:46:40,115
There is a nice signal you're having in tanh.

857
00:46:40,140 --> 00:46:44,140
Let me just draw tanh.

858
00:46:44,560 --> 00:46:46,217
So roughly.

859
00:46:46,242 --> 00:46:49,945
So tanh is not a function we're using, which is great.

860
00:46:49,970 --> 00:46:53,782
So between minus one and one.

861
00:46:53,807 --> 00:46:58,560
But here, roughly, and from here, the gradient of the tanh is coming to zero.

862
00:46:58,560 --> 00:47:01,489
And then you have this issue of multiplying by something very small.

863
00:47:01,514 --> 00:47:04,560
If you have ReLU, yeah, this is great.

864
00:47:04,560 --> 00:47:06,729
In ReLU, you can have exploding gradients.

865
00:47:06,754 --> 00:47:07,050
Yeah.

866
00:47:07,075 --> 00:47:12,340
I mean, exploding gradients are easy to handle because you can just clip the norm, maybe, or something like that.

867
00:47:12,365 --> 00:47:15,177
But vanishing gradient is an issue here.

868
00:47:15,202 --> 00:47:18,976
And ReLU actually wasn't invented before, I guess.

869
00:47:19,001 --> 00:47:21,772
ReLU is kind of new conceptually.

870
00:47:21,797 --> 00:47:25,609
So previously, deep networks were sigmoids or tanh.

871
00:47:25,660 --> 00:47:29,105
And then you get this issue of vanishing gradient.

872
00:47:29,130 --> 00:47:32,360
Because the gradient is small in these normalities.

873
00:47:32,385 --> 00:47:36,292
So does it clarify that enough?

874
00:47:36,317 --> 00:47:37,660
Good.

875
00:47:39,275 --> 00:47:42,189
So we can use the gated architecture.

876
00:47:42,214 --> 00:47:44,008
And what does it mean?

877
00:47:44,033 --> 00:47:49,404
So if you look at RMAN, it's just as a general-purpose computing device.

878
00:47:49,429 --> 00:47:52,011
Yeah, it's a very brave statement.

879
00:47:52,036 --> 00:47:52,981
But anyway.

880
00:47:53,289 --> 00:47:59,358
So the state, SI, which is the vector, it's basically memory.

881
00:47:59,383 --> 00:48:03,098
Because you're passing this state from one step to another.

882
00:48:03,123 --> 00:48:08,741
So it's sort of a memory of the previous states.

883
00:48:08,766 --> 00:48:17,130
And here's just for recall, we give the function for a simple RNN, where you have the state multiplied by some weights.

884
00:48:17,155 --> 00:48:20,871
And you have the input multiplied by some weights and adding some bias.

885
00:48:20,896 --> 00:48:22,327
And running through some normality.

886
00:48:22,352 --> 00:48:23,786
But this is important part.

887
00:48:23,811 --> 00:48:33,660
Because each application of this function R, basically it reads the current memory, s_(i - 1).

888
00:48:33,660 --> 00:48:38,275
It reads sort of the inputs, x_i.

889
00:48:38,300 --> 00:48:44,502
Operates on them in some way, basically this addition and multiplication.

890
00:48:44,527 --> 00:48:50,966
And it writes the result to the new state, s_i.

891
00:48:50,991 --> 00:48:54,226
So we're reading the previous state, we're reading the input and writing to SI.

892
00:48:54,251 --> 00:48:56,251
Right?

893
00:48:56,276 --> 00:48:58,276
Like in a very abstract way.

894
00:48:58,301 --> 00:49:04,101
We take the memory, we take the input, do some operation and write again to the memory.

895
00:49:04,126 --> 00:49:06,460
Everybody's with me on that?

896
00:49:06,460 --> 00:49:08,460
Yes. Okay.

897
00:49:08,485 --> 00:49:10,919
So we're accessing the memory.

898
00:49:10,944 --> 00:49:12,460
And the access is not controlled.

899
00:49:12,460 --> 00:49:18,460
Because at each step, the entire memory state is read and entire memory state is written.

900
00:49:18,460 --> 00:49:24,460
So we're taking previous state, the memory, we take the input, do something very fancy and save the whole memory again.

901
00:49:24,460 --> 00:49:30,923
Like we're overwriting the whole memory with the result of the computation of the previous memory and the current memory.

902
00:49:30,948 --> 00:49:32,460
So we're rewriting the whole memory.

903
00:49:32,460 --> 00:49:37,193
It might be a good thing, might be a bad thing, but it turns out it's not what we want.

904
00:49:37,218 --> 00:49:42,460
We want maybe something where we can control the access to the memory.

905
00:49:42,460 --> 00:49:45,460
So how can we provide control memory access?

906
00:49:45,460 --> 00:49:47,037
Let's have the memory.

907
00:49:47,062 --> 00:49:52,057
So we call it now just memory vector S in some dimensions and the input vector X in some dimensions.

908
00:49:52,082 --> 00:49:52,605
Right?

909
00:49:52,630 --> 00:49:54,750
Now just abstraction.

910
00:49:54,935 --> 00:49:56,190
So we have two vectors.

911
00:49:56,215 --> 00:49:57,857
This is the memory and this is the input.

912
00:49:57,882 --> 00:50:06,078
And let's have a binary vector which we call the gate, which is just zeros and ones of the same dimension.

913
00:50:06,103 --> 00:50:06,960
Okay?

914
00:50:06,960 --> 00:50:08,631
So we have now three vectors.

915
00:50:08,656 --> 00:50:12,108
Memory, the current input, and we have something which is called a gate.

916
00:50:12,133 --> 00:50:14,295
And this is just zeros and ones.

917
00:50:14,477 --> 00:50:23,960
So here's a very fancy thing, which is the element-wise multiplication called Hadamard product.

918
00:50:23,960 --> 00:50:29,020
I don't think you have to remember, but maybe you can remember this.

919
00:50:29,045 --> 00:50:33,469
It's just a very fancy name for element-wise multiplication of two vectors.

920
00:50:33,494 --> 00:50:40,172
So you take each vector, each position at one vector, multiply each position in the other vector, and this is the result.

921
00:50:40,197 --> 00:50:44,532
And I'm using this kind of circle and dot for that.

922
00:50:44,557 --> 00:50:44,960
Okay?

923
00:50:44,985 --> 00:50:50,804
So Hadamard product basically multiplies element-wise multiplication of two vectors.

924
00:50:50,829 --> 00:51:00,192
Now, anyway, if I have this memory vector and the input, and we have a gate, which is just zeros and ones,

925
00:51:00,217 --> 00:51:05,732
then I'm going to use this sort of operation to control the access to the memory.

926
00:51:05,757 --> 00:51:08,799
So what does it mean?

927
00:51:08,824 --> 00:51:17,035
So I'm going to take the entries in X corresponding to ones in G, right?

928
00:51:17,060 --> 00:51:24,353
Because if I multiply something with, you know, so the G is zero and ones, and X is just real numbers.

929
00:51:24,378 --> 00:51:32,131
So I'm going to, what I end up here is just taking the values in X for which the G is set to one.

930
00:51:32,156 --> 00:51:32,960
Right?

931
00:51:32,960 --> 00:51:35,414
It's basically like masking.

932
00:51:35,439 --> 00:51:41,601
So if G is zeros, this output will be zero for this particular choice.

933
00:51:41,626 --> 00:51:48,237
So I'm reading the entries of X corresponding to ones in the gate, and I'm going to write them to the memory.

934
00:51:48,262 --> 00:52:00,449
And in the next part, so this will be, so one plus G in binary means I'm flipping the bits in the gate.

935
00:52:00,474 --> 00:52:12,596
And now I'm taking the rest, you know, the remaining parts of the gate and reading from the previous state and saving to the memory.

936
00:52:12,621 --> 00:52:15,901
So if it looks very weird, let's work on an example here.

937
00:52:15,926 --> 00:52:17,101
So I have an example in the next slide.

938
00:52:17,126 --> 00:52:18,960
It will be much clearer.

939
00:52:19,913 --> 00:52:20,237
Okay.

940
00:52:20,262 --> 00:52:21,960
So here's our formula again.

941
00:52:21,960 --> 00:52:24,797
And I have, so this is, okay, what do we have here?

942
00:52:24,822 --> 00:52:29,232
So this is vector X, and this is the state.

943
00:52:29,257 --> 00:52:31,266
So the previous state, and this is our gate.

944
00:52:31,377 --> 00:52:32,267
So the gate is given.

945
00:52:32,292 --> 00:52:33,227
I just made it up here.

946
00:52:33,252 --> 00:52:35,572
So our gate is zero, one, and zero.

947
00:52:35,597 --> 00:52:37,960
So this is our gate, G.

948
00:52:37,960 --> 00:52:42,960
And this is the input, and this is from the previous state, so the memory.

949
00:52:42,991 --> 00:52:50,238
So what I'm doing here first is I'm just multiplying this element box.

950
00:52:50,263 --> 00:52:54,960
So basically saying this will be what?

951
00:52:54,960 --> 00:52:59,731
Zero, because zero times 10 is zero, one times 11.

952
00:52:59,756 --> 00:53:02,960
I'm basically masking the second vector with the gate.

953
00:53:02,960 --> 00:53:03,460
Right?

954
00:53:03,485 --> 00:53:05,518
Everybody with me on this operation?

955
00:53:05,543 --> 00:53:09,960
So I'm multiplying these vector element by, basically, it's masking by this mask.

956
00:53:09,960 --> 00:53:12,563
Okay?

957
00:53:12,588 --> 00:53:13,389
Good.

958
00:53:13,414 --> 00:53:20,960
So here, one plus G, it's going to, this is the flip of this.

959
00:53:21,669 --> 00:53:22,960
Right?

960
00:53:22,960 --> 00:53:28,960
Because in binary, I'm just making from zeros ones and from ones zeros.

961
00:53:28,960 --> 00:53:31,960
And here, oops, excuse me.

962
00:53:31,960 --> 00:53:35,960
And here again, I'm just masking the memory vector.

963
00:53:35,960 --> 00:53:39,960
So what's coming out of here is eight.

964
00:53:39,960 --> 00:53:44,960
Here it's coming zero, because it's zero in the mask, and here it's coming three.

965
00:53:44,960 --> 00:53:45,960
Okay?

966
00:53:45,960 --> 00:53:52,960
So here I basically mask the input, and here I negated the gate and masked the memory.

967
00:53:52,960 --> 00:54:02,244
And because they are in different position zeros, adding them together will keep me the new input from the current state,

968
00:54:02,269 --> 00:54:07,960
and from the rest will be kind of stored from the previous memory.

969
00:54:07,960 --> 00:54:08,960
Okay?

970
00:54:08,960 --> 00:54:09,960
Does it make sense?

971
00:54:09,960 --> 00:54:15,806
Why this product, why this masking works like, you know, control access to the memory?

972
00:54:15,831 --> 00:54:20,103
Basically, masking one vector and using the negation of the mask to mask another vector,

973
00:54:20,128 --> 00:54:27,960
and just sum them together to get kind of, this gate controls what gets new, basically, to the memory.

974
00:54:27,960 --> 00:54:28,960
Okay?

975
00:54:28,960 --> 00:54:30,960
Any questions?

976
00:54:38,960 --> 00:54:40,960
Yes?

977
00:54:44,960 --> 00:54:51,960
We are not at all in matrices, or sorry, in embedding the weight matrices, nothing.

978
00:54:51,960 --> 00:54:57,960
I mean, we are just, this is just completely out now of deep learning or whatever.

979
00:54:57,960 --> 00:55:03,960
This is just how you, if you have a vector of memory, if you have a vector of inputs, and you have a gate,

980
00:55:03,960 --> 00:55:09,960
how you can combine them together to save something from the input and keep the rest of the memory.

981
00:55:09,960 --> 00:55:10,960
This is like a sidestep.

982
00:55:10,960 --> 00:55:11,960
Okay?

983
00:55:11,960 --> 00:55:14,960
But you might be asking, why the heck are we doing this?

984
00:55:14,960 --> 00:55:15,960
Why?

985
00:55:15,960 --> 00:55:18,960
Why is it important for deep learning?

986
00:55:18,960 --> 00:55:27,684
So, we can use these gates for R and N, because we want to control access to our state over time.

987
00:55:27,709 --> 00:55:30,960
But here the gates are, yeah, the gates here are not learnable.

988
00:55:30,960 --> 00:55:32,960
We are kind of given this or made up.

989
00:55:32,960 --> 00:55:37,629
And these gates are not really differentiable.

990
00:55:37,654 --> 00:55:42,960
Hard gates, like 0s and 1s, yeah, this is, 0s and 1s are bad for differentiation.

991
00:55:42,960 --> 00:55:44,960
So, this is not good.

992
00:55:44,960 --> 00:55:49,960
So, we are going to replace them with so-called soft gates.

993
00:55:49,960 --> 00:55:52,960
So, how can you, okay, one brainstorming.

994
00:55:52,960 --> 00:56:01,960
So, can you replace this with something which will give you something differentiable and close to 0 and 1s?

995
00:56:01,960 --> 00:56:10,960
We have a function which gives you something almost 0 or 1 and is differentiable.

996
00:56:10,960 --> 00:56:16,960
We have a function which, whatever it comes in, gives you either 0 or 1 almost.

997
00:56:16,960 --> 00:56:18,960
Sigmoid.

998
00:56:18,960 --> 00:56:19,960
Sigmoid is differentiable?

999
00:56:19,960 --> 00:56:20,960
Yes.

1000
00:56:20,960 --> 00:56:26,960
So, maybe we'll use somewhere in Sigmoid to get something which is almost 0 and 1, and it will be used for gates, maybe.

1001
00:56:26,960 --> 00:56:27,960
Okay? Yes.

1002
00:56:27,960 --> 00:56:36,960
So, we're coming to one very famous and very interesting implementation, not implementation, architecture of RNNs.

1003
00:56:36,960 --> 00:56:40,960
It's called the long short-term memory, LSTM.

1004
00:56:40,960 --> 00:56:42,960
So, it fixes two things at once.

1005
00:56:42,960 --> 00:56:44,960
I mean, not fixes two things at once.

1006
00:56:44,960 --> 00:56:51,960
It fixes the vanishing gradient problem, and it's also the first one which introduces gating mechanism into RNNs.

1007
00:56:51,960 --> 00:56:58,632
Okay, so now LSTM splits the state vector exactly in two halves, right?

1008
00:56:58,657 --> 00:57:06,960
So, we take one half of this state vector as a memory cells, and the second will be something like we'll call working memory or something like that.

1009
00:57:06,960 --> 00:57:13,960
And this is all the answer to you, like, well, we can spit out the state as output, but here the state will be more complicated.

1010
00:57:13,960 --> 00:57:18,767
We have two vectors in the state, and they have special meaning and special role, and they don't interact that much.

1011
00:57:18,792 --> 00:57:19,960
Okay?

1012
00:57:19,960 --> 00:57:26,020
So, the memory cells are, yeah, they
are designed to preserve the memory and

1013
00:57:26,045 --> 00:57:30,960
the error degradients over time, and we
have this differential gating component.

1014
00:57:30,960 --> 00:57:42,960
So, we will have some gates, and which works as almost as we had with these binary gates, but they will be completely differentiable, which makes them learnable in a way.

1015
00:57:42,960 --> 00:57:45,960
So, they will simulate logical gates, right?

1016
00:57:45,960 --> 00:57:48,974
So, that's why I think it's important
to understand how these gates kind

1017
00:57:48,999 --> 00:57:52,298
of work before you see the Hadamard
product in LSTM, and just like, why?

1018
00:57:52,323 --> 00:57:54,548
What is this?

1019
00:57:54,573 --> 00:57:55,960
Maybe it's like, why?

1020
00:57:55,960 --> 00:57:56,960
What is this?

1021
00:57:56,960 --> 00:57:59,810
It's complicated, but anyway, I hope we'll get through that.

1022
00:57:59,835 --> 00:58:01,960
Okay, so LSTM.

1023
00:58:01,960 --> 00:58:07,787
So, as I said, like, the state, I'm switching now indices from i to j, okay?

1024
00:58:07,812 --> 00:58:15,322
So, this is, it might be confusing, but there will be something with an index i, which will have a different meaning.

1025
00:58:15,347 --> 00:58:18,960
So, now we have, you know, our input is 1, 2, and j.

1026
00:58:18,960 --> 00:58:20,960
1, 2, and j, okay?

1027
00:58:20,960 --> 00:58:22,960
So, this is the position in the input.

1028
00:58:22,960 --> 00:58:24,960
It's not i anymore.

1029
00:58:24,960 --> 00:58:36,309
So, the state is now two parts of the memory components, which is a vector, and another, the hidden state, so-called component, position j, it's also a vector.

1030
00:58:36,334 --> 00:58:39,610
And they have the same size, these two things, okay?

1031
00:58:39,635 --> 00:58:41,960
So, this is just, we're setting up a scene now.

1032
00:58:41,960 --> 00:58:50,130
And at each input state j, the gate decides
how much of the new information should be

1033
00:58:50,155 --> 00:58:55,960
written to the memory, and how much of the
current content of the memory should be forgotten.

1034
00:58:55,960 --> 00:58:57,960
Yeah, this is similar to what we had before.

1035
00:58:57,960 --> 00:59:01,658
We had, like, yeah, you know, use
the gate for keeping some information

1036
00:59:01,683 --> 00:59:06,960
from the memory, and use a gate
for adding new inputs to the memory.

1037
00:59:06,960 --> 00:59:08,960
So, here it will be more complicated.

1038
00:59:08,960 --> 00:59:11,068
We have to, like, we will have multiple gates, but their function is the same.

1039
00:59:11,100 --> 00:59:18,424
Basically, selecting, like, 0 and 1s, almost,
zeroing out information which should be ignored

1040
00:59:18,449 --> 00:59:23,960
in this step, and keeping 1s for the information
that should be kind of important in this step.

1041
00:59:23,960 --> 00:59:26,960
So, there are three gates in LSTM.

1042
00:59:26,960 --> 00:59:29,960
The input gate, the forget gate, and the output gate.

1043
00:59:29,960 --> 00:59:32,960
We'll come to that exactly right now.

1044
00:59:32,960 --> 00:59:37,960
So, let's work out through the LSTM architecture.

1045
00:59:37,960 --> 00:59:47,960
So, what we have here again, we have this memory cell from the state before, and we have this working memory state from before.

1046
00:59:47,960 --> 00:59:52,960
Okay, so this is, these two things together is the s_j-1.

1047
00:59:52,960 --> 00:59:54,960
So, this is the state.

1048
00:59:54,960 --> 00:59:56,960
Okay, and here we have the input.

1049
00:59:56,960 --> 01:00:01,960
Any questions?

1050
01:00:01,960 --> 01:00:02,960
Good, this is great.

1051
01:00:02,960 --> 01:00:11,672
I mean, we have, you know, there's no surprise here, just to understand we're on the same page, because it might get complicated a little bit, but we have to go, like, you know, step by step.

1052
01:00:11,697 --> 01:00:13,960
All right, so we're going to do one thing.

1053
01:00:13,960 --> 01:00:21,038
We're going to take the previous
hidden state, and we're going to take

1054
01:00:21,063 --> 01:00:25,960
the input and create something
which we call the update candidate.

1055
01:00:25,960 --> 01:00:33,960
So, we're going to take these two parts, and again, we're, yeah, we're multiplying the input by some matrix.

1056
01:00:33,960 --> 01:00:35,960
So, this is some linear projection.

1057
01:00:35,960 --> 01:00:44,960
We're multiplying the previous hidden state by another matrix, and going through some non-linear function, which is tanh here.

1058
01:00:44,960 --> 01:00:47,498
Okay, why are we doing this?

1059
01:00:47,523 --> 01:00:49,960
I mean, this is basically the same as a simple RNN.

1060
01:00:49,960 --> 01:00:57,960
We take the input, projection, previous state projection, and adding them up, and using some non-linearity function in there.

1061
01:00:57,960 --> 01:01:00,960
Okay, everybody's with me?

1062
01:01:00,960 --> 01:01:05,960
There is nothing substantial new, you know, as opposed to the simple RNNs.

1063
01:01:05,960 --> 01:01:08,960
Okay, good, so this is our update candidate.

1064
01:01:08,960 --> 01:01:09,960
Great.

1065
01:01:09,960 --> 01:01:15,579
We're going to do one more thing, which will be almost the same as this one.

1066
01:01:15,604 --> 01:01:18,864
So, we're going to take,again, the previous hidden state,

1067
01:01:18,888 --> 01:01:22,134
and the input as well,
and again, we're going to

1068
01:01:22,159 --> 01:01:27,471
compute, you know, multiply by some weight matrix, and

1069
01:01:27,496 --> 01:01:30,420
add the previous, also
multiplied by some weight matrix.

1070
01:01:30,445 --> 01:01:33,931
So, what's the substantial difference here?

1071
01:01:33,956 --> 01:01:34,960
What is the substantial difference?

1072
01:01:34,960 --> 01:01:36,960
Can you spot a substantial difference here?

1073
01:01:36,960 --> 01:01:38,960
There's two differences.

1074
01:01:38,960 --> 01:01:43,960
Okay, here we use sigmoid, and we use tanh here.

1075
01:01:43,960 --> 01:01:49,405
Okay, another difference.

1076
01:01:49,430 --> 01:01:51,211
We have different parameters, exactly.

1077
01:01:51,236 --> 01:01:56,960
So, we have these, this one is indexed by HF and XF.

1078
01:01:57,960 --> 01:02:07,960
So, basically, it means like I'm multiplying X by the matrix for X, and the F means this will be the forget gate.

1079
01:02:07,960 --> 01:02:09,960
Ah, okay, this is it.

1080
01:02:09,960 --> 01:02:10,960
Forget gate.

1081
01:02:10,960 --> 01:02:12,960
Nice.

1082
01:02:12,960 --> 01:02:16,960
So, these are different matrices than these, right?

1083
01:02:16,960 --> 01:02:18,960
They will be different.

1084
01:02:18,960 --> 01:02:20,694
They will learn something else.

1085
01:02:20,719 --> 01:02:22,522
And why is it the gate now?

1086
01:02:22,547 --> 01:02:30,960
Because what we saw before, the ones and zeros, and we know what sigmoid is doing, we know what's coming out of here, right?

1087
01:02:30,960 --> 01:02:41,197
From this thing, it's coming some sort of, some sort of

1088
01:02:41,222 --> 01:02:44,935
something between 0
and 1, and mostly will be 0.

1089
01:02:44,960 --> 01:02:52,376
We want it to have almost 0 or almost 1, for things that should be forgotten and for things that should be not forgotten.

1090
01:02:52,401 --> 01:02:53,960
This is why it's called Fergat gate.

1091
01:02:53,960 --> 01:02:55,960
This is why it goes through the sigmoid.

1092
01:02:55,960 --> 01:02:57,960
Okay?

1093
01:02:57,960 --> 01:03:01,960
Everybody's with me still?

1094
01:03:01,960 --> 01:03:03,960
Good.

1095
01:03:03,960 --> 01:03:08,960
Now, we're going to do something else, and the same.

1096
01:03:08,960 --> 01:03:11,960
We take the previous hidden state.

1097
01:03:11,960 --> 01:03:17,960
We take the input, and again, multiply by some weight matrix.

1098
01:03:17,960 --> 01:03:20,251
This weight matrix is
different from this and

1099
01:03:20,276 --> 01:03:24,960
different from this,
because it's called Xi.

1100
01:03:24,960 --> 01:03:29,960
This will be the input gate.

1101
01:03:29,960 --> 01:03:35,515
And we're running this again through
sigmoid, so it will be again mostly 0s

1102
01:03:35,540 --> 01:03:41,460
and 1s, but it's differentiable, so
it's like hard, kind of hard binarization.

1103
01:03:41,485 --> 01:03:41,960
Okay?

1104
01:03:41,960 --> 01:03:43,960
And we're running that.

1105
01:03:43,960 --> 01:03:44,960
So there's input gate.

1106
01:03:44,960 --> 01:03:45,960
Okay?

1107
01:03:45,960 --> 01:03:47,960
And again, we have this update candidate.

1108
01:03:47,960 --> 01:03:49,960
Yeah, why is here tanh?

1109
01:03:49,960 --> 01:03:51,960
I don't know, maybe like popular choice.

1110
01:03:51,960 --> 01:03:55,960
I think it could be ReLU in theory, but in LSTM, it's tanh.

1111
01:03:55,960 --> 01:03:59,960
But here, the sigmoid is important, because the sigmoid is saying, well, it's a gate.

1112
01:03:59,960 --> 01:04:05,024
It should be either 0 or 1, mostly,
but we cannot do it hard, so we have a

1113
01:04:05,049 --> 01:04:10,671
differential diversion, so we use this
kind of binarization through sigmoid.

1114
01:04:10,696 --> 01:04:12,825
So we have two gates, Fergat gate and input gate.

1115
01:04:12,850 --> 01:04:14,960
What are they doing?

1116
01:04:14,960 --> 01:04:15,625
We don't know yet.

1117
01:04:15,650 --> 01:04:17,960
We're not there yet, but we have something with different parameters.

1118
01:04:17,960 --> 01:04:19,532
So they will learn something different.

1119
01:04:19,557 --> 01:04:21,714
They should learn something different.

1120
01:04:21,739 --> 01:04:22,960
Everybody's with me?

1121
01:04:22,960 --> 01:04:28,960
Any questions?

1122
01:04:28,960 --> 01:04:33,960
Good.

1123
01:04:33,985 --> 01:04:38,960
So now, if you recall from the previous slide, let me just come back a little bit.

1124
01:04:38,960 --> 01:04:49,289
If you recall this thing, like this formula,
roughly, where you multiply, take a gate

1125
01:04:49,314 --> 01:04:53,341
and multiply by input,
and take another gate,

1126
01:04:53,366 --> 01:04:55,512
sort of, and multiply
by the memory, right?

1127
01:04:55,537 --> 01:04:57,960
So this is what we did here to update the memory.

1128
01:04:57,960 --> 01:05:01,611
We're not going to do something much different from that now.

1129
01:05:01,758 --> 01:05:09,425
So we're going to take the previous memory

1130
01:05:09,450 --> 01:05:12,105
cell and multiply bythe Fergat gate, right?

1131
01:05:12,130 --> 01:05:18,960
So we're going to kind of forget something from the previous memory.

1132
01:05:18,960 --> 01:05:26,812
And we take the input gate, and here this is
our input candidate or update candidate, and

1133
01:05:26,837 --> 01:05:30,000
we're going to again
multiply by the gate and

1134
01:05:30,025 --> 01:05:34,960
sum together and save
it to the next memory.

1135
01:05:34,960 --> 01:05:35,960
Okay?

1136
01:05:35,960 --> 01:05:39,008
So it's basically very
similar to what we did

1137
01:05:39,033 --> 01:05:42,960
before, but here we kind
of learned these gates.

1138
01:05:42,960 --> 01:05:44,960
So we learned this.

1139
01:05:44,960 --> 01:05:45,960
We learned this.

1140
01:05:45,960 --> 01:05:50,673
This is coming from
these previous kind of

1141
01:05:50,698 --> 01:05:54,960
transformations, and
this is the previous step.

1142
01:05:54,960 --> 01:06:01,397
And this is our input, but the input was transformed through some, again, some transformation, some projection.

1143
01:06:01,422 --> 01:06:02,556
Yes?

1144
01:06:02,581 --> 01:06:07,960
I'm just going to make the product of i and z, but they both come from xj.

1145
01:06:07,960 --> 01:06:12,935
Why do I first do separate i and z and then put them together?

1146
01:06:12,960 --> 01:06:19,960
Why you do, why you go from x to z, and why you go from x to y?

1147
01:06:19,960 --> 01:06:23,960
I mean, x to i is clear because you need to make it a gate.

1148
01:06:23,960 --> 01:06:28,471
Why you go from xj and not like that?

1149
01:06:28,601 --> 01:06:29,960
Yeah.

1150
01:06:29,960 --> 01:06:38,264
Because maybe your representation
of your input should also

1151
01:06:38,289 --> 01:06:42,960
take into account the working
memory from the previous state.

1152
01:06:42,960 --> 01:06:46,960
Because you want to learn something from the past to represent the input.

1153
01:06:46,960 --> 01:06:49,960
I mean, it's implicitly here as well, right?

1154
01:06:49,960 --> 01:06:53,767
So you could theoretically
do that, but also I

1155
01:06:53,792 --> 01:06:56,960
think you can learn some
interesting mapping here.

1156
01:06:56,960 --> 01:07:02,333
I mean, I don't have an answer like why is it so, and would it work with like plugging directly the input?

1157
01:07:02,358 --> 01:07:04,638
Maybe it will work.

1158
01:07:04,663 --> 01:07:10,580
If you pay attention, I mean, xj has to be the same dimensions as the i's, obviously.

1159
01:07:10,605 --> 01:07:11,960
But theoretically it could work, yes.

1160
01:07:11,960 --> 01:07:17,141
But here, like just an extra bunch of
parameters to learn this thing and projecting

1161
01:07:17,166 --> 01:07:21,503
and taking into account here for this
kind of learning, also the previous one.

1162
01:07:21,528 --> 01:07:23,395
That's how they designed the LSTM.

1163
01:07:23,420 --> 01:07:25,960
I think it, I'm not sure, I have to check.

1164
01:07:25,960 --> 01:07:30,995
I mean, there's this gated
recurrent unit, GRU, which is simpler

1165
01:07:31,020 --> 01:07:32,963
than LSTM and there's some
simplifications with the gates.

1166
01:07:32,988 --> 01:07:34,352
So there's not so many gates.

1167
01:07:34,377 --> 01:07:37,587
And so maybe they're doing something like that, but I don't know by heart.

1168
01:07:37,612 --> 01:07:38,285
Yeah.

1169
01:07:38,310 --> 01:07:38,686
Yeah.

1170
01:07:38,711 --> 01:07:39,960
Fair point.

1171
01:07:39,960 --> 01:07:43,960
Any other question?

1172
01:07:44,762 --> 01:07:45,166
Okay, good.

1173
01:07:45,191 --> 01:07:50,672
So what we did so far, we have an input, previous memory, and we create a current memory.

1174
01:07:50,697 --> 01:07:51,649
This is great.

1175
01:07:51,674 --> 01:07:52,379
Yeah.

1176
01:07:52,404 --> 01:07:53,935
So what is missing still there?

1177
01:07:53,960 --> 01:07:58,960
We need still, what do we need to do?

1178
01:07:58,960 --> 01:08:03,960
We need to calculate hj and we need to calculate?

1179
01:08:03,960 --> 01:08:05,960
And the output, why exactly?

1180
01:08:05,960 --> 01:08:06,484
So two things.

1181
01:08:06,509 --> 01:08:09,960
Let's start with the output.

1182
01:08:09,960 --> 01:08:19,960
So we're going to introduce another gate, which is called the output gate.

1183
01:08:19,960 --> 01:08:22,681
And here we're doing again the same thing over and over.

1184
01:08:22,706 --> 01:08:26,314
So we are multiplying by the input
by the weight matrix, multiply the

1185
01:08:26,339 --> 01:08:32,960
previous hidden state by the weight
matrix and squishing through the sigmoid.

1186
01:08:32,960 --> 01:08:33,960
Okay.

1187
01:08:33,960 --> 01:08:37,269
So this is again another
gate and there's different

1188
01:08:37,294 --> 01:08:39,960
bunch of parameters, but
conceptually it's another gate.

1189
01:08:39,960 --> 01:08:43,425
So we have three gates, forget gate, input gate, and output gate.

1190
01:08:43,960 --> 01:08:44,960
And the output gate is going to control what?

1191
01:08:44,960 --> 01:08:49,960
It's going to control what we are going to output to the next hidden state.

1192
01:08:49,960 --> 01:08:58,960
So now we're taking this gate, we're taking the current memory, which we run through tanh.

1193
01:08:58,960 --> 01:09:00,960
Don't ask me why, I don't really know.

1194
01:09:00,960 --> 01:09:10,960
But basically we're running through tanh and multiply by this output gate, this Hadamard product, and this will be our next hidden state.

1195
01:09:10,960 --> 01:09:16,185
So basically there will be this
sort of recurrence from these

1196
01:09:16,210 --> 01:09:24,175
two states and these two
cells, or cells like the vectors.

1197
01:09:24,200 --> 01:09:27,822
And the last part, what is missing is to compute the output.

1198
01:09:27,847 --> 01:09:29,132
And the output is very simple.

1199
01:09:29,157 --> 01:09:32,407
It's just the hidden vector.

1200
01:09:32,432 --> 01:09:34,513
So this is what we're outputting here.

1201
01:09:34,538 --> 01:09:39,236
We're not outputting the full state,
we're not outputting the full kind

1202
01:09:39,261 --> 01:09:44,524
of memory and the hidden state,
we're just outputting the hidden state.

1203
01:09:44,549 --> 01:09:50,960
So this is the full LSTM, which looks kind of complicated, but there's three gates and the gates have a meaning.

1204
01:09:50,960 --> 01:09:56,544
So what they're doing, they're learning something about how much to forget, I mean, what to forget and whatnot.

1205
01:09:56,569 --> 01:09:59,690
And there's a couple of weird choices.

1206
01:09:59,715 --> 01:10:02,960
To be honest, I have to check why there is tanh, I don't know.

1207
01:10:03,582 --> 01:10:07,656
And everything is
differentiable and everything

1208
01:10:07,681 --> 01:10:09,960
kind of you can train
in computational graph.

1209
01:10:09,960 --> 01:10:11,960
But it looks complicated.

1210
01:10:11,960 --> 01:10:13,960
So that's why there's newer versions of GRUs.

1211
01:10:13,960 --> 01:10:14,960
Yes?

1212
01:10:14,960 --> 01:10:17,573
How is the state vector split?

1213
01:10:17,598 --> 01:10:21,458
The state vector is half and half.

1214
01:10:21,483 --> 01:10:23,960
Well, you would treat it as two vectors, basically.

1215
01:10:23,960 --> 01:10:29,960
You would treat this as, let me see, I'm coming to that in the next slide, okay?

1216
01:10:29,960 --> 01:10:36,960
So you would have c_j and h_j have the same hidden dimension.

1217
01:10:36,960 --> 01:10:40,960
So this d_h is dimensionally of the LSTM, like the hidden layer size.

1218
01:10:40,960 --> 01:10:45,666
And all of these, so this vector and this
vector have the same size, which means

1219
01:10:45,691 --> 01:10:48,960
the output vector has the same size and all
the gates have the same size, obviously.

1220
01:10:48,960 --> 01:10:52,302
And the candidate vector
also has the same size, which is

1221
01:10:52,327 --> 01:10:57,960
the size of the hidden layer,
like the parameter of the LSTM.

1222
01:10:57,960 --> 01:11:03,948
And why there is this transformation
also means that the dimensionality of

1223
01:11:03,972 --> 01:11:09,960
the inputs could be something else
than the internal workings of the LSTM.

1224
01:11:09,960 --> 01:11:15,960
Yes, you have a question?

1225
01:11:15,960 --> 01:11:23,508
Because you're keeping things here in the cell memory.

1226
01:11:23,533 --> 01:11:28,290
And basically, they call it memory highway.

1227
01:11:28,315 --> 01:11:34,960
Basically, it keeps the information stored over a longer kind of input.

1228
01:11:34,960 --> 01:11:36,960
How exactly to work?

1229
01:11:36,960 --> 01:11:37,960
I mean, I can't do the math right now.

1230
01:11:37,960 --> 01:11:38,960
I don't know.

1231
01:11:39,960 --> 01:11:46,954
There are definitely proofs saying, yeah, if you
have this memory, which is kept and updated, just

1232
01:11:46,979 --> 01:11:51,729
because you update only the information you need
at a point and not overwriting the full memory.

1233
01:11:51,754 --> 01:11:52,960
So maybe that's why.

1234
01:11:52,960 --> 01:11:57,276
But to be honest, I don't really know how exactly, why exactly that works as it is.

1235
01:11:57,301 --> 01:11:58,960
Yes, you have a question?

1236
01:11:58,960 --> 01:12:05,960
If you use the same parameters for every part, wouldn't you never update the memory?

1237
01:12:05,960 --> 01:12:14,779
Like, that one at the beginning, and then it's never overwritten because the gate never allowed it to be overwritten at any later point?

1238
01:12:14,804 --> 01:12:15,960
I don't understand.

1239
01:12:15,960 --> 01:12:20,046
Like, if you, for example, if
you have a gate or a vector that's

1240
01:12:20,070 --> 01:12:24,611
like 001, and the first two are like
the memory that gets overwritten

1241
01:12:24,635 --> 01:12:29,181
by the working memory by the
next state, then whatever you set

1242
01:12:29,206 --> 01:12:32,960
at the beginning of the memory
will just never get overwritten.

1243
01:12:32,960 --> 01:12:37,509
Because every time it gets
to that, it's like, yes, we get

1244
01:12:37,534 --> 01:12:40,960
to the next vector 00, so we
don't write anything to memory.

1245
01:12:40,960 --> 01:12:43,597
We just write our last part
to working memory, and then

1246
01:12:43,622 --> 01:12:45,960
we get to the next part,
and then it happens again.

1247
01:12:45,960 --> 01:12:48,960
So the memory is just never changed at all.

1248
01:12:48,960 --> 01:12:53,960
Sure, but in general, it should be learnable through, I mean, you're penalized that.

1249
01:12:53,960 --> 01:12:58,478
I mean, if it doesn't learn anything, you penalize it by a huge loss.

1250
01:12:58,960 --> 01:13:01,960
I feel like that's inherent with the gate mechanism.

1251
01:13:01,960 --> 01:13:09,622
If you have a memory that's like, if you have a memory gate, that's the same for every single...

1252
01:13:09,647 --> 01:13:12,960
Yeah, but it's parametrized by the, so look at this.

1253
01:13:12,960 --> 01:13:16,488
I mean, the memory gate is parametrized by the current input.

1254
01:13:16,513 --> 01:13:17,960
It's a point.

1255
01:13:17,960 --> 01:13:21,960
So whatever comes in, it will change the gate.

1256
01:13:21,960 --> 01:13:25,016
So if you have different, I mean, and
you have different words, like different

1257
01:13:25,041 --> 01:13:30,960
inputs at each position, so the gate will
be different based on the word coming in.

1258
01:13:30,960 --> 01:13:32,960
So it won't be the same.

1259
01:13:32,960 --> 01:13:33,960
I mean, you don't have a static gate.

1260
01:13:33,960 --> 01:13:36,628
You have a learnable
gate, and the learnable is

1261
01:13:36,652 --> 01:13:39,960
conditioned on the input
and the previous hidden state.

1262
01:13:39,960 --> 01:13:40,960
That's the trick.

1263
01:13:40,960 --> 01:13:42,960
Like, you're taking the full context.

1264
01:13:42,960 --> 01:13:49,388
So here, you have the full context of the previous things, because it gets to here.

1265
01:13:49,413 --> 01:13:50,960
So this is the recurrent part.

1266
01:13:50,960 --> 01:13:52,407
So you're running this over and over.

1267
01:13:52,432 --> 01:13:53,960
You keep something in the working memory.

1268
01:13:53,960 --> 01:13:55,920
You keep something here as well.

1269
01:13:55,959 --> 01:13:59,353
But OK, for the gates, you're taking
the previous, all the information from

1270
01:13:59,378 --> 01:14:05,935
previous steps, which, by the way, depends
also recursively on everything, right?

1271
01:14:05,960 --> 01:14:10,233
Because the h_j takes the whole thing here, like

1272
01:14:10,258 --> 01:14:12,960
the long-term memory
and the working memory.

1273
01:14:12,960 --> 01:14:15,960
So you're conditioned on that and conditioned on the input.

1274
01:14:15,960 --> 01:14:18,960
So you will output different gates for that.

1275
01:14:19,730 --> 01:14:20,452
Yeah.

1276
01:14:22,199 --> 01:14:23,606
Sorry, where are we?

1277
01:14:24,526 --> 01:14:24,932
Yes.

1278
01:14:24,957 --> 01:14:25,957
One more question.

1279
01:14:25,982 --> 01:14:28,960
Do we have a round-covered gate with 0 and 1?

1280
01:14:28,960 --> 01:14:29,960
No.

1281
01:14:29,960 --> 01:14:30,960
No, no, no.

1282
01:14:30,960 --> 01:14:32,960
It's a you don't.

1283
01:14:32,960 --> 01:14:37,960
I mean, you want them to learn that they are very close to 0, very close to 1.

1284
01:14:37,960 --> 01:14:38,960
Yeah.

1285
01:14:38,960 --> 01:14:42,960
And if you think about the sigmoid, you will end up there.

1286
01:14:42,960 --> 01:14:46,960
So it will be like very soft, but mostly 0 and mostly 1.

1287
01:14:46,960 --> 01:14:52,960
So you just accept that our safe operation takes away the bit of the...

1288
01:14:52,960 --> 01:14:53,960
Exactly, yes, yes, exactly.

1289
01:14:53,960 --> 01:14:54,960
It's soft.

1290
01:14:54,960 --> 01:14:59,960
So you have something, you know, you have very little addition from everything.

1291
01:14:59,960 --> 01:15:02,960
But you prefer those which are kind of very close to 1.

1292
01:15:02,960 --> 01:15:09,680
I mean, they're got, you know, yeah, they're weighted by basically by these 0s and 1s.

1293
01:15:09,705 --> 01:15:15,050
OK, another question?

1294
01:15:15,075 --> 01:15:19,808
So, OK, here, basically, as a layer, so this is, again, the abstraction we had before.

1295
01:15:19,833 --> 01:15:20,960
So the recurrent abstraction.

1296
01:15:20,960 --> 01:15:28,960
So we split the split the state vector into two parts, which could be concatenated here.

1297
01:15:28,960 --> 01:15:33,960
But there are different two different vectors basically running through the state, through

1298
01:15:33,960 --> 01:15:36,566
the, sorry, through the LSTM layer.

1299
01:15:36,591 --> 01:15:41,960
And you have the input and basically the all the parameterization are all these different matrices.

1300
01:15:41,960 --> 01:15:51,960
So for each of the gates, for output gates, input gate, forget gate.

1301
01:15:51,960 --> 01:15:53,960
And this is the update candidate.

1302
01:15:53,960 --> 01:15:56,698
We also didn't add the bias term, but you can

1303
01:15:56,722 --> 01:15:59,960
have, theoretically, you can have in all of those,

1304
01:15:59,960 --> 01:16:06,960
you can have a bias term here, plus bias term, plus bias term, plus bias term.

1305
01:16:06,960 --> 01:16:09,960
So you have more parameters if you really want.

1306
01:16:09,960 --> 01:16:14,960
I didn't put them there, but, you know, just ignore them for clarity.

1307
01:16:14,960 --> 01:16:17,948
And this is it. Basically,
you have this R. These were

1308
01:16:17,972 --> 01:16:20,960
R, you know, R and O
functions, but in a very fancy way.

1309
01:16:20,960 --> 01:16:23,504
So we have gates and
we have all these products

1310
01:16:23,528 --> 01:16:25,960
and stuff like that,
very many parameters.

1311
01:16:25,960 --> 01:16:26,960
But they're there.

1312
01:16:26,960 --> 01:16:27,960
Yes.

1313
01:16:27,960 --> 01:16:29,960
So you're basically learning the gates?

1314
01:16:29,960 --> 01:16:30,960
Yes.

1315
01:16:30,960 --> 01:16:31,960
As we go?

1316
01:16:31,960 --> 01:16:34,960
Yes. You learn the gates as you go.

1317
01:16:34,960 --> 01:16:38,960
Because the gates are just the gates are vectors.

1318
01:16:38,960 --> 01:16:41,924
So you learn the projections
based on based on your

1319
01:16:41,949 --> 01:16:43,960
input and previous, the
input and previous state.

1320
01:16:43,960 --> 01:16:47,960
You're learning gates. You learn what to remember and learn what to forget.

1321
01:16:47,960 --> 01:16:50,960
This is the, this is the power of these LSTMs.

1322
01:16:50,960 --> 01:16:53,960
Any other question?

1323
01:16:53,960 --> 01:16:57,960
Good. So.

1324
01:16:57,960 --> 01:17:00,643
It was fast. I think I had some questions for you, but

1325
01:17:00,668 --> 01:17:02,960
I forgot all of them because I mean, it's complicated.

1326
01:17:02,960 --> 01:17:05,960
So the question is, like, should I remember all of these?

1327
01:17:05,960 --> 01:17:08,960
I don't know. You should.

1328
01:17:08,960 --> 01:17:12,267
I don't remember it either by heart, like all these tiny details,

1329
01:17:12,291 --> 01:17:15,960
but you should remember there are these gates and what they're doing.

1330
01:17:15,960 --> 01:17:17,960
Why are there?

1331
01:17:17,985 --> 01:17:22,985
And conceptually, how it works.

1332
01:17:23,954 --> 01:17:29,960
The point is, so recap, we have RNNs for arbitrarily long input.

1333
01:17:29,960 --> 01:17:31,960
So this is the big advantage of them.

1334
01:17:31,960 --> 01:17:36,195
And they can encode
the entire sequence and

1335
01:17:36,220 --> 01:17:38,960
basically each step of
that, as we saw before.

1336
01:17:38,960 --> 01:17:42,201
We have the bidirectional
RNNs, so we can model from left to

1337
01:17:42,226 --> 01:17:44,960
right, right to left and combine
together, which is great.

1338
01:17:44,960 --> 01:17:49,678
And we have the gating mechanism to effectively work with the memory cells.

1339
01:17:49,703 --> 01:17:54,960
And LSTM is a particle-powerful RNN. You might want to look up GRUs.

1340
01:17:54,960 --> 01:17:58,960
So this is from, I guess, 2012, 2013, maybe.

1341
01:17:58,960 --> 01:18:02,504
And it's, it's quite simpler than LSTM, but LSTM is nice because

1342
01:18:02,528 --> 01:18:05,960
it shows the gating mechanism, you know, in its full power.

1343
01:18:05,960 --> 01:18:07,960
Why is it important?

1344
01:18:07,960 --> 01:18:11,960
And we're really fast today, so I don't know.

1345
01:18:11,960 --> 01:18:16,948
Anyway, so thanks a lot. And I'll, next week, actually, I'll be here as well with

1346
01:18:16,972 --> 01:18:21,960
Martin, but Martin will take over and we'll be talking about generation with RNNs.

1347
01:18:21,960 --> 01:18:23,960
So thanks a lot.

