1
00:03:38,900 --> 00:03:42,460
Okay, we're complete.

2
00:03:42,460 --> 00:03:43,740
We're sold out.

3
00:03:43,740 --> 00:03:45,180
That's great.

4
00:03:45,180 --> 00:03:48,820
Welcome to Deep Learning for NLP, 2023 edition.

5
00:03:48,820 --> 00:03:50,980
My name is Ivan Habernal, and this is Martin Dutek.

6
00:03:50,980 --> 00:03:53,340
I'll introduce ourselves later on again.

7
00:03:53,340 --> 00:03:54,780
Come in, yes, come in.

8
00:03:55,900 --> 00:03:57,620
There's still free seats.

9
00:03:57,620 --> 00:03:59,820
So you have to be creative, but you'll find.

10
00:04:00,940 --> 00:04:04,340
This is not the biggest room at university,

11
00:04:04,340 --> 00:04:06,080
and this is not definitely the best room

12
00:04:06,080 --> 00:04:08,800
for actually seeing something on the screen

13
00:04:09,520 --> 00:04:11,520
just from the middle, I'm sorry about it.

14
00:04:11,520 --> 00:04:15,180
I complained, and they said, I don't know,

15
00:04:15,180 --> 00:04:18,920
something like, yeah, we're short of lecture rooms

16
00:04:18,920 --> 00:04:19,840
or whatever.

17
00:04:19,840 --> 00:04:22,420
So there is only one cool thing about this lecture hall

18
00:04:22,420 --> 00:04:24,120
is it has air condition.

19
00:04:24,120 --> 00:04:26,240
So in summer, we love it.

20
00:04:26,240 --> 00:04:28,000
I can guarantee you, okay?

21
00:04:29,260 --> 00:04:32,080
But that's the only advantage of this room, I'm sorry.

22
00:04:32,080 --> 00:04:33,620
Okay, so let's get started.

23
00:04:33,620 --> 00:04:35,680
Can you see something at least on the slides?

24
00:04:35,680 --> 00:04:38,680
If you can see anything on the slides,

25
00:04:39,520 --> 00:04:41,480
take your note, iPad, whatever you have,

26
00:04:41,480 --> 00:04:43,020
and you can download the slides

27
00:04:43,020 --> 00:04:45,720
and just have a local copy so you can read locally.

28
00:04:47,280 --> 00:04:49,120
I'll show you where to get it.

29
00:04:49,120 --> 00:04:52,040
So, okay, let's start with some motivation

30
00:04:52,040 --> 00:04:52,920
while we're here.

31
00:04:52,920 --> 00:04:54,580
So deep learning for NLP.

32
00:04:55,720 --> 00:04:56,560
Let me ask a question.

33
00:04:56,560 --> 00:04:59,760
So who has studied something with NLP,

34
00:04:59,760 --> 00:05:00,780
natural language processing?

35
00:05:00,780 --> 00:05:03,440
Raise your hand, natural language processing.

36
00:05:03,440 --> 00:05:06,260
Okay, cool, thanks.

37
00:05:06,260 --> 00:05:07,240
Deep learning.

38
00:05:08,240 --> 00:05:10,080
Wow.

39
00:05:10,080 --> 00:05:11,480
Okay, cool, why are you here?

40
00:05:13,800 --> 00:05:14,640
Okay, wait.

41
00:05:16,180 --> 00:05:17,020
Yeah, that's it.

42
00:05:17,020 --> 00:05:20,440
Machine learning, it's deep learning, yeah, okay, cool.

43
00:05:20,440 --> 00:05:22,400
So why are we here?

44
00:05:22,400 --> 00:05:26,120
What's your motivation to be here if you studied already?

45
00:05:26,120 --> 00:05:28,280
So those people who study deep learning and NLP,

46
00:05:28,280 --> 00:05:29,120
why are you here?

47
00:05:32,560 --> 00:05:35,840
I studied deep learning, specifically for NLP.

48
00:05:35,840 --> 00:05:38,000
Okay, is there something like specific deep learning

49
00:05:38,000 --> 00:05:38,840
for NLP?

50
00:05:38,840 --> 00:05:40,840
Well, I hope so, I hope so.

51
00:05:40,840 --> 00:05:42,040
But times are changing.

52
00:05:42,040 --> 00:05:44,140
Any other motivation to be here?

53
00:05:45,060 --> 00:05:48,140
Because I listened to podcast with Richard Socher.

54
00:05:48,140 --> 00:05:48,980
Okay.

55
00:05:48,980 --> 00:05:51,540
And I was like, yeah, I want to know more.

56
00:05:51,540 --> 00:05:52,380
Oh, that's cool.

57
00:05:52,380 --> 00:05:54,480
Okay, so you listened to podcast with Richard Socher.

58
00:05:54,480 --> 00:05:57,920
Okay, Richard, Richard Socher, Socher, he's German.

59
00:05:57,920 --> 00:06:01,880
Okay, cool, so the motivation why to be here,

60
00:06:01,880 --> 00:06:04,180
there's so many things, but this is it.

61
00:06:06,840 --> 00:06:08,960
GPT-4, is there anybody in the room

62
00:06:08,960 --> 00:06:11,880
who hasn't used JetGPT or GPT-4 or JetGPT?

63
00:06:11,880 --> 00:06:13,800
Anyone haven't heard of JetGPT?

64
00:06:14,860 --> 00:06:17,240
So I don't think we need more motivation for this course.

65
00:06:17,240 --> 00:06:19,040
We need to understand the thing.

66
00:06:19,040 --> 00:06:20,440
We need to learn it.

67
00:06:20,440 --> 00:06:23,540
After this course, you should be able to build it,

68
00:06:23,540 --> 00:06:24,600
but not to train it.

69
00:06:25,720 --> 00:06:28,800
Unless you work for big companies like Google, Meta,

70
00:06:28,800 --> 00:06:31,840
or the UKP lab, or my lab.

71
00:06:31,840 --> 00:06:33,480
No, well, we don't have so many resources,

72
00:06:33,480 --> 00:06:37,080
but the goal is to learn more about this

73
00:06:37,080 --> 00:06:39,580
and understand it as much as possible.

74
00:06:40,960 --> 00:06:43,320
So we stitched together a little bit of roadmap,

75
00:06:44,360 --> 00:06:46,880
and this is something which will update a little bit,

76
00:06:46,880 --> 00:06:48,560
but here you have a little bit of overview

77
00:06:48,560 --> 00:06:51,320
what's coming up to you, so you can look it up later on.

78
00:06:51,320 --> 00:06:55,960
And we're trying to pack as much as possible,

79
00:06:55,960 --> 00:06:58,180
and we'll have two guest lectures at the end,

80
00:06:58,180 --> 00:06:59,840
and we'll have some buffer as well.

81
00:06:59,840 --> 00:07:03,160
But this is something which roughly will be around.

82
00:07:03,680 --> 00:07:06,680
So first, logistics, what we're gonna do.

83
00:07:06,680 --> 00:07:09,400
So we're two lecturers, and I'm the first one.

84
00:07:09,400 --> 00:07:11,360
I'm Ivan, nice to meet you.

85
00:07:11,360 --> 00:07:12,780
I'm leading the TrustLHLT group,

86
00:07:12,780 --> 00:07:13,840
which I'll introduce later on.

87
00:07:13,840 --> 00:07:16,520
And it's my pleasure to introduce you also,

88
00:07:16,520 --> 00:07:17,360
Martin Tutek.

89
00:07:19,220 --> 00:07:20,060
Say hi.

90
00:07:21,320 --> 00:07:22,160
Oh, okay.

91
00:07:24,040 --> 00:07:29,040
And Martin is a postdoc at the UKP lab from Irina Gurevich,

92
00:07:29,680 --> 00:07:32,480
and we're jointly basically running this course

93
00:07:32,480 --> 00:07:35,000
with a couple of other people, the tutors,

94
00:07:35,000 --> 00:07:39,400
so Martin, Mingwu, Doanam, Yanran, and Hatice,

95
00:07:39,400 --> 00:07:43,760
who will be helping us with the exercises and homework,

96
00:07:43,760 --> 00:07:45,480
so I'll get to that.

97
00:07:45,480 --> 00:07:47,720
So, and we'll be splitting the lectures, basically.

98
00:07:47,720 --> 00:07:49,940
I'll take the first six lectures,

99
00:07:49,940 --> 00:07:52,520
then Martin will take over and take the rest,

100
00:07:52,520 --> 00:07:55,720
like seven until 12, and then we'll have this guest lecture.

101
00:07:55,720 --> 00:07:57,020
This is the plan, roughly.

102
00:07:58,120 --> 00:07:59,980
Great, any questions so far?

103
00:07:59,980 --> 00:08:03,020
That's not a case.

104
00:08:03,020 --> 00:08:03,860
If you wanna sit down,

105
00:08:03,860 --> 00:08:05,280
we have really a couple of places somewhere,

106
00:08:05,280 --> 00:08:06,680
but you just have to be creative

107
00:08:06,680 --> 00:08:08,940
and somehow push the people a little bit.

108
00:08:09,900 --> 00:08:11,060
Sorry, yeah.

109
00:08:11,920 --> 00:08:12,780
So let's move on.

110
00:08:12,780 --> 00:08:16,780
Okay, so this is a live course, excuse me.

111
00:08:17,820 --> 00:08:19,900
So we're doing live lectures,

112
00:08:19,900 --> 00:08:23,820
but also we have a couple of online resources

113
00:08:23,820 --> 00:08:28,080
to make it more kind of bringing to the 21st century.

114
00:08:29,040 --> 00:08:31,200
So obviously, we have Moodle.

115
00:08:31,200 --> 00:08:35,840
I'll be, the user interface from 2007 is still here,

116
00:08:35,840 --> 00:08:38,040
so I think it's cool again, like the 90s.

117
00:08:38,040 --> 00:08:39,660
So we have Moodle.

118
00:08:39,660 --> 00:08:41,540
It's horrible, but it's official and it works,

119
00:08:41,540 --> 00:08:43,900
so go there and sign up for Moodle,

120
00:08:43,900 --> 00:08:46,520
because all the announcements will be done there.

121
00:08:46,520 --> 00:08:47,920
There's discussion forums,

122
00:08:47,920 --> 00:08:49,640
so you can ask questions there as well.

123
00:08:49,640 --> 00:08:53,360
We'll use Moodle mostly for the exercises and the homeworks.

124
00:08:53,360 --> 00:08:56,520
The lectures are, this is the next point,

125
00:08:56,520 --> 00:08:58,160
lectures are on GitHub.

126
00:08:58,160 --> 00:09:02,200
So all these PDFs are open source on GitHub,

127
00:09:02,200 --> 00:09:04,840
so you can download the PDF, the LaTeX code if you want.

128
00:09:04,840 --> 00:09:07,140
You can just use it whatever reasons you want.

129
00:09:07,140 --> 00:09:10,220
This is Creative Commons by license.

130
00:09:11,200 --> 00:09:13,880
And we also bring it to the 2023,

131
00:09:13,880 --> 00:09:16,520
so I set up the Discord channel.

132
00:09:16,520 --> 00:09:17,920
So sign up to the Discord.

133
00:09:17,920 --> 00:09:19,560
I mean, all these links are in the slides as well,

134
00:09:19,560 --> 00:09:22,240
so you can download the slides and have the links.

135
00:09:22,240 --> 00:09:26,240
Go to Discord, which brings the user experience

136
00:09:26,240 --> 00:09:29,760
really to 2023, and we'll use it for basically,

137
00:09:29,760 --> 00:09:31,440
I don't know, like addressing your questions,

138
00:09:31,440 --> 00:09:33,080
maybe some announcements.

139
00:09:33,080 --> 00:09:35,300
It's more unofficial, but don't be nasty, okay?

140
00:09:35,300 --> 00:09:37,960
So it's like your typical Discord channel.

141
00:09:37,960 --> 00:09:39,080
I don't know what you do on Discord,

142
00:09:39,080 --> 00:09:42,660
but you can use nicknames, I don't mind,

143
00:09:42,660 --> 00:09:45,120
but just behave wisely.

144
00:09:46,640 --> 00:09:49,200
You can reach us on one of those things,

145
00:09:49,200 --> 00:09:50,840
but typically not on Discord on the weekend.

146
00:09:50,840 --> 00:09:54,040
So if you ping me on Discord,

147
00:09:54,040 --> 00:09:56,760
the day before the exam, it's gonna be Sunday.

148
00:09:56,760 --> 00:09:57,600
I'm sorry.

149
00:09:59,320 --> 00:10:00,740
Also, we're gonna, I mean,

150
00:10:00,740 --> 00:10:03,060
the plan is that we record the lectures

151
00:10:03,060 --> 00:10:05,300
and put them on YouTube, right?

152
00:10:05,300 --> 00:10:08,320
So I'll post it somewhere to Moodle then,

153
00:10:08,320 --> 00:10:10,640
the link to the YouTube lectures.

154
00:10:10,640 --> 00:10:12,720
And the thing is,

155
00:10:14,720 --> 00:10:16,200
you know, it's better to participate here

156
00:10:16,200 --> 00:10:18,480
because you can make it more interactive.

157
00:10:18,480 --> 00:10:20,780
At least if you have questions, you can ask here.

158
00:10:20,780 --> 00:10:23,760
And also on YouTube, I mean, I'm using microphone here,

159
00:10:24,340 --> 00:10:25,180
so you will hear me,

160
00:10:25,180 --> 00:10:26,240
but you won't hear the questions from the audience

161
00:10:26,240 --> 00:10:27,360
or the discussion from the audience,

162
00:10:27,360 --> 00:10:29,440
which is kind of another great experience.

163
00:10:29,440 --> 00:10:32,720
So the idea is to put it on YouTube for your reference,

164
00:10:32,720 --> 00:10:33,960
if you wanna watch it later,

165
00:10:33,960 --> 00:10:36,840
maybe just to refresh a couple of things.

166
00:10:36,840 --> 00:10:38,800
So, and because we're on YouTube,

167
00:10:38,800 --> 00:10:40,380
you know, subscribe, like.

168
00:10:41,960 --> 00:10:43,160
No, I mean, it makes me happy.

169
00:10:43,160 --> 00:10:44,000
You know, if I see a like,

170
00:10:44,000 --> 00:10:46,040
you know, if I put something on Moodle,

171
00:10:46,040 --> 00:10:47,840
it's like, ah, yeah.

172
00:10:47,840 --> 00:10:50,040
If I post it on Discord and I see like a like,

173
00:10:50,040 --> 00:10:52,720
you know, my brain starts, you know,

174
00:10:52,720 --> 00:10:55,000
I get, how's it called?

175
00:10:55,000 --> 00:10:56,520
Endorphins, you know, release,

176
00:10:56,520 --> 00:10:58,280
and it makes me happy, you know,

177
00:10:58,280 --> 00:11:00,120
because it's social media in the works.

178
00:11:00,120 --> 00:11:03,240
Anyway, so YouTube channels, it's the same thing, you know.

179
00:11:03,240 --> 00:11:06,080
Anyway, but we're not doing it

180
00:11:06,080 --> 00:11:08,440
for the sake of getting likes on YouTube.

181
00:11:09,880 --> 00:11:12,280
So resources, we'll, okay,

182
00:11:12,280 --> 00:11:14,480
there is no textbook for the whole course.

183
00:11:14,480 --> 00:11:16,340
There's two reasons for that.

184
00:11:16,340 --> 00:11:18,080
Deep learning for NLP or deep learning,

185
00:11:18,080 --> 00:11:21,160
the field is evolving so rapidly that, you know,

186
00:11:21,160 --> 00:11:23,240
if you write a textbook two years later,

187
00:11:23,240 --> 00:11:25,200
you can use half of it.

188
00:11:25,200 --> 00:11:28,560
Second, we're using freely available resources mostly.

189
00:11:28,560 --> 00:11:32,460
So everything we use, you can get for free legally.

190
00:11:33,920 --> 00:11:36,440
And basically the top-notch research

191
00:11:36,440 --> 00:11:38,280
in natural language processing is also open source.

192
00:11:38,280 --> 00:11:42,360
So basically the community of research in NLP

193
00:11:42,360 --> 00:11:46,240
is kind of centered around the Association

194
00:11:46,240 --> 00:11:48,700
for Competition Linguistics, shortly ACL,

195
00:11:48,700 --> 00:11:50,560
and they run a couple of conferences,

196
00:11:50,840 --> 00:11:52,840
and the ACL conference, NACL, and so on.

197
00:11:52,840 --> 00:11:55,960
So you'll see citations to those conferences

198
00:11:55,960 --> 00:12:00,840
and all the papers are kind of freely available there

199
00:12:00,840 --> 00:12:03,460
in the anthology or archives.

200
00:12:03,460 --> 00:12:05,600
So we're gonna use basically public resources.

201
00:12:05,600 --> 00:12:08,400
If there's something which you need to read,

202
00:12:08,400 --> 00:12:09,240
I'll let you know.

203
00:12:09,240 --> 00:12:12,300
Most of the links, the references are just

204
00:12:12,300 --> 00:12:14,240
that you can really go to the sources

205
00:12:14,240 --> 00:12:17,600
and get more material if you're interested in topic.

206
00:12:17,600 --> 00:12:18,440
Any questions?

207
00:12:21,320 --> 00:12:22,160
Good.

208
00:12:23,600 --> 00:12:25,040
Exercises and homeworks.

209
00:12:25,040 --> 00:12:26,360
So we have exercises,

210
00:12:26,360 --> 00:12:29,600
which is something that you do on your own,

211
00:12:29,600 --> 00:12:32,860
and you should deepen your understanding of the matter,

212
00:12:32,860 --> 00:12:34,360
and this is not great.

213
00:12:34,360 --> 00:12:36,840
This is something for you to take it

214
00:12:36,840 --> 00:12:38,480
from a little bit different angle

215
00:12:38,480 --> 00:12:40,560
and try something hands-on,

216
00:12:40,560 --> 00:12:42,800
which is related to the topics we're discussing.

217
00:12:44,060 --> 00:12:48,220
If you wanna use JetGPT for exercises, do it.

218
00:12:48,220 --> 00:12:49,960
I'm fine.

219
00:12:49,960 --> 00:12:52,840
Will you learn something from using JetGPT for exercises?

220
00:12:54,680 --> 00:12:58,520
I don't know, but it's a nice experience.

221
00:12:58,520 --> 00:12:59,720
Use whatever you choose you want.

222
00:12:59,720 --> 00:13:02,200
If you wanna cheat the whole course, you can do it easily.

223
00:13:02,200 --> 00:13:03,080
That's fine.

224
00:13:03,080 --> 00:13:04,400
Maybe except for the exam.

225
00:13:04,400 --> 00:13:05,360
I'm not sure.

226
00:13:05,360 --> 00:13:07,080
I haven't seen so many people cheating on the exam

227
00:13:07,080 --> 00:13:09,080
because I use the same exam.

228
00:13:09,080 --> 00:13:09,920
Can I say it?

229
00:13:14,440 --> 00:13:18,860
I used the same exam over maybe two years, maybe,

230
00:13:18,860 --> 00:13:21,780
and the distribution of errors was most of the same.

231
00:13:23,360 --> 00:13:25,140
So that's fine, but don't cheat.

232
00:13:25,140 --> 00:13:26,620
I mean, it's up to you.

233
00:13:26,620 --> 00:13:28,420
You have to know why you're here.

234
00:13:28,420 --> 00:13:29,840
Then we have homeworks.

235
00:13:29,840 --> 00:13:34,620
Yeah, this is maybe a bigger issue for homeworks.

236
00:13:34,620 --> 00:13:37,480
So it's basically some programming exercises

237
00:13:37,480 --> 00:13:39,940
where you really, really, really get your hands dirty,

238
00:13:39,940 --> 00:13:42,380
and you'll be doing something, some programming.

239
00:13:44,660 --> 00:13:48,420
It's a bigger class, so we have groups of two.

240
00:13:48,940 --> 00:13:50,820
So you have to pair up with somebody

241
00:13:50,820 --> 00:13:53,200
will post the logistic exactly on Moodle.

242
00:13:54,120 --> 00:13:56,320
And this is graded.

243
00:13:56,320 --> 00:14:01,160
And if you achieve over roughly 70% of the points,

244
00:14:01,160 --> 00:14:03,220
you will get a bonus for the exam.

245
00:14:03,220 --> 00:14:04,060
It's not much.

246
00:14:04,060 --> 00:14:07,860
So basically it's for you to get them dirty hands

247
00:14:07,860 --> 00:14:10,040
and try something, code something.

248
00:14:10,040 --> 00:14:11,580
And the bonus is just a bonus.

249
00:14:11,580 --> 00:14:13,120
I would recommend to do it anyway,

250
00:14:13,120 --> 00:14:14,780
because you learn something.

251
00:14:14,780 --> 00:14:17,500
If you wanna use GPT for that, I'm fine.

252
00:14:17,500 --> 00:14:19,780
But maybe you won't learn that much.

253
00:14:19,780 --> 00:14:21,660
So if you do it only for the bonus points,

254
00:14:21,660 --> 00:14:23,380
then I would say like, let it go.

255
00:14:24,400 --> 00:14:26,120
So this is not set up yet.

256
00:14:26,120 --> 00:14:28,660
How exactly, how many exercises and so on,

257
00:14:28,660 --> 00:14:31,420
exercises and homeworks, because we're just starting

258
00:14:31,420 --> 00:14:34,500
and we have the team ready with tutors.

259
00:14:34,500 --> 00:14:35,740
We're meeting this week to set it up.

260
00:14:35,740 --> 00:14:38,760
So there will be an exercise, I guess, next week,

261
00:14:38,760 --> 00:14:41,900
the first one, and then the homeworks will follow, okay?

262
00:14:41,900 --> 00:14:42,840
Any questions?

263
00:14:44,180 --> 00:14:45,020
Yes.

264
00:14:45,220 --> 00:14:49,760
So the exercises will be roughly weekly.

265
00:14:49,760 --> 00:14:52,740
Homeworks will be overall, I guess,

266
00:14:52,740 --> 00:14:54,420
four or five homeworks in total.

267
00:14:54,420 --> 00:14:56,060
So not a weekly basis.

268
00:14:56,060 --> 00:14:59,020
Some will be bigger programming things, some shorter.

269
00:15:00,260 --> 00:15:01,360
Another question, yes.

270
00:15:04,020 --> 00:15:05,820
No, do it on your own.

271
00:15:05,820 --> 00:15:08,140
So it's just single.

272
00:15:09,580 --> 00:15:11,200
These are mostly like theoretical

273
00:15:11,200 --> 00:15:13,960
or you just read something and make up, you know,

274
00:15:13,960 --> 00:15:15,440
trying to address an issue

275
00:15:16,340 --> 00:15:19,120
instead of just really programming the project, okay?

276
00:15:20,040 --> 00:15:20,860
Anything else?

277
00:15:23,240 --> 00:15:24,060
Good.

278
00:15:25,200 --> 00:15:26,120
Final exam.

279
00:15:27,040 --> 00:15:27,940
Save the date.

280
00:15:27,940 --> 00:15:31,240
It's Monday, July 31.

281
00:15:31,240 --> 00:15:36,240
So 31st at leaked visa.

282
00:15:37,880 --> 00:15:40,820
If you've never been to leaked visa, it's not really nice.

283
00:15:40,820 --> 00:15:43,540
The Mensa is as good as here,

284
00:15:44,460 --> 00:15:46,420
but they have this huge lecture hall.

285
00:15:46,420 --> 00:15:48,800
So there are four rooms, but it's one.

286
00:15:48,800 --> 00:15:52,220
And it fits, I guess, you know, a thousand people or so.

287
00:15:53,380 --> 00:15:55,120
So we'll be there.

288
00:15:55,120 --> 00:15:55,960
Yeah, come in.

289
00:15:57,500 --> 00:15:59,580
And it's just a standard exam.

290
00:15:59,580 --> 00:16:02,780
So you register to come and yeah, okay.

291
00:16:02,780 --> 00:16:04,940
The language, so you might have noticed

292
00:16:04,940 --> 00:16:06,860
this course will be held in English,

293
00:16:06,860 --> 00:16:10,060
at least for me, from Martin as well,

294
00:16:10,540 --> 00:16:12,260
the guest lectures from Timur as well.

295
00:16:12,260 --> 00:16:16,020
And maybe Thomas will do it in German, but I don't think so.

296
00:16:16,020 --> 00:16:18,500
I think everything will be in English, okay?

297
00:16:18,500 --> 00:16:22,420
So the final exam, the questions will be in English as well.

298
00:16:22,420 --> 00:16:27,100
And you can answer in any meaningful language you want.

299
00:16:27,100 --> 00:16:30,140
So English or German or something else,

300
00:16:30,140 --> 00:16:31,140
which we do not understand.

301
00:16:31,140 --> 00:16:34,420
I mean Czech, Croatian, and Timur speaks five languages.

302
00:16:34,420 --> 00:16:35,980
So you can choose your language, okay?

303
00:16:35,980 --> 00:16:38,120
But we don't speak Chinese, I'm sorry.

304
00:16:38,760 --> 00:16:41,580
So the, you know, how big the exam,

305
00:16:41,580 --> 00:16:44,760
so the exam basically, I think it took like an hour.

306
00:16:44,760 --> 00:16:45,680
Was it an hour?

307
00:16:47,160 --> 00:16:49,540
It takes an hour and it should be fun.

308
00:16:49,540 --> 00:16:51,620
I mean, people kind of get, you know,

309
00:16:51,620 --> 00:16:53,280
if you ask people from previous years, they will say,

310
00:16:53,280 --> 00:16:55,400
yeah, it was kind of fun, but hard.

311
00:16:55,400 --> 00:16:59,920
So don't ask, yeah, I mean, it can be easy, right?

312
00:16:59,920 --> 00:17:02,620
I mean, it makes no point, but it should be,

313
00:17:02,620 --> 00:17:03,520
it should make sense.

314
00:17:03,520 --> 00:17:07,600
And we tried to, we tried hard to make it somehow realistic.

315
00:17:08,400 --> 00:17:10,080
One thing is that I don't have exams

316
00:17:10,080 --> 00:17:12,320
because we reused the exams from last year.

317
00:17:12,320 --> 00:17:15,520
Don't ask me for exams, you know, from last year.

318
00:17:15,520 --> 00:17:18,040
I'm not sharing them because it takes a painful lot of time

319
00:17:18,040 --> 00:17:18,960
to create a new exam.

320
00:17:18,960 --> 00:17:21,320
So maybe we'll reuse a part of that.

321
00:17:21,320 --> 00:17:22,480
So if you ask me, oh,

322
00:17:22,480 --> 00:17:24,640
what is it's close available from this lecture?

323
00:17:24,640 --> 00:17:27,120
I say, I don't know.

324
00:17:27,120 --> 00:17:28,400
And that's true because I don't know,

325
00:17:28,400 --> 00:17:30,320
because we'll do the exam June

326
00:17:30,320 --> 00:17:32,520
and then we'll see how much we covered

327
00:17:32,520 --> 00:17:34,320
and what's kind of makes sense to ask you

328
00:17:34,320 --> 00:17:36,360
or what we're kind of expecting, you know,

329
00:17:36,360 --> 00:17:37,760
that you should understand.

330
00:17:38,640 --> 00:17:40,880
But the same thing we were saying in the lectures,

331
00:17:40,880 --> 00:17:43,280
like what, you know, what are you learning and why?

332
00:17:43,280 --> 00:17:44,480
Why is it important?

333
00:17:44,480 --> 00:17:46,200
Okay, any question to the exam?

334
00:17:52,160 --> 00:17:56,160
Good, so it will be standard paper writing exam.

335
00:17:56,160 --> 00:17:59,420
You don't need anything, no calculator, nothing.

336
00:18:00,360 --> 00:18:01,880
Okay, so this is not online.

337
00:18:01,880 --> 00:18:04,920
It will take some time to grade the exams,

338
00:18:05,000 --> 00:18:06,600
a couple of weeks in summer

339
00:18:06,600 --> 00:18:08,960
because you're a larger crowd.

340
00:18:11,240 --> 00:18:14,680
Obviously, this is not only my course or mine and Martin's,

341
00:18:14,680 --> 00:18:16,780
so it should be more interactive

342
00:18:16,780 --> 00:18:19,240
and your feedback matters actually.

343
00:18:19,240 --> 00:18:23,480
So, you know, whatever you like or dislike, tell us.

344
00:18:24,360 --> 00:18:26,880
Put it on Discord, write in the forums, send us an email.

345
00:18:26,880 --> 00:18:28,680
Any questions you might have, yeah, come in.

346
00:18:28,680 --> 00:18:32,360
Any questions you might have, you can approach us easily.

347
00:18:33,360 --> 00:18:35,440
The email addresses are there as well,

348
00:18:35,440 --> 00:18:37,980
so if you want to send us email, that's fine.

349
00:18:37,980 --> 00:18:42,980
What we're trying to do is to post some anonymous feedback

350
00:18:45,840 --> 00:18:48,480
forms just to get feedback, you know,

351
00:18:48,480 --> 00:18:51,400
because it's important, like you never know

352
00:18:51,400 --> 00:18:53,240
whether you covered everything well or not,

353
00:18:53,240 --> 00:18:56,920
so you're trying to make it really more open source,

354
00:18:56,920 --> 00:18:58,400
you know, which means build it yourself,

355
00:18:58,400 --> 00:18:59,800
so your feedback matters.

356
00:19:00,640 --> 00:19:03,120
And we'll take it into account.

357
00:19:04,560 --> 00:19:06,880
And the same for slides, so if there's issues,

358
00:19:06,880 --> 00:19:08,720
typos, whatever in the slides, it's LaTeX,

359
00:19:08,720 --> 00:19:11,400
so who, okay, who doesn't know LaTeX here?

360
00:19:11,400 --> 00:19:14,360
Just, I'm curious, who doesn't know LaTeX?

361
00:19:14,360 --> 00:19:17,080
Fine, okay, cool, so the majority, great.

362
00:19:17,080 --> 00:19:18,900
So if you see something wrong in the slides,

363
00:19:18,900 --> 00:19:20,600
who doesn't have a GitHub account?

364
00:19:22,160 --> 00:19:24,060
GitHub account, everybody has one?

365
00:19:25,680 --> 00:19:26,900
Okay, that's brilliant, okay, cool.

366
00:19:26,900 --> 00:19:28,260
So whatever you see is wrong here,

367
00:19:28,260 --> 00:19:33,060
just open a issue on GitHub, any feedback issue on GitHub,

368
00:19:33,060 --> 00:19:35,820
pull request, whatever, that will be really awesome.

369
00:19:37,540 --> 00:19:40,580
All right, and I think we're through mostly.

370
00:19:40,580 --> 00:19:43,060
I'll shortly introduce my research group.

371
00:19:43,060 --> 00:19:45,260
Martin, if you wanna leave, then you feel free

372
00:19:46,600 --> 00:19:50,660
because you have a pretty tight deadline, so thanks a lot.

373
00:19:50,660 --> 00:19:53,260
Say hi to Martin, see you in five lectures.

374
00:19:53,700 --> 00:19:58,700
Okay, so a little bit of introduction, who are we?

375
00:20:00,020 --> 00:20:03,620
And Martin will introduce himself once he starts

376
00:20:03,620 --> 00:20:06,180
his lectures later on, and I'm gonna introduce

377
00:20:06,180 --> 00:20:09,020
a little bit of my group.

378
00:20:09,020 --> 00:20:11,580
It's called Trustworthy Human Language Technologies.

379
00:20:12,820 --> 00:20:13,660
What does it mean?

380
00:20:13,660 --> 00:20:15,620
It means many things, but mainly,

381
00:20:15,620 --> 00:20:17,140
we are dealing with privacy.

382
00:20:17,140 --> 00:20:18,920
So we're looking into privacy-preserving

383
00:20:18,920 --> 00:20:20,520
natural language processing.

384
00:20:20,520 --> 00:20:22,120
What does it mean to protect privacy of you?

385
00:20:22,120 --> 00:20:23,800
If you're part of the data somewhere

386
00:20:23,800 --> 00:20:25,560
and we're training with your data,

387
00:20:25,560 --> 00:20:29,800
can we protect your privacy if it's natural language text?

388
00:20:29,800 --> 00:20:32,120
So it's also a little bit tricky.

389
00:20:32,120 --> 00:20:34,920
What we're using is differential privacy,

390
00:20:35,880 --> 00:20:40,600
and deep learning is definitely the tool of the day,

391
00:20:40,600 --> 00:20:41,720
graph metrics and so on.

392
00:20:41,720 --> 00:20:42,560
So this is our research.

393
00:20:42,560 --> 00:20:43,960
If you're interested in what we're doing,

394
00:20:43,960 --> 00:20:46,040
just go to the website and look at the papers.

395
00:20:46,040 --> 00:20:47,680
We have maybe some talks as well.

396
00:20:48,680 --> 00:20:52,080
And the second part is argument mining that matters.

397
00:20:52,080 --> 00:20:53,000
So what does it mean?

398
00:20:53,000 --> 00:20:57,440
Well, if you argue,

399
00:20:57,440 --> 00:21:01,520
if you argue on the internet, mostly it doesn't matter,

400
00:21:01,520 --> 00:21:05,760
but if you argue in front of the court on the case,

401
00:21:05,760 --> 00:21:06,840
maybe it matters more.

402
00:21:06,840 --> 00:21:10,200
So we're looking into arguments at the court,

403
00:21:10,200 --> 00:21:14,120
so how people argue in front of the European Court

404
00:21:14,120 --> 00:21:15,480
for Human Rights, for instance,

405
00:21:15,480 --> 00:21:17,080
and we're trying to analyze the arguments,

406
00:21:17,320 --> 00:21:19,880
the patterns from the legal perspective as well.

407
00:21:19,880 --> 00:21:23,040
So this is more interdisciplinary research,

408
00:21:23,040 --> 00:21:24,920
legal NLP, legal argument mining,

409
00:21:26,360 --> 00:21:27,720
which means also,

410
00:21:27,720 --> 00:21:30,360
if you're interested in doing master's thesis,

411
00:21:30,360 --> 00:21:34,360
or if you're interested in doing a heavy job,

412
00:21:34,360 --> 00:21:37,480
like a student research assistant, just get in touch.

413
00:21:37,480 --> 00:21:38,360
We have a couple of postings.

414
00:21:38,360 --> 00:21:40,360
We're desperately always looking for people

415
00:21:40,360 --> 00:21:41,760
who are motivated to learn something

416
00:21:41,760 --> 00:21:43,200
and do something for research.

417
00:21:44,360 --> 00:21:45,200
Talk to us.

418
00:21:45,200 --> 00:21:46,040
We have a couple of postings.

419
00:21:46,040 --> 00:21:46,880
Check out the website.

420
00:21:47,520 --> 00:21:50,520
We're just looking now for at least three persons now

421
00:21:50,520 --> 00:21:52,160
who can do some coding

422
00:21:52,160 --> 00:21:54,120
or people who actually speak more languages

423
00:21:54,120 --> 00:21:56,800
because we're doing, for the ECHR project,

424
00:21:56,800 --> 00:21:59,720
we need people speaking French, I guess,

425
00:21:59,720 --> 00:22:00,960
or maybe other languages,

426
00:22:01,920 --> 00:22:05,120
and doing some data analysis there.

427
00:22:06,520 --> 00:22:08,160
So get in touch.

428
00:22:08,160 --> 00:22:09,120
Any questions?

429
00:22:10,760 --> 00:22:11,680
Yes.

430
00:22:11,680 --> 00:22:14,600
Are the trust NLP, is that part of UKT

431
00:22:14,600 --> 00:22:15,680
or is it a different path?

432
00:22:15,680 --> 00:22:16,760
That's a great question.

433
00:22:17,520 --> 00:22:18,360
Whether we are part of UKP or not.

434
00:22:18,360 --> 00:22:21,720
So yes and no, officially not.

435
00:22:21,720 --> 00:22:23,720
So this is like independent research group

436
00:22:23,720 --> 00:22:26,520
and I'm independent research group leader.

437
00:22:26,520 --> 00:22:31,360
So I'm sort of independent by definition,

438
00:22:31,360 --> 00:22:34,000
but we are cooperating closely with UKP,

439
00:22:34,920 --> 00:22:37,560
which means we're sitting, we're sharing some offices,

440
00:22:37,560 --> 00:22:39,560
we're sharing hardware,

441
00:22:39,560 --> 00:22:43,480
and we're closely collaborating on research projects as well.

442
00:22:43,480 --> 00:22:45,920
But I'm independent of UKP,

443
00:22:45,960 --> 00:22:47,520
but we are really on good terms.

444
00:22:47,520 --> 00:22:49,880
So Martin is basically postdoc at UKP.

445
00:22:49,880 --> 00:22:52,200
I used to be a postdoc at UKP before and so on.

446
00:22:52,200 --> 00:22:54,400
So, but we're independent.

447
00:22:54,400 --> 00:22:55,320
Any other question?

448
00:22:56,320 --> 00:22:57,960
Anyone wants to get a job?

449
00:22:59,240 --> 00:23:00,080
No, not really.

450
00:23:00,080 --> 00:23:00,920
Oh, come on.

451
00:23:00,920 --> 00:23:01,760
Okay.

452
00:23:02,720 --> 00:23:03,560
Later.

453
00:23:03,560 --> 00:23:04,400
I'll convince you.

454
00:23:04,400 --> 00:23:05,240
Okay, good.

455
00:23:06,240 --> 00:23:07,080
Let's move on.

456
00:23:08,280 --> 00:23:09,480
Let's move on to the actual content.

457
00:23:09,480 --> 00:23:10,880
So let me just take it down.

458
00:23:11,920 --> 00:23:15,800
So the actual content of this lecture

459
00:23:16,640 --> 00:23:18,600
is not about the history of deep learning

460
00:23:18,600 --> 00:23:21,040
because everybody knows it from you.

461
00:23:21,040 --> 00:23:25,000
It's not about the perceptron,

462
00:23:25,000 --> 00:23:26,240
which everybody starts with like,

463
00:23:26,240 --> 00:23:27,880
oh, how can I even build a perceptron?

464
00:23:27,880 --> 00:23:30,520
And you know, it's not about that.

465
00:23:30,520 --> 00:23:33,160
And we're taking a twist a little bit here

466
00:23:33,160 --> 00:23:35,000
and we're starting from the end.

467
00:23:35,920 --> 00:23:40,560
We're starting from some NLP tasks and their evaluation.

468
00:23:40,560 --> 00:23:41,560
And why is that so?

469
00:23:41,560 --> 00:23:44,560
So why should we learn this?

470
00:23:44,560 --> 00:23:46,560
Why should we do this?

471
00:23:46,560 --> 00:23:48,240
I mean, it's important to ask like,

472
00:23:48,240 --> 00:23:50,440
why should I learn something about NLP task?

473
00:23:50,440 --> 00:23:52,360
I want to build tools.

474
00:23:52,360 --> 00:23:55,000
I want to build GPT force and this is cool.

475
00:23:55,000 --> 00:23:56,760
So I don't care about the tasks.

476
00:23:56,760 --> 00:24:01,760
Well, my perspective on that is that deep learning

477
00:24:01,840 --> 00:24:04,840
is just a tool, it's any other tool.

478
00:24:04,840 --> 00:24:07,080
And we need to understand why we need this tool

479
00:24:07,080 --> 00:24:08,040
in the first place.

480
00:24:09,160 --> 00:24:10,680
Why do we need it?

481
00:24:10,680 --> 00:24:12,280
What are we going to solve it?

482
00:24:12,280 --> 00:24:13,640
And how do we know?

483
00:24:13,640 --> 00:24:16,120
This is the right tool for our task.

484
00:24:16,120 --> 00:24:18,200
Like, is it a good hammer?

485
00:24:18,200 --> 00:24:22,840
But if you're not hammering nails, then why hammer?

486
00:24:22,840 --> 00:24:25,400
So we need to understand what the task are we solving

487
00:24:26,280 --> 00:24:29,560
and whether we're good at it or not objectively.

488
00:24:29,560 --> 00:24:31,960
So we're kind of starting to set up the scene.

489
00:24:31,960 --> 00:24:33,400
Why do we need this?

490
00:24:36,760 --> 00:24:41,760
And we'll start with sort of like typology of the tasks.

491
00:24:42,720 --> 00:24:43,920
So what is a task?

492
00:24:43,920 --> 00:24:46,480
Why are people talking about tasks and benchmarks?

493
00:24:46,480 --> 00:24:47,320
What does it mean?

494
00:24:48,440 --> 00:24:53,440
So basically, basically very, very kind of course typology

495
00:24:56,120 --> 00:24:59,440
working with text is that either you do text classification

496
00:25:00,360 --> 00:25:04,320
or you do text generation roughly, roughly, roughly, roughly.

497
00:25:05,360 --> 00:25:07,120
Okay, there is more flavors to that.

498
00:25:07,120 --> 00:25:08,160
And we'll see at the end,

499
00:25:08,160 --> 00:25:10,840
maybe you can cast everything as one task,

500
00:25:10,840 --> 00:25:15,040
but roughly and historically you can classify texts

501
00:25:15,040 --> 00:25:17,400
or you can create text, generate text.

502
00:25:19,320 --> 00:25:21,400
So let's start with the classification a little bit.

503
00:25:21,400 --> 00:25:23,560
And I'm gonna go through,

504
00:25:23,560 --> 00:25:26,320
let's say some typical tasks and their data sets.

505
00:25:26,320 --> 00:25:30,240
So then once you see them later on, like doing, I don't know,

506
00:25:30,240 --> 00:25:32,520
doing master thesis, going to the industry somewhere

507
00:25:32,520 --> 00:25:35,120
and seeing, oh yeah, yeah, this is this task.

508
00:25:35,120 --> 00:25:35,960
It rings a bell.

509
00:25:35,960 --> 00:25:37,760
So you know it exists

510
00:25:37,760 --> 00:25:40,920
and it's well-established in research and the practice.

511
00:25:40,920 --> 00:25:42,960
This is the goal, you know, what's there?

512
00:25:42,960 --> 00:25:44,240
Which tasks are typical?

513
00:25:44,240 --> 00:25:46,560
You know, what people are trying to solve.

514
00:25:47,920 --> 00:25:50,480
So let's start with something which is super, super,

515
00:25:50,480 --> 00:25:54,120
super simple to solve, at least theoretically.

516
00:25:54,120 --> 00:25:57,160
And it's the sentiment classification of movie reviews.

517
00:25:57,160 --> 00:25:59,320
Okay, so those of you who did NLP

518
00:25:59,320 --> 00:26:01,760
and maybe some deep learning,

519
00:26:01,760 --> 00:26:03,960
who of you did something with sentiment analysis

520
00:26:03,960 --> 00:26:05,620
of reviews or movies?

521
00:26:05,620 --> 00:26:08,740
Okay, not many.

522
00:26:08,740 --> 00:26:09,580
Okay, great.

523
00:26:09,580 --> 00:26:10,420
So what did you do?

524
00:26:10,420 --> 00:26:14,460
You used exactly this data set.

525
00:26:14,460 --> 00:26:15,300
Okay, cool.

526
00:26:15,300 --> 00:26:16,140
So what did you build?

527
00:26:25,860 --> 00:26:26,700
Ensemble learning.

528
00:26:26,700 --> 00:26:27,540
Okay, wow.

529
00:26:27,540 --> 00:26:28,620
That's wild.

530
00:26:28,620 --> 00:26:32,020
Okay, good.

531
00:26:32,020 --> 00:26:32,860
Yeah, random first.

532
00:26:33,060 --> 00:26:34,380
Wasn't deep learning.

533
00:26:34,380 --> 00:26:39,380
And I didn't use any like a complex word

534
00:26:43,100 --> 00:26:45,820
or anything, it just was back and forth.

535
00:26:45,820 --> 00:26:46,660
Okay.

536
00:26:48,060 --> 00:26:48,900
Okay, cool.

537
00:26:48,900 --> 00:26:50,100
Anyone else who did it?

538
00:26:50,100 --> 00:26:51,380
You raise your hand.

539
00:26:51,380 --> 00:26:52,780
So what did you do?

540
00:26:57,020 --> 00:26:59,140
Classification model for tweets.

541
00:26:59,140 --> 00:27:00,100
So what did you do with tweets?

542
00:27:00,100 --> 00:27:02,340
Like sentiment for tweets?

543
00:27:02,660 --> 00:27:03,500
Okay, great.

544
00:27:03,500 --> 00:27:06,180
So my question is like, why did you do it?

545
00:27:06,180 --> 00:27:07,140
Why tweets?

546
00:27:07,140 --> 00:27:08,620
Why classification of tweets?

547
00:27:12,620 --> 00:27:13,460
Yeah, it was false.

548
00:27:13,460 --> 00:27:14,300
Okay, good.

549
00:27:14,300 --> 00:27:15,120
Yes, okay.

550
00:27:15,120 --> 00:27:16,900
Somebody told you to do it, so you did it, right?

551
00:27:16,900 --> 00:27:18,500
Okay, so you didn't ask like,

552
00:27:18,500 --> 00:27:20,900
well, yeah, I mean, that's a fair reason, right?

553
00:27:20,900 --> 00:27:23,420
So you didn't ask like, why the heck should I,

554
00:27:23,420 --> 00:27:24,980
like, what is sentiment analysis Twitter?

555
00:27:24,980 --> 00:27:25,820
You know, I don't care.

556
00:27:25,820 --> 00:27:26,780
Yes, you have a point.

557
00:27:26,820 --> 00:27:27,820
For tickets.

558
00:27:31,660 --> 00:27:32,780
Oh, for tickets, okay.

559
00:27:38,780 --> 00:27:43,780
I see a movie of your angle having to prioritize the ticket.

560
00:27:44,300 --> 00:27:46,580
So if the customers are more angry,

561
00:27:46,580 --> 00:27:49,060
then you would try to prioritize it higher.

562
00:27:49,060 --> 00:27:50,620
I wouldn't do it the other way.

563
00:27:50,620 --> 00:27:52,380
I mean, I would really put, you know,

564
00:27:52,380 --> 00:27:54,060
the anger one put on hold and, you know,

565
00:27:54,060 --> 00:27:55,980
just relax and I'll get to you.

566
00:27:56,780 --> 00:28:01,020
Okay, okay.

567
00:28:02,980 --> 00:28:03,940
Happy customers.

568
00:28:03,940 --> 00:28:05,140
Okay, that makes sense.

569
00:28:05,140 --> 00:28:07,260
So exactly, so sentiment,

570
00:28:07,260 --> 00:28:09,500
so let's thank you for your input.

571
00:28:09,500 --> 00:28:11,940
So sentiment classification of movie reviews.

572
00:28:11,940 --> 00:28:13,660
Basically it's a binary classification,

573
00:28:13,660 --> 00:28:15,860
mostly of reviews from IMDb.

574
00:28:15,860 --> 00:28:19,060
I'm talking about this particular paper from Andrew Maas

575
00:28:19,060 --> 00:28:21,780
and others from ACL 2011.

576
00:28:21,780 --> 00:28:24,020
This is the IMDb data set, right?

577
00:28:24,020 --> 00:28:26,860
The IMDb data set, which you download somewhere.

578
00:28:26,860 --> 00:28:28,940
And it's, here's an example.

579
00:28:28,940 --> 00:28:30,580
I look into that and it's like,

580
00:28:30,580 --> 00:28:33,220
I think the shortest review there.

581
00:28:33,220 --> 00:28:34,940
Read a book, forget the movie,

582
00:28:36,300 --> 00:28:39,020
which is negative, obviously against the movie.

583
00:28:39,020 --> 00:28:42,660
And basically you have negative and positive labels there.

584
00:28:42,660 --> 00:28:44,860
What you need to understand,

585
00:28:44,860 --> 00:28:46,820
what you need to understand to classify the review

586
00:28:46,820 --> 00:28:49,220
of a movie into positive or negative sentiments,

587
00:28:49,220 --> 00:28:51,260
like bad movie or good movie,

588
00:28:51,260 --> 00:28:52,980
is a semantic compositional it is.

589
00:28:52,980 --> 00:28:57,380
So how words work together in their meaning,

590
00:28:57,380 --> 00:29:00,580
and also if the movies are long, the reviews are long,

591
00:29:00,580 --> 00:29:03,980
how one part from the beginning is related to the rest,

592
00:29:03,980 --> 00:29:06,260
because you have long dependencies in the text.

593
00:29:06,260 --> 00:29:07,940
You need to, maybe you need to understand,

594
00:29:07,940 --> 00:29:11,300
maybe not, it's just maybe a few words like ugly and so on.

595
00:29:12,260 --> 00:29:13,940
But here, there's no such a word.

596
00:29:13,940 --> 00:29:14,980
Read a book, forget a movie.

597
00:29:14,980 --> 00:29:18,540
So you need to understand what it means.

598
00:29:18,540 --> 00:29:19,380
It's not easy.

599
00:29:19,380 --> 00:29:21,100
Like there's no word signaling like

600
00:29:21,100 --> 00:29:22,500
this is a negative thing.

601
00:29:23,020 --> 00:29:27,020
You need to understand the contrast between movie and book.

602
00:29:27,020 --> 00:29:28,020
What does it mean?

603
00:29:28,020 --> 00:29:29,700
Like forget the movie.

604
00:29:29,700 --> 00:29:30,540
What does it mean?

605
00:29:30,540 --> 00:29:31,860
And so on.

606
00:29:31,860 --> 00:29:36,500
So anyone did digit recognition or MNIST?

607
00:29:36,500 --> 00:29:37,900
Who does know MNIST?

608
00:29:37,900 --> 00:29:39,260
What MNIST is?

609
00:29:39,260 --> 00:29:41,020
Okay, what is MNIST?

610
00:29:41,020 --> 00:29:44,860
If all the most basic imitators are in traffic.

611
00:29:44,860 --> 00:29:45,980
Exactly, exactly.

612
00:29:45,980 --> 00:29:48,460
So for computer vision people,

613
00:29:48,460 --> 00:29:51,380
MNIST is just a bunch of digits,

614
00:29:51,420 --> 00:29:53,420
and you classify the image of the digit

615
00:29:53,420 --> 00:29:55,260
into the actual digit.

616
00:29:55,260 --> 00:29:56,900
And everything you try in deep learning

617
00:29:56,900 --> 00:30:00,100
for computer vision, you have to try on MNIST.

618
00:30:00,100 --> 00:30:01,500
It's like the litmus paper.

619
00:30:01,500 --> 00:30:04,300
So IMDB is sort of for NLP, the MNIST.

620
00:30:04,300 --> 00:30:06,620
You should try it on IMDB to see whether it works.

621
00:30:06,620 --> 00:30:07,780
You have new model.

622
00:30:08,620 --> 00:30:11,660
If you don't try IMDB and claim this is really great,

623
00:30:11,660 --> 00:30:12,500
or it used to be like that at least.

624
00:30:12,500 --> 00:30:13,820
It's like a standard data set,

625
00:30:13,820 --> 00:30:16,860
which you pull out of box and try something.

626
00:30:16,860 --> 00:30:18,860
How is my random forest working on IMDB?

627
00:30:19,860 --> 00:30:23,500
81%, 83%, 86?

628
00:30:24,540 --> 00:30:25,380
Something like that.

629
00:30:25,380 --> 00:30:26,940
86% accuracy, I would say.

630
00:30:26,940 --> 00:30:28,580
Something around, I mean if it's 50-50,

631
00:30:28,580 --> 00:30:30,860
then you're like oh, that is bad.

632
00:30:30,860 --> 00:30:35,380
So the question is, why is it interesting?

633
00:30:35,380 --> 00:30:36,420
Why was it interesting?

634
00:30:36,420 --> 00:30:39,220
Why sentiment of movies?

635
00:30:39,220 --> 00:30:41,140
Why is it interesting?

636
00:30:41,140 --> 00:30:44,100
Why people start using movie reviews

637
00:30:44,100 --> 00:30:48,460
for predicting the sentiment of the text?

638
00:30:48,460 --> 00:30:49,660
Why movie reviews?

639
00:30:51,620 --> 00:30:53,700
Probably because of the bunch of data,

640
00:30:53,700 --> 00:30:55,780
which is already labeled.

641
00:30:55,780 --> 00:30:57,340
Nice, yes.

642
00:30:57,340 --> 00:31:00,220
Very opportunistically, there's a bunch of data somewhere

643
00:31:00,220 --> 00:31:01,780
which is labeled.

644
00:31:01,780 --> 00:31:02,620
How labeled?

645
00:31:02,620 --> 00:31:03,460
What does it mean?

646
00:31:03,460 --> 00:31:05,420
I don't know how IMDB rating works,

647
00:31:05,420 --> 00:31:08,220
but probably like one to five stars.

648
00:31:08,220 --> 00:31:09,500
Exactly, somebody wrote the review

649
00:31:09,500 --> 00:31:12,180
and say like it's one star, four stars, five stars.

650
00:31:12,180 --> 00:31:13,140
So people say, yeah, okay,

651
00:31:13,140 --> 00:31:14,820
so everything which is one star is negative.

652
00:31:14,820 --> 00:31:16,900
Everything five stars is positive.

653
00:31:16,900 --> 00:31:18,060
You scrape it from the internet

654
00:31:18,660 --> 00:31:20,380
and then you have a data set for free.

655
00:31:20,380 --> 00:31:22,020
That's one reason.

656
00:31:22,020 --> 00:31:23,140
Any other reason why people,

657
00:31:23,140 --> 00:31:25,180
yeah, so this is like very opportunistic.

658
00:31:32,580 --> 00:31:35,420
Okay, so it's like, it's a variety of language there

659
00:31:35,420 --> 00:31:36,860
from different people, that's what you were saying.

660
00:31:36,860 --> 00:31:38,020
Yeah, that's a great point.

661
00:31:38,020 --> 00:31:41,580
So one of the pioneers of this data set

662
00:31:41,580 --> 00:31:43,220
or of this task was Lily and Lee

663
00:31:43,220 --> 00:31:45,260
and they had a paper in 2004

664
00:31:45,260 --> 00:31:46,900
on classifying reviews on Twitter.

665
00:31:47,860 --> 00:31:50,140
And she gave a talk and she said

666
00:31:50,140 --> 00:31:53,180
the reason why they started with the movie reviews,

667
00:31:53,180 --> 00:31:56,220
one of the reasons was a variety of the language

668
00:31:56,220 --> 00:31:59,340
and she said the beauty of language.

669
00:32:00,260 --> 00:32:02,220
Because these reviews, well not this one,

670
00:32:02,220 --> 00:32:05,900
this is really short and kind of really to the point,

671
00:32:05,900 --> 00:32:09,180
but most of movie reviews are kind of poetic

672
00:32:09,180 --> 00:32:12,940
and nice to read and very kind of figurative language

673
00:32:12,940 --> 00:32:15,140
which 20 years back was considered something

674
00:32:15,140 --> 00:32:17,660
super hard to understand for machines.

675
00:32:17,660 --> 00:32:19,260
So they say like, well if a machine

676
00:32:19,260 --> 00:32:21,180
can kind of understand the sentiment

677
00:32:21,180 --> 00:32:24,660
of this long review in this kind of figurative language,

678
00:32:24,660 --> 00:32:26,020
this would be an achievement

679
00:32:26,020 --> 00:32:28,380
and it was fun to read them as well.

680
00:32:28,380 --> 00:32:30,140
So there's different motivations why people do it.

681
00:32:30,140 --> 00:32:32,940
Like you have to do this for customers.

682
00:32:32,940 --> 00:32:34,780
Satisfaction, yeah, that's a clear goal.

683
00:32:34,780 --> 00:32:36,140
Somebody is just curious and is like,

684
00:32:36,140 --> 00:32:38,740
yeah, this is a nice language so we should understand it.

685
00:32:38,740 --> 00:32:40,660
And then somebody, and you have data for free,

686
00:32:40,660 --> 00:32:43,380
so absolutely, this is the killer, exactly.

687
00:32:43,380 --> 00:32:44,420
Okay, any questions?

688
00:32:45,860 --> 00:32:47,740
So where do you get the data?

689
00:32:47,740 --> 00:32:48,740
Where do you get it?

690
00:32:51,340 --> 00:32:52,180
Yes?

691
00:32:54,660 --> 00:32:56,220
You can scrape IMDB, exactly.

692
00:32:56,220 --> 00:32:58,020
How do you get the data in 2023?

693
00:32:58,020 --> 00:32:59,420
I mean this is great, this is a great answer,

694
00:32:59,420 --> 00:33:01,780
but you can, there's easier way.

695
00:33:01,780 --> 00:33:03,540
Hugging face, exactly.

696
00:33:03,540 --> 00:33:08,540
So hugging face is a collection of now,

697
00:33:08,540 --> 00:33:13,340
what is it, 28,800 something data sets

698
00:33:13,340 --> 00:33:15,580
which somebody did the dirty job for you

699
00:33:15,580 --> 00:33:17,460
and collected and put it there in a nice format

700
00:33:17,460 --> 00:33:19,340
so you can just take it out of box

701
00:33:19,340 --> 00:33:22,900
and run your random forest thing

702
00:33:22,900 --> 00:33:24,900
and just don't care about what's the language,

703
00:33:24,900 --> 00:33:27,700
beauty of language and labels and so on.

704
00:33:27,700 --> 00:33:31,540
Which is great, but you have to understand what's behind

705
00:33:31,540 --> 00:33:33,580
if you're serious about it.

706
00:33:33,580 --> 00:33:35,460
So you go to the Hugging Face data set

707
00:33:35,460 --> 00:33:37,180
and you download through API or whatever

708
00:33:37,180 --> 00:33:38,540
so you can get data sets.

709
00:33:39,460 --> 00:33:41,780
So and as I said, like the IMDB data set,

710
00:33:41,780 --> 00:33:43,940
well you have to be really sure

711
00:33:43,940 --> 00:33:45,980
if you talk about data set, you cite it

712
00:33:46,860 --> 00:33:48,140
and maybe link what exactly,

713
00:33:48,140 --> 00:33:50,140
because IMDB you can scrape anytime you want

714
00:33:50,140 --> 00:33:51,740
and you have different data set.

715
00:33:51,740 --> 00:33:53,140
But you have to compare apples to apples

716
00:33:53,140 --> 00:33:54,660
so everybody should use the same data set

717
00:33:54,660 --> 00:33:56,460
if they're comparing systems, right?

718
00:33:58,220 --> 00:34:00,100
Okay, any questions to sentiment analysis on movies?

719
00:34:00,100 --> 00:34:02,420
So the IMDB data set, the litmus paper.

720
00:34:02,420 --> 00:34:04,900
You should somehow remember there's a thing

721
00:34:04,900 --> 00:34:07,420
like IMDB data set because everybody's using them.

722
00:34:09,100 --> 00:34:10,460
Cool, let's move on.

723
00:34:10,500 --> 00:34:11,900
So there's another task which is called

724
00:34:11,900 --> 00:34:13,460
natural language inference.

725
00:34:13,460 --> 00:34:17,060
And the standard paper for natural language interest

726
00:34:17,060 --> 00:34:21,980
is the SNLI, the Stanford Natural Language Inference task.

727
00:34:21,980 --> 00:34:22,820
What does it mean?

728
00:34:22,820 --> 00:34:24,180
We have two sentences.

729
00:34:25,100 --> 00:34:27,060
And these are somehow related to each other.

730
00:34:27,060 --> 00:34:29,300
And either they're related as an entitlement

731
00:34:29,300 --> 00:34:31,500
or contradiction or they're neutral.

732
00:34:32,700 --> 00:34:36,780
And these two sentences are called a text and a hypothesis.

733
00:34:36,780 --> 00:34:38,860
And the text is here in this example,

734
00:34:38,860 --> 00:34:41,620
a soccer game with multiple males playing,

735
00:34:41,620 --> 00:34:44,060
with basically, yeah, come in.

736
00:34:44,060 --> 00:34:45,900
So it's basically describing a scene, right?

737
00:34:45,900 --> 00:34:49,900
There's a soccer game with multiple males playing.

738
00:34:49,900 --> 00:34:54,060
And the hypothesis text is some men are playing sport.

739
00:34:54,060 --> 00:34:57,620
And the goal is when you hear the text,

740
00:34:57,620 --> 00:35:01,500
so the text is true, is the hypothesis true as well?

741
00:35:01,500 --> 00:35:04,700
So if I tell you a soccer game with multiple males playing,

742
00:35:04,700 --> 00:35:07,220
is it true that some men are playing sport?

743
00:35:08,900 --> 00:35:10,660
Yes, it's true.

744
00:35:10,660 --> 00:35:12,940
If there's a soccer, you know,

745
00:35:12,940 --> 00:35:14,740
if males playing soccer somewhere,

746
00:35:14,740 --> 00:35:17,620
it's true that some men are playing sport, right?

747
00:35:17,620 --> 00:35:18,540
Does it make sense?

748
00:35:19,740 --> 00:35:21,340
So this is entitlement.

749
00:35:21,340 --> 00:35:23,180
If the second sentence would be a soccer game

750
00:35:23,180 --> 00:35:24,260
with multiple males playing,

751
00:35:24,260 --> 00:35:27,020
and the sentence would be, yeah,

752
00:35:27,020 --> 00:35:31,100
a butterfly is sitting on a flower,

753
00:35:31,100 --> 00:35:34,020
it would be like neutral because there is no relationship.

754
00:35:34,020 --> 00:35:37,300
If the second sentence would be some women are playing sport,

755
00:35:37,340 --> 00:35:38,180
it would be contradiction

756
00:35:38,180 --> 00:35:41,060
because we have multiple males playing, right?

757
00:35:41,060 --> 00:35:44,620
So we have three kind of three different labels

758
00:35:44,620 --> 00:35:49,620
for two sentences, how they work together.

759
00:35:49,660 --> 00:35:52,300
So the standard paper is the SNLI paper,

760
00:35:52,300 --> 00:35:57,300
and it's almost 600,000 of these sentence pairs.

761
00:35:57,700 --> 00:36:02,500
So they took a lot of money, hired some persons

762
00:36:02,500 --> 00:36:05,500
like crowd workers on Amazon Mechanical Turk,

763
00:36:05,500 --> 00:36:07,820
and let them write these kinds of things

764
00:36:07,820 --> 00:36:10,140
and collected a bunch of almost half a million

765
00:36:10,140 --> 00:36:12,300
human written English sentence pairs.

766
00:36:12,300 --> 00:36:13,860
So it's really a huge data set,

767
00:36:14,820 --> 00:36:17,740
and they do some in labeling for balanced classification.

768
00:36:17,740 --> 00:36:19,860
So it's balanced data set.

769
00:36:19,860 --> 00:36:24,180
So, and the difference here between the IMDb before

770
00:36:24,180 --> 00:36:26,860
is that, yeah, sorry, it's really low,

771
00:36:26,860 --> 00:36:28,380
so you can't see that, but just, you know,

772
00:36:28,380 --> 00:36:30,540
open your local slides and just follow.

773
00:36:30,540 --> 00:36:33,860
That IMDb was for free, right?

774
00:36:33,860 --> 00:36:34,860
You scrape it from the internet,

775
00:36:35,100 --> 00:36:36,540
you get the stars and call it a day.

776
00:36:36,540 --> 00:36:38,980
Here, you have to pay people to do it.

777
00:36:38,980 --> 00:36:42,060
And I think they paid a lot of money

778
00:36:42,060 --> 00:36:44,660
to create this data set, 600,000 pairs.

779
00:36:44,660 --> 00:36:47,940
How many of these pairs can you write in an hour?

780
00:36:49,820 --> 00:36:51,900
How many pairs can you write in an hour?

781
00:36:55,300 --> 00:36:58,380
What you're gonna do after writing 60 pairs of that?

782
00:36:58,380 --> 00:37:00,180
You're gonna hate it.

783
00:37:00,180 --> 00:37:02,100
You know, you're gonna hate a job because it's terrible.

784
00:37:02,100 --> 00:37:04,220
I mean, it's not fun.

785
00:37:04,220 --> 00:37:06,780
So that's why you need a lot of, you know,

786
00:37:06,780 --> 00:37:11,780
a large pool of people and just, you know, kind of scale up.

787
00:37:11,980 --> 00:37:14,060
Half a million sentences is a lot.

788
00:37:16,140 --> 00:37:19,820
Which brings me to one sort of side step,

789
00:37:19,820 --> 00:37:22,260
and it's the gold standard data.

790
00:37:22,260 --> 00:37:23,300
So what does it mean?

791
00:37:24,900 --> 00:37:26,540
It might have multiple meanings.

792
00:37:26,540 --> 00:37:31,260
So many data sets are annotated by experts, you know,

793
00:37:31,260 --> 00:37:32,740
and it could be super costly.

794
00:37:33,740 --> 00:37:38,740
And in these examples, each example is annotated

795
00:37:38,900 --> 00:37:40,060
by multiple annotators.

796
00:37:40,060 --> 00:37:42,180
So annotator is somebody who's kind of annotating the data

797
00:37:42,180 --> 00:37:44,740
saying, well, contradiction, entitlement, or neutral,

798
00:37:44,740 --> 00:37:46,420
or positive or negative on the tweet

799
00:37:46,420 --> 00:37:49,100
because tweets don't have any labels.

800
00:37:50,260 --> 00:37:53,580
And then the final label is decided.

801
00:37:53,580 --> 00:37:56,140
So the final gold label.

802
00:37:56,140 --> 00:37:58,660
So the gold could be, could mean, okay,

803
00:37:58,660 --> 00:38:02,620
so several people agreed on a label for particle instance.

804
00:38:02,620 --> 00:38:04,580
So this is like the gold truth.

805
00:38:05,820 --> 00:38:09,220
Or the gold can mean it's costly because, you know,

806
00:38:09,220 --> 00:38:11,580
you have to pay a lot of people, these experts.

807
00:38:11,580 --> 00:38:15,380
One example here is that we, so I put this reference here.

808
00:38:15,380 --> 00:38:18,540
We were annotating legal decisions

809
00:38:18,540 --> 00:38:21,460
from the Court of European Human Rights

810
00:38:21,460 --> 00:38:25,500
with six law students over a year.

811
00:38:25,500 --> 00:38:28,500
So how much did it cost?

812
00:38:28,500 --> 00:38:29,340
Any estimate.

813
00:38:29,340 --> 00:38:31,100
How much we paid for annotating a data set,

814
00:38:31,100 --> 00:38:33,980
which is now publicly available in this publication.

815
00:38:33,980 --> 00:38:34,820
How much did we pay?

816
00:38:34,820 --> 00:38:35,660
Any estimates?

817
00:38:37,740 --> 00:38:40,340
50 to 100,000.

818
00:38:40,340 --> 00:38:41,900
50 to 100,000 would be great.

819
00:38:41,900 --> 00:38:43,700
I know, it's way too much.

820
00:38:43,700 --> 00:38:46,780
No, 20K, roughly.

821
00:38:46,780 --> 00:38:47,620
It's a lot of money.

822
00:38:47,620 --> 00:38:50,660
20,000 euro, whew, okay, a year of work.

823
00:38:51,700 --> 00:38:53,620
You know, so it's costly because you need these experts.

824
00:38:53,620 --> 00:38:56,060
We need lawyers or law students

825
00:38:56,060 --> 00:38:57,300
to understand the legal language

826
00:38:57,300 --> 00:38:58,940
because I couldn't understand anything, you know,

827
00:38:58,940 --> 00:39:00,900
so I couldn't annotate anything.

828
00:39:01,500 --> 00:39:04,340
So that's why it's called gold-labeled data.

829
00:39:04,340 --> 00:39:08,340
So it's super costly to produce good data.

830
00:39:08,340 --> 00:39:11,780
It's easy to get it from Hugging Face data sets, right,

831
00:39:11,780 --> 00:39:12,740
for free.

832
00:39:12,740 --> 00:39:13,620
This is great.

833
00:39:13,620 --> 00:39:15,660
So appreciate somebody who did some annotating.

834
00:39:15,660 --> 00:39:17,140
Anyone annotated here something?

835
00:39:17,140 --> 00:39:20,620
Anybody did like some human annotation study?

836
00:39:23,100 --> 00:39:25,260
No one, okay, great.

837
00:39:25,260 --> 00:39:27,420
If you have a chance, do it.

838
00:39:27,420 --> 00:39:29,060
Do it, because then you will understand

839
00:39:29,060 --> 00:39:31,580
how terrible that is to annotate data.

840
00:39:31,580 --> 00:39:34,260
No, it is, it's boring.

841
00:39:34,260 --> 00:39:36,700
The worst thing you can do is annotating something

842
00:39:36,700 --> 00:39:39,340
on the internet, like social media.

843
00:39:39,340 --> 00:39:43,100
So for example, annotating posts on Facebook,

844
00:39:43,100 --> 00:39:44,540
whether they're toxic or not.

845
00:39:45,540 --> 00:39:48,140
Not only are you gonna hate it, but it's gonna hurt.

846
00:39:49,020 --> 00:39:49,860
It's true.

847
00:39:49,860 --> 00:39:54,660
It's just how much stupid it is out there in the world.

848
00:39:54,660 --> 00:39:55,620
That really hurts.

849
00:39:56,500 --> 00:39:59,340
If somebody, you know, if OpenAI is annotating their data,

850
00:39:59,340 --> 00:40:01,660
they're outsourcing it to somebody else,

851
00:40:01,660 --> 00:40:04,540
paying low wages, I'm recording on YouTube, I don't care.

852
00:40:04,540 --> 00:40:09,100
They're paying low wages and let people for low wages

853
00:40:09,100 --> 00:40:11,980
annotate toxic comments or even maybe pornography.

854
00:40:11,980 --> 00:40:12,980
It's a really bad thing.

855
00:40:12,980 --> 00:40:14,940
Annotating data could be, you know,

856
00:40:14,940 --> 00:40:16,300
could hurt you like sickly.

857
00:40:17,580 --> 00:40:20,260
Anyway, but if you have a chance of annotating something,

858
00:40:20,260 --> 00:40:21,100
do it.

859
00:40:21,100 --> 00:40:23,140
Just, you know, understand how time consuming

860
00:40:23,140 --> 00:40:24,940
and interesting the task is.

861
00:40:26,180 --> 00:40:29,620
How do you make sure that you have good quality of data?

862
00:40:29,620 --> 00:40:32,700
So basically you compare, if you have like, I don't know,

863
00:40:32,700 --> 00:40:34,820
like three people for the same example,

864
00:40:34,820 --> 00:40:37,780
you're gonna compare them together

865
00:40:37,780 --> 00:40:40,780
and make sure that, you know, the chance,

866
00:40:40,780 --> 00:40:44,260
if they agree on the label, it could be by chance.

867
00:40:44,260 --> 00:40:49,260
So if you just taken, let's say, an average agreement,

868
00:40:50,580 --> 00:40:52,740
it will be maybe bigger than the actual agreement is.

869
00:40:52,740 --> 00:40:55,940
So there are a couple of measures for inter,

870
00:40:55,940 --> 00:40:58,220
so-called inter-annotator agreements.

871
00:40:58,220 --> 00:41:01,060
And there's few measures like Cohen's Kappa,

872
00:41:01,060 --> 00:41:02,460
P, Kripa and so forth.

873
00:41:02,460 --> 00:41:03,740
I don't want you to remember all of them,

874
00:41:03,740 --> 00:41:07,340
but I want you to remember that there's actual measures

875
00:41:07,340 --> 00:41:10,180
for agreement computation among people,

876
00:41:10,180 --> 00:41:11,660
among annotators, you know?

877
00:41:11,660 --> 00:41:16,020
So if you annotate a data set and you ask for quality,

878
00:41:17,420 --> 00:41:19,620
they should have done something

879
00:41:19,620 --> 00:41:21,180
as internet data agreements.

880
00:41:21,180 --> 00:41:23,220
And there's some metrics and they have meaning.

881
00:41:23,220 --> 00:41:26,940
So it's important to know how, what does it tell?

882
00:41:26,940 --> 00:41:28,660
Well, it tells us also like there's a subjectivity

883
00:41:28,660 --> 00:41:29,940
in the task, you know?

884
00:41:29,940 --> 00:41:31,900
And if this is completely subjective,

885
00:41:31,900 --> 00:41:36,260
then maybe it makes no sense to run machines on that, right?

886
00:41:36,260 --> 00:41:38,820
Okay, any questions to gold standard data?

887
00:41:40,580 --> 00:41:41,420
Great.

888
00:41:42,660 --> 00:41:44,380
Second question is a side step.

889
00:41:44,380 --> 00:41:46,060
So who creates these tasks, why?

890
00:41:46,060 --> 00:41:48,820
I mean, so we had this kind of example of a movie reviews

891
00:41:48,820 --> 00:41:51,260
because it's, you know, poetic and fun.

892
00:41:51,260 --> 00:41:52,300
So mostly researchers,

893
00:41:52,300 --> 00:41:55,260
mostly researchers are interested in doing some tasks.

894
00:41:55,260 --> 00:41:57,420
Sometimes companies saying, well, I wanna, you know,

895
00:41:57,420 --> 00:42:02,140
I wanna improve, what was that, the customer experience.

896
00:42:02,140 --> 00:42:04,180
So if somebody is calling me angry,

897
00:42:04,180 --> 00:42:06,740
I need to solve it and I need to prioritize things.

898
00:42:06,740 --> 00:42:08,300
Yeah, so it's fine.

899
00:42:08,300 --> 00:42:11,740
So there's different stakeholders in the game,

900
00:42:11,740 --> 00:42:13,660
but these tasks I'm presenting today,

901
00:42:13,660 --> 00:42:14,860
just mostly researchers,

902
00:42:14,860 --> 00:42:18,260
because they are interested in some phenomenon language.

903
00:42:18,260 --> 00:42:19,980
For example, a sentiment,

904
00:42:19,980 --> 00:42:22,740
because it's just an interesting thing to observe in data

905
00:42:23,580 --> 00:42:26,620
and to which extent we can solve them by tools.

906
00:42:26,620 --> 00:42:29,420
So basically it's also setting up benchmarks

907
00:42:29,420 --> 00:42:32,100
for automatic machines or for machines

908
00:42:32,100 --> 00:42:34,180
to kind of tackle with that.

909
00:42:34,180 --> 00:42:38,860
Are we able to say whether an email is a spam or not?

910
00:42:39,740 --> 00:42:41,060
It's solved, right?

911
00:42:41,060 --> 00:42:43,020
I mean, it's not interesting anymore.

912
00:42:43,020 --> 00:42:45,060
20 years back, it was kind of like interesting.

913
00:42:45,060 --> 00:42:48,260
Yeah, can we decide automatically if it's a spam or not?

914
00:42:49,780 --> 00:42:53,580
And also the data sets created and sharing them,

915
00:42:53,580 --> 00:42:55,780
it was popular with machine learning in NLP.

916
00:42:55,780 --> 00:42:58,780
So basically you start creating data sets

917
00:42:58,780 --> 00:43:01,380
and they become sort of like standard benchmarks.

918
00:43:01,380 --> 00:43:02,260
What is the standard?

919
00:43:02,260 --> 00:43:03,820
It's defined by the community.

920
00:43:03,820 --> 00:43:05,500
So the community somehow is interested now

921
00:43:05,500 --> 00:43:07,780
in doing IMDB data sets, sentiment analysis,

922
00:43:07,780 --> 00:43:09,660
and everybody's using that.

923
00:43:09,660 --> 00:43:11,300
Nobody is pushing you to do so,

924
00:43:11,300 --> 00:43:14,460
but you kind of feel like this is the standard data set.

925
00:43:15,660 --> 00:43:18,300
And obviously you're sharing data set

926
00:43:18,300 --> 00:43:19,860
and comparing them your new system

927
00:43:19,860 --> 00:43:21,300
because you have new network and say like,

928
00:43:21,300 --> 00:43:25,780
oh, I'm better than a random forest on IMDB.

929
00:43:25,780 --> 00:43:29,780
So 87%, I have like this new kind of thing, GPT-5.

930
00:43:29,780 --> 00:43:32,340
It's gonna be like 99%, whatever,

931
00:43:32,340 --> 00:43:34,740
but you have to do it on the same data set, okay?

932
00:43:36,100 --> 00:43:37,380
So the tasks are classified

933
00:43:37,380 --> 00:43:39,700
into various arbitrary taxonomies

934
00:43:39,700 --> 00:43:42,740
with mostly agreed upon names.

935
00:43:42,740 --> 00:43:46,420
So for example, sentiment analysis is sort of a task

936
00:43:46,420 --> 00:43:48,100
and it's part of text classification

937
00:43:48,100 --> 00:43:51,260
or sentence classification, document classification.

938
00:43:51,260 --> 00:43:54,100
The SNLI, the Stanford Nature Language Inference

939
00:43:54,100 --> 00:43:57,140
is a sort of sentence pair classification.

940
00:43:57,140 --> 00:43:58,540
But these are arbitrary names,

941
00:43:58,540 --> 00:44:00,940
so you can call them differently and it's fine as well.

942
00:44:00,940 --> 00:44:04,500
So there is no like set in stone taxonomy of tasks,

943
00:44:04,500 --> 00:44:07,100
but people kind of converge into seeing,

944
00:44:07,100 --> 00:44:09,340
ah, yeah, this is a document classification.

945
00:44:10,320 --> 00:44:11,620
Okay, any question to this?

946
00:44:13,700 --> 00:44:14,540
Right.

947
00:44:18,420 --> 00:44:19,420
So let's move on.

948
00:44:20,340 --> 00:44:21,660
And now let's say

949
00:44:24,300 --> 00:44:26,180
we are going deeper into a sentence

950
00:44:27,300 --> 00:44:29,580
and we're interested in finding entities

951
00:44:29,580 --> 00:44:32,540
or so-called main entities in a sentence.

952
00:44:33,820 --> 00:44:36,260
So we have a predefined types of entities

953
00:44:36,260 --> 00:44:38,300
and we wanna locate them in the sentence.

954
00:44:38,300 --> 00:44:39,380
And here's an example.

955
00:44:39,380 --> 00:44:41,800
So the sentence goes from, you know,

956
00:44:42,780 --> 00:44:44,300
top to bottom.

957
00:44:44,300 --> 00:44:48,540
UN official ECOS, ECOS, sorry, heads for Baghdad.

958
00:44:49,540 --> 00:44:53,420
So this is some news from 2002, I guess.

959
00:44:54,700 --> 00:44:59,060
And here we have the UN is labeled as organization.

960
00:44:59,060 --> 00:45:01,340
ECOS is probably some person

961
00:45:01,340 --> 00:45:03,900
and Baghdad is the definite location to sit in.

962
00:45:04,980 --> 00:45:06,340
So how to model this task?

963
00:45:06,340 --> 00:45:07,340
Because it's not like saying,

964
00:45:07,340 --> 00:45:10,540
well, this is binary decision over the whole document.

965
00:45:10,540 --> 00:45:12,820
So how can we model that?

966
00:45:13,740 --> 00:45:16,500
And well, this is a question.

967
00:45:16,500 --> 00:45:18,100
How would you model that?

968
00:45:20,380 --> 00:45:22,980
How would you label this thing?

969
00:45:22,980 --> 00:45:24,980
If I want you to find entities in a text

970
00:45:24,980 --> 00:45:27,100
like organization, person, location,

971
00:45:27,100 --> 00:45:28,440
how to annotate such a task?

972
00:45:28,440 --> 00:45:32,260
So the only thing you see is a sentence, right?

973
00:45:32,260 --> 00:45:35,400
And the sentence would be like a plain text, basically.

974
00:45:35,400 --> 00:45:37,060
No new lines.

975
00:45:37,060 --> 00:45:37,900
You have a sentence.

976
00:45:37,900 --> 00:45:39,340
So what do you need to do first?

977
00:45:40,540 --> 00:45:44,540
Oh, one, two, three, okay.

978
00:45:44,540 --> 00:45:45,380
You.

979
00:45:48,060 --> 00:45:53,060
Okay, so you're assuming you have some somewhere,

980
00:46:03,980 --> 00:46:05,740
a list of all these organizations

981
00:46:05,740 --> 00:46:08,560
and you want to create the data set out of it.

982
00:46:08,560 --> 00:46:12,400
Yeah, maybe I would say you have news

983
00:46:12,400 --> 00:46:13,560
and you have nothing else.

984
00:46:13,560 --> 00:46:15,460
So maybe it won't work.

985
00:46:15,460 --> 00:46:17,000
Any other ideas?

986
00:46:17,000 --> 00:46:17,840
So you.

987
00:46:21,060 --> 00:46:22,940
Exactly, we should first tokenize a thing

988
00:46:22,940 --> 00:46:25,560
because here you see the tokenization,

989
00:46:25,560 --> 00:46:26,600
so splitting into words.

990
00:46:26,600 --> 00:46:29,320
So we have to say UN is a word, official is a word,

991
00:46:29,320 --> 00:46:31,120
ECOS is a word, and so on,

992
00:46:31,120 --> 00:46:32,360
and the dot is not a word

993
00:46:32,360 --> 00:46:35,400
because we are going to label the tokens.

994
00:46:35,400 --> 00:46:36,800
Okay, this is one first assumption

995
00:46:36,800 --> 00:46:38,240
we need to take into account.

996
00:46:39,480 --> 00:46:41,520
Then the second thing, we have the tokens,

997
00:46:41,520 --> 00:46:43,480
so what are you gonna do?

998
00:46:43,480 --> 00:46:44,760
Then it's easy, yes?

999
00:46:48,440 --> 00:46:51,520
Get the base of the tokens

1000
00:46:51,520 --> 00:46:55,200
and then do part of the sentence thing

1001
00:46:55,200 --> 00:46:57,600
because all of the things that are not going to take

1002
00:46:57,600 --> 00:47:00,600
are nouns here, and take only things

1003
00:47:00,600 --> 00:47:02,460
that have a scope that are nouns.

1004
00:47:02,460 --> 00:47:04,720
Oh, okay, so you're saying you can take only nouns

1005
00:47:04,720 --> 00:47:06,560
and annotating nouns and ignoring rest.

1006
00:47:06,560 --> 00:47:08,040
Yeah, I mean, you can do this as well.

1007
00:47:08,400 --> 00:47:09,620
I would say it's much easier.

1008
00:47:09,620 --> 00:47:14,620
So you tokenize, and then assign each word,

1009
00:47:14,680 --> 00:47:18,140
or each token, let's be more concrete, a type.

1010
00:47:18,140 --> 00:47:20,520
So we're gonna say the UN was organization,

1011
00:47:20,520 --> 00:47:23,380
ECOS was person, and Baghdad was something else.

1012
00:47:23,380 --> 00:47:26,600
What we are using here is so-called BIO tagging.

1013
00:47:27,680 --> 00:47:29,460
It will be on the next slide as well.

1014
00:47:29,460 --> 00:47:31,860
And we're saying each of these words has a label,

1015
00:47:31,860 --> 00:47:35,600
so these are the labels, right?

1016
00:47:35,640 --> 00:47:39,920
And the O label is, it's not zero, it's O,

1017
00:47:39,920 --> 00:47:42,480
and it means like, maybe it's a zero.

1018
00:47:42,480 --> 00:47:44,080
No, it's O actually, it's BIO tagging.

1019
00:47:44,080 --> 00:47:46,920
So it's like out of entities, so there's nothing.

1020
00:47:46,920 --> 00:47:50,200
So each word, every word that is not the entity

1021
00:47:50,200 --> 00:47:54,400
is assigned with O, and we're assigning these I minus org

1022
00:47:54,400 --> 00:47:57,000
and I minus per and I minus log

1023
00:47:57,000 --> 00:48:00,220
to these tokens, which are the entity, okay?

1024
00:48:00,220 --> 00:48:02,800
It looks weird, but I'll show you why.

1025
00:48:03,120 --> 00:48:03,960
Yes?

1026
00:48:06,640 --> 00:48:07,480
Why isn't it B?

1027
00:48:07,480 --> 00:48:08,320
Yeah, that's a good question.

1028
00:48:08,320 --> 00:48:09,160
So why isn't it B?

1029
00:48:09,160 --> 00:48:11,320
So you know something.

1030
00:48:11,320 --> 00:48:14,580
So what is I and what is B then?

1031
00:48:15,600 --> 00:48:16,980
B is the gain, okay?

1032
00:48:22,640 --> 00:48:25,720
Exactly, that's interesting observation,

1033
00:48:25,720 --> 00:48:27,960
and I'm full with you.

1034
00:48:27,960 --> 00:48:31,800
So what we have here that if we had, say,

1035
00:48:32,720 --> 00:48:36,320
a longer entity, which would be, I don't know,

1036
00:48:36,320 --> 00:48:41,200
a word entity, a person, okay, Angela Merkel.

1037
00:48:42,800 --> 00:48:45,760
So if we had a text, something, something,

1038
00:48:45,760 --> 00:48:48,960
Angela Merkel, something, something,

1039
00:48:48,960 --> 00:48:53,960
and you would annotate it as a beginning of person,

1040
00:48:54,400 --> 00:48:57,000
and here this would be inside person,

1041
00:48:57,000 --> 00:49:00,440
like that would be your understanding.

1042
00:49:00,440 --> 00:49:01,960
And I guess it's not wrong,

1043
00:49:01,960 --> 00:49:03,840
and actually this is how I understand as well,

1044
00:49:03,840 --> 00:49:06,680
but I look into the original paper of this,

1045
00:49:06,680 --> 00:49:09,200
you know, this annotation,

1046
00:49:09,200 --> 00:49:12,420
and what they did was something else.

1047
00:49:12,420 --> 00:49:15,700
They said if two consequent tokens are the same type,

1048
00:49:17,160 --> 00:49:18,680
whenever two entities of the type

1049
00:49:18,680 --> 00:49:20,280
are immediately next to each other.

1050
00:49:20,280 --> 00:49:21,480
So it wasn't the Angela Merkel.

1051
00:49:21,480 --> 00:49:26,240
It would be, what could be like two entities

1052
00:49:26,240 --> 00:49:28,600
of the same type next to each other?

1053
00:49:31,440 --> 00:49:33,360
Without comma.

1054
00:49:35,360 --> 00:49:40,360
Austin, Texas, okay, so Austin, Texas,

1055
00:49:42,520 --> 00:49:47,520
blah, blah, blah, Austin, Texas, blah, blah, blah.

1056
00:49:49,680 --> 00:49:51,720
I would put comma here actually,

1057
00:49:51,720 --> 00:49:53,920
and it would be all, and the problem would be solved.

1058
00:49:53,920 --> 00:49:56,580
If there is no comma, then you would,

1059
00:49:56,580 --> 00:49:58,680
based on this original understanding,

1060
00:49:58,680 --> 00:50:01,060
original description of BIO tagging,

1061
00:50:01,060 --> 00:50:05,200
you would say this is E location,

1062
00:50:05,200 --> 00:50:09,500
and this is B location, right?

1063
00:50:09,500 --> 00:50:11,520
And it's kind of not intuitive.

1064
00:50:11,520 --> 00:50:14,320
How I would annotate it would be,

1065
00:50:14,320 --> 00:50:16,760
yeah, there's a beginning of location,

1066
00:50:16,760 --> 00:50:19,800
and here's another beginning of location, right?

1067
00:50:19,800 --> 00:50:21,120
This kind of makes sense, right?

1068
00:50:21,120 --> 00:50:23,640
I mean, here we have beginning and inside,

1069
00:50:23,640 --> 00:50:28,000
and then it would be Os maybe, and Os, which is clear.

1070
00:50:28,000 --> 00:50:32,400
Here, it's kind of awkward, but this is how they define,

1071
00:50:32,400 --> 00:50:36,540
and I got it wrong for a long period of time.

1072
00:50:36,540 --> 00:50:40,200
So there is many flavors of these BIO encoding,

1073
00:50:42,200 --> 00:50:45,860
and there is also others called below,

1074
00:50:47,200 --> 00:50:51,800
below, and BoA, and so on.

1075
00:50:51,800 --> 00:50:56,640
So just that you know, it doesn't matter

1076
00:50:56,640 --> 00:51:00,000
how you define the task, it has to be unambiguous.

1077
00:51:00,000 --> 00:51:02,780
So if you have two consecutive entities,

1078
00:51:02,780 --> 00:51:06,620
you have to make sure that this will be clear,

1079
00:51:06,620 --> 00:51:08,660
like what does it mean, what labeling means.

1080
00:51:08,660 --> 00:51:10,300
Whether this is B log and I log,

1081
00:51:10,300 --> 00:51:12,680
or B log and B log, and so on,

1082
00:51:12,680 --> 00:51:15,720
it's a matter of the actual data set and actual task.

1083
00:51:15,720 --> 00:51:17,920
But as I said, there is a standard data set,

1084
00:51:17,920 --> 00:51:22,920
so this is the, sorry, coming back, the CONELOT 2003,

1085
00:51:23,440 --> 00:51:26,280
they have four entities, person, organization,

1086
00:51:26,280 --> 00:51:29,880
location, and MISTs, like miscellaneous,

1087
00:51:29,880 --> 00:51:32,360
and they use this BIO encoding in the way

1088
00:51:32,360 --> 00:51:34,840
they describe here, so this is from the paper.

1089
00:51:34,840 --> 00:51:37,320
Kind of, yeah, their decision, basically.

1090
00:51:38,180 --> 00:51:39,440
Okay, any question, yeah.

1091
00:51:41,880 --> 00:51:43,200
By hand.

1092
00:51:43,200 --> 00:51:46,040
They took the, so the data set for this task

1093
00:51:46,040 --> 00:51:48,320
was taken from the Reuters corpus,

1094
00:51:48,320 --> 00:51:50,480
which you had to buy back then,

1095
00:51:50,480 --> 00:51:53,000
the license, you bought the Reuters corpus,

1096
00:51:53,000 --> 00:51:55,860
and they let people, maybe their authors

1097
00:51:55,860 --> 00:51:58,640
or somebody from their department annotate it per hand,

1098
00:51:58,640 --> 00:52:00,900
the entities, a lot of money back then, right?

1099
00:52:00,900 --> 00:52:04,600
So now you can download it for free,

1100
00:52:04,600 --> 00:52:06,680
like it's basically public domain, the data set.

1101
00:52:06,680 --> 00:52:08,820
It's been here for 20 years.

1102
00:52:08,820 --> 00:52:10,480
Any question to BIO tagging?

1103
00:52:14,120 --> 00:52:15,460
Okay, that's great.

1104
00:52:15,460 --> 00:52:17,620
So keep in mind, namely the reorganization

1105
00:52:17,620 --> 00:52:19,760
by BIO and sequence labeling.

1106
00:52:20,820 --> 00:52:21,720
Let's move on.

1107
00:52:23,000 --> 00:52:25,040
Superglue, okay.

1108
00:52:25,040 --> 00:52:28,680
So superglue, it's a popular benchmark,

1109
00:52:28,680 --> 00:52:30,640
well, three years, four years old now.

1110
00:52:32,160 --> 00:52:34,040
Collection, it's a collection of benchmarks.

1111
00:52:34,040 --> 00:52:35,740
It's not just a single task,

1112
00:52:35,740 --> 00:52:38,760
like name of the recognition or SNLI.

1113
00:52:38,760 --> 00:52:42,040
It's a collection of various tasks in English.

1114
00:52:42,040 --> 00:52:43,520
There's also multilingual superglue,

1115
00:52:43,520 --> 00:52:45,400
but this one is just in English.

1116
00:52:45,400 --> 00:52:46,920
And the goal of superglue is to provide

1117
00:52:46,920 --> 00:52:49,040
a simple, robust evolution metric

1118
00:52:49,040 --> 00:52:51,440
of any method capable of being applied

1119
00:52:51,440 --> 00:52:55,300
to a broad range of language understanding tasks.

1120
00:52:55,300 --> 00:53:00,300
So glue means general purpose language understanding, GLUA,

1121
00:53:02,080 --> 00:53:06,680
and super means just they took a glue and make it bigger.

1122
00:53:06,680 --> 00:53:10,500
So it's superglue, but everybody knows that.

1123
00:53:10,500 --> 00:53:11,640
So you should know as well.

1124
00:53:11,640 --> 00:53:13,940
Superglue or glue are just a standard task

1125
00:53:13,940 --> 00:53:14,880
for language understanding.

1126
00:53:14,880 --> 00:53:17,240
So if you see a paper or somebody is writing,

1127
00:53:17,240 --> 00:53:20,800
oh, we got human performance on superglue,

1128
00:53:20,800 --> 00:53:21,800
then you should know, okay,

1129
00:53:21,800 --> 00:53:24,400
superglue is a collection of different tasks,

1130
00:53:24,400 --> 00:53:28,000
and we'll show a couple of those in the next slides.

1131
00:53:28,000 --> 00:53:32,640
So it's like one data set which has multiple sub-data sets

1132
00:53:32,640 --> 00:53:34,740
of different kinds, okay?

1133
00:53:36,540 --> 00:53:39,840
And one of them is RTE, recognizing textual entitlement.

1134
00:53:39,840 --> 00:53:42,440
So what is it, textual entitlement?

1135
00:53:42,440 --> 00:53:45,480
We have binary classification,

1136
00:53:45,480 --> 00:53:49,340
and we have a text and the hypothesis,

1137
00:53:49,340 --> 00:53:53,080
and we have to say whether the text entiles the hypothesis,

1138
00:53:53,080 --> 00:53:56,440
yes or no, which means if you read the text

1139
00:53:56,440 --> 00:53:59,820
and the hypothesis, would you say the hypothesis is true?

1140
00:53:59,820 --> 00:54:02,120
So the text here would be Dana Reeve,

1141
00:54:02,120 --> 00:54:04,360
the widow of the actor Christopher Reeve,

1142
00:54:04,360 --> 00:54:06,720
has died of lung cancer at age 44

1143
00:54:06,720 --> 00:54:08,920
according to the Christopher Reeve Foundation.

1144
00:54:08,920 --> 00:54:10,240
This is the text.

1145
00:54:10,240 --> 00:54:12,880
The hypothesis is Christopher Reeve had an accident.

1146
00:54:13,880 --> 00:54:15,840
So if you read a text, would you say

1147
00:54:15,840 --> 00:54:17,460
the hypothesis is true or not?

1148
00:54:19,040 --> 00:54:22,560
It's not, so it's false, okay?

1149
00:54:22,560 --> 00:54:24,820
And now we say, oh, text and hypothesis,

1150
00:54:24,820 --> 00:54:25,960
I've seen it before.

1151
00:54:25,960 --> 00:54:28,320
I saw it in the natural language inference task.

1152
00:54:28,320 --> 00:54:30,640
So is it the same thing or not?

1153
00:54:30,640 --> 00:54:33,720
So Stanford SNLI basically adapted this,

1154
00:54:33,720 --> 00:54:35,400
recognizing textual entitlement.

1155
00:54:35,400 --> 00:54:37,120
So RTE is somehow older,

1156
00:54:38,040 --> 00:54:39,940
and only just a binary classification,

1157
00:54:39,940 --> 00:54:42,000
and SNLI was three ways,

1158
00:54:42,000 --> 00:54:45,360
so natural entitlement and contradiction.

1159
00:54:45,360 --> 00:54:47,620
So this is a very similar task,

1160
00:54:47,620 --> 00:54:48,660
and who defined the task?

1161
00:54:48,660 --> 00:54:49,500
Just the researchers.

1162
00:54:49,500 --> 00:54:52,140
They create a data set and say, like, this is RTE,

1163
00:54:52,140 --> 00:54:53,780
and then came people from Stanford and say,

1164
00:54:53,780 --> 00:54:55,900
oh, this is SNLI, it's a little bit different,

1165
00:54:55,900 --> 00:54:58,300
so we call it differently, right?

1166
00:54:59,800 --> 00:55:03,620
But this is the standard textual entitlement.

1167
00:55:03,620 --> 00:55:04,540
Any questions?

1168
00:55:06,420 --> 00:55:07,260
Okay, good.

1169
00:55:07,260 --> 00:55:11,060
Then, so this is part of SuperGLUE.

1170
00:55:11,060 --> 00:55:12,300
One of these tasks in SuperGLUE

1171
00:55:12,300 --> 00:55:13,700
is recognizing textual entitlement,

1172
00:55:13,700 --> 00:55:15,280
and what you need for this is, again,

1173
00:55:15,280 --> 00:55:18,400
understanding the semantic of one document and the other,

1174
00:55:18,400 --> 00:55:20,760
and how they relate to each other.

1175
00:55:20,760 --> 00:55:22,140
Whether if somebody,

1176
00:55:23,340 --> 00:55:24,260
if somebody,

1177
00:55:27,100 --> 00:55:31,620
yeah, if somebody who died on cancer from a foundation

1178
00:55:31,620 --> 00:55:34,780
has something to do with somebody else who had an accident.

1179
00:55:34,780 --> 00:55:37,380
So you need to kind of understand this connection, right?

1180
00:55:37,380 --> 00:55:40,240
So the model, if you wanna solve this task,

1181
00:55:40,240 --> 00:55:42,940
you need to understand this, how this works together,

1182
00:55:42,940 --> 00:55:46,080
some sort of word knowledge, I would say.

1183
00:55:47,940 --> 00:55:51,260
So another data set in SuperGLUE is coreference resolution,

1184
00:55:52,500 --> 00:55:55,140
where you have a sentence with a pronoun

1185
00:55:55,140 --> 00:55:57,100
and a list of non-phrases from the sentence,

1186
00:55:57,100 --> 00:55:58,180
and you have to determine

1187
00:55:58,180 --> 00:55:59,940
the correct reference of the pronoun.

1188
00:55:59,940 --> 00:56:02,000
So the example makes it clear.

1189
00:56:02,000 --> 00:56:03,140
The text reads,

1190
00:56:03,140 --> 00:56:05,840
Mark told Pete many lies about himself,

1191
00:56:05,840 --> 00:56:07,620
which Pete included in his book.

1192
00:56:07,620 --> 00:56:09,260
He should have been more truthful.

1193
00:56:10,340 --> 00:56:14,900
And the question is whether he is Pete or not.

1194
00:56:14,900 --> 00:56:17,340
So again, Mark told Pete many lies about himself,

1195
00:56:17,340 --> 00:56:19,060
which Pete included in his book.

1196
00:56:19,060 --> 00:56:20,760
He should have been more truthful.

1197
00:56:25,020 --> 00:56:26,660
And I'm wondering, wait a second.

1198
00:56:26,980 --> 00:56:29,060
I see my line right here.

1199
00:56:33,060 --> 00:56:34,000
Should be true, right?

1200
00:56:34,000 --> 00:56:35,900
I mean, he is Pete, isn't it?

1201
00:56:35,900 --> 00:56:36,740
It's Mark.

1202
00:56:37,860 --> 00:56:40,260
Mark told Pete, truthful.

1203
00:56:40,260 --> 00:56:41,840
Yeah, truthful, okay, yes.

1204
00:56:44,220 --> 00:56:46,100
Yeah, so the hard thing about this course,

1205
00:56:46,100 --> 00:56:47,860
it's like right after lunch.

1206
00:56:49,720 --> 00:56:51,800
And if you had Goulash with Pumas,

1207
00:56:52,480 --> 00:56:57,120
it puts you basically on random performance here

1208
00:56:57,120 --> 00:56:59,120
on these logical tasks.

1209
00:56:59,120 --> 00:57:01,680
But thanks for correcting.

1210
00:57:01,680 --> 00:57:04,360
What do you need for solving this is everyday,

1211
00:57:04,360 --> 00:57:06,040
common sense knowledge, common sense reasoning.

1212
00:57:06,040 --> 00:57:08,240
So this is a hard task for machines,

1213
00:57:08,240 --> 00:57:10,760
as you just saw for humans too sometimes.

1214
00:57:10,760 --> 00:57:13,240
So this is a reference resolution

1215
00:57:13,240 --> 00:57:14,400
Winograd Schema Challenge,

1216
00:57:14,400 --> 00:57:16,540
one of part of this, part of super new.

1217
00:57:16,540 --> 00:57:17,380
Any question?

1218
00:57:19,120 --> 00:57:19,960
Good.

1219
00:57:20,880 --> 00:57:24,200
And then we have, I'm showing, I guess,

1220
00:57:24,200 --> 00:57:26,800
the third from Superglue is BooQ,

1221
00:57:26,800 --> 00:57:29,080
which is a Boolean question answering.

1222
00:57:29,080 --> 00:57:31,160
So each example is a short passage

1223
00:57:31,160 --> 00:57:35,320
and yes and no question about the passage, yes.

1224
00:57:35,320 --> 00:57:37,020
So example, the question is,

1225
00:57:37,020 --> 00:57:39,320
has the UK been hit by a hurricane?

1226
00:57:39,320 --> 00:57:44,320
And the passage is, great storm of 1997

1227
00:57:44,640 --> 00:57:47,340
was a violent extra tropical cyclone,

1228
00:57:47,340 --> 00:57:49,220
which caused casualties in England, France,

1229
00:57:49,220 --> 00:57:51,780
and the Channel Island, blah, blah, blah.

1230
00:57:51,780 --> 00:57:54,740
And you have to say whether it's true or not.

1231
00:57:54,740 --> 00:57:57,820
So has the UK been hit by a hurricane

1232
00:57:57,820 --> 00:57:59,980
after reading this paragraph?

1233
00:57:59,980 --> 00:58:02,480
And the answer here is yes,

1234
00:58:02,480 --> 00:58:04,340
because there was like the great storm

1235
00:58:04,340 --> 00:58:06,260
and casualties in England, so okay.

1236
00:58:06,260 --> 00:58:09,580
So you need to understand that extra tropical cyclone

1237
00:58:09,580 --> 00:58:11,020
is a hurricane.

1238
00:58:11,020 --> 00:58:12,560
And then you have to understand

1239
00:58:13,980 --> 00:58:15,900
that UK is England and so on.

1240
00:58:15,900 --> 00:58:18,320
So a lot of kind of common sense understanding and reasoning.

1241
00:58:18,320 --> 00:58:19,600
So this is a task.

1242
00:58:19,600 --> 00:58:23,960
Non-factoid information or difficult entitlement

1243
00:58:23,960 --> 00:58:25,360
like inference to solve.

1244
00:58:25,360 --> 00:58:26,840
So this is what you need to know

1245
00:58:26,840 --> 00:58:31,840
or the machine needs to know in order to solve this task.

1246
00:58:31,880 --> 00:58:33,800
Okay, any question?

1247
00:58:33,800 --> 00:58:34,620
Yes.

1248
00:58:34,620 --> 00:58:39,620
Is it an entitlement or not?

1249
00:58:40,240 --> 00:58:41,660
Okay, this is a cool, yes.

1250
00:58:43,680 --> 00:58:44,720
Yes and no.

1251
00:58:44,720 --> 00:58:49,720
If you turn this into a statement, not a question,

1252
00:58:50,980 --> 00:58:53,400
the UKP has been hit by a hurricane

1253
00:58:53,400 --> 00:58:54,660
and this would be like the text

1254
00:58:54,660 --> 00:58:55,720
and this would be a hypothesis,

1255
00:58:55,720 --> 00:58:57,320
then it would be entails or not.

1256
00:58:58,200 --> 00:58:59,880
See, yes.

1257
00:58:59,880 --> 00:59:01,160
But you're asking a question.

1258
00:59:01,160 --> 00:59:03,720
So maybe when you ask a question,

1259
00:59:03,720 --> 00:59:05,960
you might give an example here as well.

1260
00:59:05,960 --> 00:59:07,320
So it's not maybe obligatory,

1261
00:59:07,320 --> 00:59:08,760
but you know, you kind of,

1262
00:59:08,760 --> 00:59:11,600
but it needs the same thing, yeah.

1263
00:59:11,600 --> 00:59:13,000
That's a good question, yes.

1264
00:59:15,720 --> 00:59:18,640
I don't think so.

1265
00:59:18,640 --> 00:59:20,440
I think it's just binary, yes and no.

1266
00:59:20,440 --> 00:59:21,720
It's a zero one.

1267
00:59:21,720 --> 00:59:24,200
So I guess it's not graded, but I haven't checked.

1268
00:59:24,200 --> 00:59:25,360
So you can have a look at the paper

1269
00:59:25,360 --> 00:59:27,960
and see whether they kind of take it into account.

1270
00:59:27,960 --> 00:59:28,960
I didn't check.

1271
00:59:28,960 --> 00:59:30,640
Yeah, it's a good point.

1272
00:59:30,640 --> 00:59:31,640
Okay, anything else?

1273
00:59:32,680 --> 00:59:33,640
Here, let's move on.

1274
00:59:34,760 --> 00:59:37,320
So the last one from squat,

1275
00:59:37,320 --> 00:59:41,200
sorry, last one from superglue is the multi-RC,

1276
00:59:41,200 --> 00:59:43,040
multi-sentence reading comprehension.

1277
00:59:44,040 --> 00:59:45,280
So what does it mean?

1278
00:59:45,280 --> 00:59:48,320
You have, again, a context paragraph,

1279
00:59:48,320 --> 00:59:50,880
you have questions or a question about a paragraph,

1280
00:59:50,880 --> 00:59:53,520
and then you have a list of possible answers

1281
00:59:53,520 --> 00:59:54,800
which are true or false.

1282
00:59:54,800 --> 00:59:57,440
I don't have an example here because it's super long,

1283
00:59:57,440 --> 00:59:59,160
but you see it's getting more complicated

1284
00:59:59,160 --> 01:00:02,080
because each possible correct answer,

1285
01:00:02,080 --> 01:00:04,400
so multiple possible correct answers,

1286
01:00:04,400 --> 01:00:07,840
so you have a list and each of them could be right or wrong.

1287
01:00:07,840 --> 01:00:10,240
And then answering each question,

1288
01:00:11,240 --> 01:00:13,160
they are independent of each other,

1289
01:00:13,160 --> 01:00:16,160
so you have to evaluate them by one.

1290
01:00:16,160 --> 01:00:17,080
So you have, I don't know,

1291
01:00:17,080 --> 01:00:18,680
an example paragraph about the hurricane,

1292
01:00:18,680 --> 01:00:21,640
and then you have like five statements or five questions,

1293
01:00:21,640 --> 01:00:24,600
and you need to have all of them correct,

1294
01:00:24,600 --> 01:00:27,320
and they kind of can be tricky.

1295
01:00:27,320 --> 01:00:29,400
And answering question requires drawing facts

1296
01:00:29,400 --> 01:00:31,320
from multiple context sentences.

1297
01:00:31,320 --> 01:00:33,720
So there's not only like single sentence in the document

1298
01:00:33,720 --> 01:00:36,400
but multiple, and you have to combine them together

1299
01:00:36,400 --> 01:00:38,600
in order to answer the question.

1300
01:00:38,880 --> 01:00:40,320
So it's getting more complicated.

1301
01:00:40,320 --> 01:00:42,920
You need really like reading comprehension,

1302
01:00:42,920 --> 01:00:45,040
so you need to comprehend the text,

1303
01:00:45,040 --> 01:00:46,680
and that's basically what you do also.

1304
01:00:46,680 --> 01:00:48,840
If you learn like second language and make a test,

1305
01:00:48,840 --> 01:00:50,200
there's also like reading comprehension

1306
01:00:50,200 --> 01:00:51,640
which you need to understand the language

1307
01:00:51,640 --> 01:00:53,400
and blah, blah, blah.

1308
01:00:53,400 --> 01:00:55,760
So this is multi-RC, kind of challenging task.

1309
01:00:57,280 --> 01:00:58,120
Any question?

1310
01:00:59,480 --> 01:01:00,320
Okay, cool.

1311
01:01:01,400 --> 01:01:03,360
And we're finishing,

1312
01:01:03,360 --> 01:01:06,640
I don't think this is part of superglue anymore,

1313
01:01:06,640 --> 01:01:09,640
so superglue was for these tasks different, there's more.

1314
01:01:09,640 --> 01:01:14,640
So you can have superglue is like maybe 10 tasks together

1315
01:01:15,000 --> 01:01:16,080
or maybe more,

1316
01:01:16,080 --> 01:01:19,040
but these were like the prototypical examples.

1317
01:01:19,040 --> 01:01:22,600
Now we're getting to question answering, which is a squat.

1318
01:01:22,600 --> 01:01:25,240
So I guess this stands for Stanford

1319
01:01:25,240 --> 01:01:26,640
question answering data set

1320
01:01:27,960 --> 01:01:30,920
because it was done by people from Stanford.

1321
01:01:32,200 --> 01:01:33,280
And here's an example.

1322
01:01:33,280 --> 01:01:35,360
So here's in paragraph,

1323
01:01:35,360 --> 01:01:36,600
other legislation followed

1324
01:01:36,600 --> 01:01:38,720
including Immigration Bird Conservation Act,

1325
01:01:38,720 --> 01:01:42,600
the treaty prohibiting the hunting of frite and gray whales,

1326
01:01:42,600 --> 01:01:45,360
blah, blah, something super boring from Wikipedia.

1327
01:01:45,360 --> 01:01:47,440
And then the question is like

1328
01:01:47,440 --> 01:01:49,760
which laws face significant opposition?

1329
01:01:49,760 --> 01:01:51,240
And you have the reference.

1330
01:01:51,240 --> 01:01:52,080
You have the paragraph

1331
01:01:52,080 --> 01:01:54,120
and you ask questions about this paragraph.

1332
01:01:54,960 --> 01:01:59,960
The point is these questions are kind of unanswerable.

1333
01:02:00,480 --> 01:02:03,880
You cannot answer them from this text,

1334
01:02:04,880 --> 01:02:08,480
but they have like plausible answers, which are incorrect

1335
01:02:08,480 --> 01:02:12,120
because there is this kind of a lexical,

1336
01:02:13,160 --> 01:02:15,040
the same words are used in this paragraph

1337
01:02:15,040 --> 01:02:16,280
as in the questions.

1338
01:02:16,280 --> 01:02:20,200
So it kind of tricks the machine to make shortcuts

1339
01:02:20,200 --> 01:02:23,200
and see, yeah, well, the answer should be later laws here,

1340
01:02:23,200 --> 01:02:24,040
but it's wrong.

1341
01:02:24,040 --> 01:02:27,040
Or here, what was the name of the treaty?

1342
01:02:27,040 --> 01:02:28,320
And if you just don't pay attention

1343
01:02:28,320 --> 01:02:30,200
and just, you know, keyboard spotting,

1344
01:02:30,200 --> 01:02:31,080
then you say, yeah, yeah, yeah,

1345
01:02:31,120 --> 01:02:34,160
it was this Bald Eagle Protection Act, which is wrong.

1346
01:02:34,160 --> 01:02:36,520
So it's kind of hard task for machines

1347
01:02:36,520 --> 01:02:38,800
not to jump on these, you know, quick keyword spotting

1348
01:02:38,800 --> 01:02:40,920
and just, oh, this is the answer.

1349
01:02:40,920 --> 01:02:42,640
So this is really interesting

1350
01:02:42,640 --> 01:02:45,400
because squat, the first version,

1351
01:02:45,400 --> 01:02:47,320
it had these issues like, you know,

1352
01:02:47,320 --> 01:02:49,040
keyword spotting and you can answer even like

1353
01:02:49,040 --> 01:02:53,080
without reading a lot of this paragraph.

1354
01:02:54,280 --> 01:02:55,120
Questions?

1355
01:02:58,080 --> 01:02:59,480
Yes.

1356
01:02:59,520 --> 01:03:02,520
Who exactly writes the question?

1357
01:03:02,520 --> 01:03:06,200
Does the same people who have to read the text

1358
01:03:06,200 --> 01:03:08,320
and then put down the answer,

1359
01:03:08,320 --> 01:03:10,400
or is there another entity?

1360
01:03:10,400 --> 01:03:14,120
So I guess, so the question is who writes these questions?

1361
01:03:15,320 --> 01:03:16,840
It was done by crowd workers.

1362
01:03:16,840 --> 01:03:19,200
So who knows the word crowd worker?

1363
01:03:20,960 --> 01:03:23,160
So, okay, what is crowd worker?

1364
01:03:23,160 --> 01:03:24,960
Somebody who didn't speak at all.

1365
01:03:24,960 --> 01:03:27,320
Who didn't speak at all and who's crowd worker?

1366
01:03:28,320 --> 01:03:29,880
Okay, so what is it?

1367
01:03:29,880 --> 01:03:31,000
Yes, crowd worker.

1368
01:03:44,360 --> 01:03:46,960
Exactly, so a small task for people on the internet.

1369
01:03:46,960 --> 01:03:48,400
So somebody is sitting at home and saying,

1370
01:03:48,400 --> 01:03:50,760
I want to earn $5 per hour

1371
01:03:50,760 --> 01:03:52,480
because I have to earn money somehow.

1372
01:03:52,480 --> 01:03:57,040
And then this is labeling cats and dogs and toxic comments.

1373
01:03:57,640 --> 01:03:58,920
And all these tasks are writing questions

1374
01:03:58,920 --> 01:04:01,320
about paragraphs on Wikipedia.

1375
01:04:01,320 --> 01:04:04,760
So the best, the biggest platform is Amazon Mechanical Turk

1376
01:04:04,760 --> 01:04:07,040
where you can, if you're living in the US,

1377
01:04:07,040 --> 01:04:08,400
I think you need social security number,

1378
01:04:08,400 --> 01:04:09,960
you can sign up as a worker.

1379
01:04:09,960 --> 01:04:12,280
If you want something to crowd source,

1380
01:04:12,280 --> 01:04:15,400
universities, researchers are just going there.

1381
01:04:15,400 --> 01:04:17,200
You have thousands of people

1382
01:04:17,200 --> 01:04:19,320
who are kind of get paid for this tiny job.

1383
01:04:19,320 --> 01:04:21,400
Maybe, you know, and you say, well, the task is,

1384
01:04:21,400 --> 01:04:24,080
here's a, you know, a paragraph from Wikipedia,

1385
01:04:24,080 --> 01:04:26,200
write a meaningful question about it.

1386
01:04:26,240 --> 01:04:27,840
And the person says, okay, wow.

1387
01:04:29,320 --> 01:04:31,880
Which laws facing the single positions or something like that?

1388
01:04:31,880 --> 01:04:34,120
I mean, you instruct these people to write it, right?

1389
01:04:34,120 --> 01:04:36,760
So crowd workers is sort of scalable thing

1390
01:04:36,760 --> 01:04:38,400
of annotating or creating data sets.

1391
01:04:38,400 --> 01:04:40,200
This was done by crowdsourcing.

1392
01:04:40,200 --> 01:04:41,800
Yeah, the whole data set.

1393
01:04:41,800 --> 01:04:43,080
Okay, any other questions?

1394
01:04:45,840 --> 01:04:49,560
Good, okay, so we've finished text classification

1395
01:04:49,560 --> 01:04:50,400
and now,

1396
01:04:51,400 --> 01:04:52,240
all right.

1397
01:04:53,680 --> 01:04:58,680
And now let's move on to text generation.

1398
01:04:59,120 --> 01:05:02,360
So what is text generation?

1399
01:05:02,360 --> 01:05:04,360
Well, I'll give you examples and you know what it means.

1400
01:05:04,360 --> 01:05:06,680
So machine translation.

1401
01:05:06,680 --> 01:05:09,520
Machine translation is still hard, even like in,

1402
01:05:09,520 --> 01:05:14,200
so I took this picture two months back in Tenerife

1403
01:05:14,200 --> 01:05:17,440
and there's a menu which says news of the month,

1404
01:05:17,440 --> 01:05:18,760
which is wrong already.

1405
01:05:18,760 --> 01:05:20,720
You know, this is translation of,

1406
01:05:20,720 --> 01:05:22,720
I guess from Spanish to English

1407
01:05:22,720 --> 01:05:24,920
and the translation to German is even longer,

1408
01:05:24,920 --> 01:05:27,600
nachricht in des monads on the menu.

1409
01:05:27,600 --> 01:05:29,800
I don't know, I don't wanna eat nachricht in des monads.

1410
01:05:29,800 --> 01:05:34,480
So yeah, so machine, and obviously they did like,

1411
01:05:34,480 --> 01:05:36,520
they use Google translate to translate this,

1412
01:05:36,520 --> 01:05:38,320
but if you don't have the context of the menu,

1413
01:05:38,320 --> 01:05:40,120
it will just, what's the news of the month?

1414
01:05:40,120 --> 01:05:42,480
Yeah, it's nachricht in des monads, of course.

1415
01:05:42,480 --> 01:05:44,120
If you don't know if it's on the menu, right?

1416
01:05:44,120 --> 01:05:45,960
So it's still hard problem.

1417
01:05:46,760 --> 01:05:49,000
And there are a couple of standard data sets

1418
01:05:49,000 --> 01:05:49,840
for machine translation.

1419
01:05:49,840 --> 01:05:51,040
So the task of machine translation,

1420
01:05:51,040 --> 01:05:53,840
obviously sentence input, sentence output mostly,

1421
01:05:53,840 --> 01:05:55,440
one language, another language.

1422
01:05:56,520 --> 01:06:00,240
And there's very many standard data sets from the WMT,

1423
01:06:00,240 --> 01:06:02,600
which used to be a workshop on machine translation.

1424
01:06:02,600 --> 01:06:04,880
Now it's called conference on machine translation,

1425
01:06:04,880 --> 01:06:08,400
but they're using the WMT shortcut for data sets.

1426
01:06:08,400 --> 01:06:11,960
So WMT 14 is for example, very famous data set

1427
01:06:11,960 --> 01:06:15,400
for English, German translation on like general topics.

1428
01:06:16,760 --> 01:06:19,880
So machine translation, what is so hard about it?

1429
01:06:19,880 --> 01:06:22,200
What is hard about machine translation?

1430
01:06:23,520 --> 01:06:24,960
Like conceptually?

1431
01:06:26,280 --> 01:06:29,000
Some words mean different things in different contexts.

1432
01:06:29,000 --> 01:06:30,920
Some words mean different things in different contexts

1433
01:06:30,920 --> 01:06:32,800
like nachrichten, of course, yes.

1434
01:06:32,800 --> 01:06:33,640
Anything else?

1435
01:06:33,640 --> 01:06:37,080
Anything which is like, sorry, you?

1436
01:06:37,080 --> 01:06:38,080
You.

1437
01:06:38,080 --> 01:06:41,720
Yeah, you have like a simonins and homonins

1438
01:06:41,720 --> 01:06:45,080
or words that have multiple meanings

1439
01:06:45,080 --> 01:06:49,520
or two words that mean the same things and.

1440
01:06:49,520 --> 01:06:51,240
That's true, you have multiple meanings

1441
01:06:51,240 --> 01:06:53,160
and homonins, simonins, but if you understand language,

1442
01:06:53,160 --> 01:06:54,720
you can translate to the target language.

1443
01:06:54,720 --> 01:06:57,520
I mean, I can speak English, but you know,

1444
01:06:57,520 --> 01:06:59,480
and I'm calling the message of obviously,

1445
01:06:59,480 --> 01:07:02,520
but some things in my mind are homonins as well.

1446
01:07:05,840 --> 01:07:07,480
Good, yeah, maybe the sound words don't exist.

1447
01:07:07,480 --> 01:07:08,360
So what do you do?

1448
01:07:10,480 --> 01:07:13,440
You have to explain them or say it differently, okay?

1449
01:07:13,440 --> 01:07:15,200
Anything else, like what is the real,

1450
01:07:15,200 --> 01:07:17,200
what's the issue of machine translation

1451
01:07:17,200 --> 01:07:18,800
if you wanna do it properly?

1452
01:07:20,280 --> 01:07:22,040
And if you have different structures,

1453
01:07:22,040 --> 01:07:25,360
some of them use words like beginning and homonins.

1454
01:07:25,360 --> 01:07:27,400
That's fine, but that's fine, but you can do that.

1455
01:07:27,400 --> 01:07:28,760
I mean, sure, I mean, you speak,

1456
01:07:28,760 --> 01:07:31,360
I guess you speak also like multiple languages at least,

1457
01:07:31,360 --> 01:07:33,880
so you have to learn these difficulties, yes, but.

1458
01:07:36,200 --> 01:07:37,040
Say it?

1459
01:07:37,560 --> 01:07:42,560
You can say it in German as well.

1460
01:07:50,120 --> 01:07:52,240
Okay, I'll let you just sometimes think about it.

1461
01:07:52,240 --> 01:07:54,400
Yes, okay, last attempt, sorry.

1462
01:08:04,400 --> 01:08:05,280
Yeah, that's true, yeah.

1463
01:08:05,280 --> 01:08:06,800
It's okay to say something in one language

1464
01:08:07,040 --> 01:08:07,880
and it sounds weird,

1465
01:08:07,880 --> 01:08:10,160
but if you do like proper translation with all these things,

1466
01:08:10,160 --> 01:08:14,040
you can translate idioms and they will work mostly or jokes.

1467
01:08:14,040 --> 01:08:17,200
The point is you have so many different options

1468
01:08:17,200 --> 01:08:18,240
to translate one thing

1469
01:08:19,200 --> 01:08:21,680
and you can't tell which one is better, mostly.

1470
01:08:21,680 --> 01:08:25,520
So there is one example from this book from Philip Cohen,

1471
01:08:25,520 --> 01:08:28,160
the translation of, sorry, I don't speak French.

1472
01:08:29,640 --> 01:08:32,000
Okay, can anyone read this?

1473
01:08:32,000 --> 01:08:32,960
Who speaks French?

1474
01:08:32,960 --> 01:08:33,800
Yes.

1475
01:08:36,840 --> 01:08:37,680
Thank you very much.

1476
01:08:37,680 --> 01:08:42,680
So here's so many translations here of that in English

1477
01:08:44,440 --> 01:08:47,840
and here's how people assess that

1478
01:08:47,840 --> 01:08:49,960
whether it's correct or wrong

1479
01:08:49,960 --> 01:08:51,520
and there's no agreement what's a correct

1480
01:08:51,520 --> 01:08:53,640
or not correct translation

1481
01:08:53,640 --> 01:08:55,360
because you can translate the same sentence

1482
01:08:55,360 --> 01:08:57,520
in so many different ways, right?

1483
01:08:57,520 --> 01:09:00,480
I mean, if you speak French and English,

1484
01:09:00,480 --> 01:09:02,000
you can kind of do it for yourself.

1485
01:09:02,000 --> 01:09:04,200
I can't, but they mean the same thing

1486
01:09:04,200 --> 01:09:05,040
but a little different,

1487
01:09:05,080 --> 01:09:06,880
there's a little bit of twist of that

1488
01:09:06,880 --> 01:09:10,240
and it's just hard to say what's correct and what's wrong

1489
01:09:10,240 --> 01:09:12,480
because you can say the same thing in many different ways

1490
01:09:12,480 --> 01:09:14,760
and maybe all of them are correct, maybe not

1491
01:09:14,760 --> 01:09:16,920
and people don't agree on that.

1492
01:09:16,920 --> 01:09:18,480
So people disagree on what's good.

1493
01:09:18,480 --> 01:09:20,840
I mean, some are clearly bad or correct

1494
01:09:20,840 --> 01:09:24,240
but there's disagreement, so it makes it hard

1495
01:09:24,240 --> 01:09:25,560
if you wanna do it with machines, right?

1496
01:09:25,560 --> 01:09:28,040
You wanna show an example and this is hard

1497
01:09:29,080 --> 01:09:31,720
on top of what you just said before, which is correct.

1498
01:09:32,680 --> 01:09:36,360
So, as I said, machine translation is hard.

1499
01:09:36,360 --> 01:09:37,560
We have standard data sets

1500
01:09:37,560 --> 01:09:39,680
and the evaluation of machine translation

1501
01:09:39,680 --> 01:09:43,080
or evaluation of generation is hard, as we will see later.

1502
01:09:43,080 --> 01:09:44,880
Any question on machine translation?

1503
01:09:47,040 --> 01:09:47,880
Okay, good.

1504
01:09:48,840 --> 01:09:52,800
So, now we have document summarization

1505
01:09:52,800 --> 01:09:54,200
or abstracted document summarization

1506
01:09:54,200 --> 01:09:56,520
which means you take a long document

1507
01:09:58,000 --> 01:10:00,280
which, for example, here in this CNN Daily Mail

1508
01:10:00,280 --> 01:10:02,480
standard data set for summarization,

1509
01:10:02,480 --> 01:10:07,480
you take a document which is 800 tokens long

1510
01:10:08,280 --> 01:10:09,680
and you wanna have a summary

1511
01:10:10,600 --> 01:10:14,440
which is roughly 50, 60 tokens on average.

1512
01:10:14,440 --> 01:10:17,160
So basically extracting the most information

1513
01:10:17,160 --> 01:10:21,080
but not extracting very badly in the tokens but rephrasing.

1514
01:10:21,080 --> 01:10:22,840
What's the gist of this document?

1515
01:10:24,120 --> 01:10:26,000
And it's hard.

1516
01:10:26,000 --> 01:10:27,440
Why is it hard?

1517
01:10:27,440 --> 01:10:28,720
You can, because you don't know

1518
01:10:28,720 --> 01:10:29,800
what's important in the document.

1519
01:10:30,280 --> 01:10:32,120
So we have multiple kind of scenarios

1520
01:10:32,120 --> 01:10:33,240
because everybody understands

1521
01:10:33,240 --> 01:10:34,440
something different in a document

1522
01:10:34,440 --> 01:10:37,800
and if I show you a news article and ask you,

1523
01:10:37,800 --> 01:10:39,200
can we write a summary?

1524
01:10:39,200 --> 01:10:41,080
I can guarantee you from 100 people here

1525
01:10:41,080 --> 01:10:43,280
I will get 80 different summaries,

1526
01:10:43,280 --> 01:10:45,560
100 different summaries mostly.

1527
01:10:45,560 --> 01:10:49,520
So it's also kind of complicated how to evaluate

1528
01:10:49,520 --> 01:10:50,720
but there's a training data set

1529
01:10:50,720 --> 01:10:53,520
which have some human-labeled summaries

1530
01:10:53,520 --> 01:10:55,440
and there's tricks how to do summarization,

1531
01:10:55,440 --> 01:10:56,880
how to do evaluation.

1532
01:10:56,880 --> 01:10:59,680
So it's a thing, it's a task, document summarization.

1533
01:11:00,400 --> 01:11:01,240
Why is it important?

1534
01:11:01,240 --> 01:11:02,080
Well, because people are lazy

1535
01:11:02,080 --> 01:11:03,480
and you wanna write the article.

1536
01:11:03,480 --> 01:11:05,920
Well, now we have ggpt so everything is solved

1537
01:11:05,920 --> 01:11:07,880
but maybe before you had an article

1538
01:11:07,880 --> 01:11:09,840
and you would just want to write a summary for the web

1539
01:11:09,840 --> 01:11:14,440
like short snippets, what the article is about automatically.

1540
01:11:14,440 --> 01:11:15,280
Why not?

1541
01:11:16,360 --> 01:11:17,240
So clear?

1542
01:11:17,240 --> 01:11:18,080
Any question?

1543
01:11:18,080 --> 01:11:18,920
Yes.

1544
01:11:18,920 --> 01:11:20,920
How would you validate it?

1545
01:11:20,920 --> 01:11:24,200
Because the machine could be like the same,

1546
01:11:24,200 --> 01:11:26,040
not like with different words.

1547
01:11:26,040 --> 01:11:28,120
How would you validate it?

1548
01:11:28,120 --> 01:11:29,280
How would you validate it?

1549
01:11:29,760 --> 01:11:30,600
What do you mean?

1550
01:11:31,800 --> 01:11:35,080
So there's a metrics for saying

1551
01:11:35,080 --> 01:11:37,120
this is a good summary or not.

1552
01:11:37,120 --> 01:11:38,000
And this is what you use.

1553
01:11:38,000 --> 01:11:41,200
You have some examples and you compare,

1554
01:11:41,200 --> 01:11:45,960
you have some documents and some summaries

1555
01:11:45,960 --> 01:11:49,560
written by humans and you let your system create summaries

1556
01:11:49,560 --> 01:11:50,880
and then you compare your summary

1557
01:11:50,880 --> 01:11:54,000
with the gold standard summary and there's metric

1558
01:11:54,000 --> 01:11:56,840
to say like, well, this is the same or not.

1559
01:11:56,840 --> 01:11:58,520
There's metrics to that

1560
01:11:58,520 --> 01:12:00,680
which has some flaws, we'll come to that later.

1561
01:12:00,680 --> 01:12:02,000
Okay?

1562
01:12:02,000 --> 01:12:02,840
Good.

1563
01:12:04,200 --> 01:12:06,120
And the last thing about generate,

1564
01:12:06,120 --> 01:12:07,520
there's so many things about generation

1565
01:12:07,520 --> 01:12:09,640
but the last one is the person not chat.

1566
01:12:09,640 --> 01:12:10,480
So dialogue.

1567
01:12:10,480 --> 01:12:11,440
So basically you have a dialogue

1568
01:12:11,440 --> 01:12:14,680
and you wanna do, predict the next returns.

1569
01:12:14,680 --> 01:12:17,440
So this is like the chit chat.

1570
01:12:17,440 --> 01:12:19,480
So you have two persons and they say,

1571
01:12:19,480 --> 01:12:20,880
hey, hello, how are you today?

1572
01:12:20,880 --> 01:12:21,960
Oh, I'm good, thank you.

1573
01:12:21,960 --> 01:12:22,800
How are you?

1574
01:12:22,800 --> 01:12:23,640
Oh, great, thanks.

1575
01:12:23,640 --> 01:12:25,600
My children, I just about to watch a Game of Thrones.

1576
01:12:25,600 --> 01:12:28,160
Oh, nice, blah, blah, blah, blah, blah, blah.

1577
01:12:28,640 --> 01:12:29,480
And the task could be, yeah,

1578
01:12:29,480 --> 01:12:32,560
just create the next sentence in this dialogue

1579
01:12:32,560 --> 01:12:35,000
because you maybe you're programming Alexa,

1580
01:12:35,000 --> 01:12:38,880
Amazon Alexa and you wanna entertain people about things.

1581
01:12:38,880 --> 01:12:41,200
And in this data set, they kind of create also personas.

1582
01:12:41,200 --> 01:12:44,040
So you know that person one is somebody who liked the sky

1583
01:12:44,040 --> 01:12:44,880
and blah, blah, blah,

1584
01:12:44,880 --> 01:12:47,000
and person two is doing something else.

1585
01:12:47,000 --> 01:12:49,480
So this is one of the data sets you wanna train

1586
01:12:49,480 --> 01:12:52,400
on just chit chat dialogues, right?

1587
01:12:53,440 --> 01:12:54,520
Yeah, it could be fun.

1588
01:12:54,520 --> 01:12:56,720
I mean, dialogue research, it's fun.

1589
01:12:56,720 --> 01:13:00,440
Then came chat GPT and just, oh, it's hard.

1590
01:13:00,440 --> 01:13:02,480
Okay, so this would be generation.

1591
01:13:04,360 --> 01:13:05,560
Now the thing is,

1592
01:13:05,560 --> 01:13:09,280
we did this distinction classification and generation,

1593
01:13:09,280 --> 01:13:13,840
but basically you can turn everything into generation.

1594
01:13:13,840 --> 01:13:14,760
How?

1595
01:13:14,760 --> 01:13:16,880
Well, you just tell it what to do.

1596
01:13:16,880 --> 01:13:20,560
So here's an example, translation English to German.

1597
01:13:20,560 --> 01:13:23,120
The input of your model could be verbically.

1598
01:13:23,120 --> 01:13:26,040
Translate English to German, that is good.

1599
01:13:26,040 --> 01:13:28,080
That's the input to the model.

1600
01:13:28,080 --> 01:13:30,680
And what you're expecting, this is good.

1601
01:13:31,920 --> 01:13:33,680
Yeah, this is machine translation, cool.

1602
01:13:33,680 --> 01:13:34,640
So nothing has changed.

1603
01:13:34,640 --> 01:13:36,680
Text in, text out.

1604
01:13:36,680 --> 01:13:38,240
How about here?

1605
01:13:38,240 --> 01:13:39,680
Nature language inference.

1606
01:13:39,680 --> 01:13:43,280
So NLI and MNLI means multilingual MNLI.

1607
01:13:43,280 --> 01:13:45,040
And you would write to the model.

1608
01:13:45,040 --> 01:13:48,480
MNLI premise, I hate pigeons.

1609
01:13:48,480 --> 01:13:51,040
And hypothesis, my feeling towards pigeons

1610
01:13:51,040 --> 01:13:52,240
are filled with animals.

1611
01:13:53,120 --> 01:13:56,440
And what you expect the model to output

1612
01:13:56,440 --> 01:13:59,240
is a word called, it's a word entitlement.

1613
01:14:00,400 --> 01:14:03,360
So it's not saying like zero, one or two, the label,

1614
01:14:03,360 --> 01:14:06,800
but it's basically spinning out the label as text, right?

1615
01:14:06,800 --> 01:14:09,640
It's doing basically the classification by saying,

1616
01:14:09,640 --> 01:14:12,400
if this were a tweet, you would say,

1617
01:14:12,400 --> 01:14:13,640
no, it is very toxic.

1618
01:14:13,640 --> 01:14:16,800
It was the hotline call.

1619
01:14:16,800 --> 01:14:20,920
You would say, hotline comment, period,

1620
01:14:20,920 --> 01:14:21,760
blah, blah, blah, blah.

1621
01:14:21,760 --> 01:14:24,480
Oh, I hate you, T-Mobile, blah, blah, blah.

1622
01:14:24,480 --> 01:14:28,040
And you would say, and you would finish maybe sentiment.

1623
01:14:28,920 --> 01:14:32,240
And the model would always put text saying negative.

1624
01:14:34,000 --> 01:14:35,840
This is how you turn basically classification

1625
01:14:35,840 --> 01:14:37,920
into the generation, but it's the same thing, right?

1626
01:14:37,920 --> 01:14:39,600
You can do that.

1627
01:14:39,600 --> 01:14:42,960
There's no basically difference, right?

1628
01:14:42,960 --> 01:14:43,840
Does it make sense?

1629
01:14:43,840 --> 01:14:46,760
Is it really like, oh, well, why should I generate text

1630
01:14:46,760 --> 01:14:48,320
because I can classify?

1631
01:14:48,320 --> 01:14:53,160
But there's models like GPT text in anything out

1632
01:14:53,160 --> 01:14:55,200
and you can do classification with them as well.

1633
01:14:55,200 --> 01:14:56,040
The very same way.

1634
01:14:56,040 --> 01:14:57,520
You just create a task as you say,

1635
01:14:57,520 --> 01:15:01,360
oh, premise is that and that, hypothesis is that and that

1636
01:15:01,360 --> 01:15:03,080
and spit out the decision.

1637
01:15:03,080 --> 01:15:06,240
If you do it to try with GPT, you know, try it.

1638
01:15:06,240 --> 01:15:08,120
What's the sentiment of this tweet?

1639
01:15:09,120 --> 01:15:11,760
And you write a tweet and GPT will spit out

1640
01:15:11,760 --> 01:15:15,200
maybe something longer, but maybe just, yeah, negative.

1641
01:15:15,200 --> 01:15:16,240
This is how it works.

1642
01:15:17,080 --> 01:15:18,920
Any questions?

1643
01:15:20,360 --> 01:15:21,680
This is important to understand.

1644
01:15:21,680 --> 01:15:24,760
Like you can turn everything into text from text to text,

1645
01:15:24,760 --> 01:15:27,680
even like regression, just outputs a number.

1646
01:15:33,680 --> 01:15:34,520
Yeah, prompting.

1647
01:15:34,520 --> 01:15:35,560
Yes, yeah.

1648
01:15:35,560 --> 01:15:37,200
No, no, it's just a different word from prompting,

1649
01:15:37,200 --> 01:15:40,600
but prompting is, it means something, zero shot,

1650
01:15:40,600 --> 01:15:42,080
one shot, blah, blah, blah, from things.

1651
01:15:42,080 --> 01:15:43,240
Yeah, this is prompting.

1652
01:15:43,240 --> 01:15:45,440
You can turn everything into text in text out.

1653
01:15:45,480 --> 01:15:46,320
That's important.

1654
01:15:47,440 --> 01:15:48,680
Okay, great.

1655
01:15:49,760 --> 01:15:52,600
So we know what we need to solve.

1656
01:15:52,600 --> 01:15:54,040
How good are we in that?

1657
01:15:54,040 --> 01:15:55,320
That's another question.

1658
01:15:55,320 --> 01:15:58,000
Okay, so let's move to evaluation a little bit.

1659
01:15:58,000 --> 01:16:00,480
And since most of people have here some, you know,

1660
01:16:00,480 --> 01:16:02,280
deep learning, machine learning and so on,

1661
01:16:02,280 --> 01:16:03,600
so I'm gonna just go quickly

1662
01:16:03,600 --> 01:16:05,040
and don't spend so much time on that.

1663
01:16:05,040 --> 01:16:06,640
If you have questions, just ask.

1664
01:16:07,480 --> 01:16:10,240
So we have these data sets, we have these tasks

1665
01:16:10,240 --> 01:16:15,240
and we wanna do some proper splitting on training

1666
01:16:15,840 --> 01:16:17,040
and validation and tests.

1667
01:16:17,040 --> 01:16:19,240
So basically we have training and test data.

1668
01:16:19,240 --> 01:16:22,880
So training, this is where we train our model and test data.

1669
01:16:22,880 --> 01:16:27,520
We never train our model on test data, never.

1670
01:16:27,520 --> 01:16:30,000
If there's one thing you should remember

1671
01:16:30,000 --> 01:16:31,040
when you leave the university,

1672
01:16:31,040 --> 01:16:32,800
never train your model on test data

1673
01:16:33,880 --> 01:16:35,680
because you train the model on test data,

1674
01:16:35,680 --> 01:16:36,880
the model will remember test data,

1675
01:16:36,880 --> 01:16:40,160
then you show, oh, I have 99% accuracy on test data

1676
01:16:41,200 --> 01:16:42,520
to make your boss happy.

1677
01:16:42,520 --> 01:16:45,240
And then you deploy somewhere and it just fail

1678
01:16:46,000 --> 01:16:46,840
because you can't, you know,

1679
01:16:46,840 --> 01:16:49,280
you just completely messed up your evaluation.

1680
01:16:49,280 --> 01:16:52,240
So these have to be like put into safe

1681
01:16:52,240 --> 01:16:55,560
and never ever touch before the final thing, okay.

1682
01:16:55,560 --> 01:16:56,880
This is really important.

1683
01:16:57,840 --> 01:17:00,360
So then you play around with training data and validation

1684
01:17:00,360 --> 01:17:02,040
so you can split it and, you know,

1685
01:17:02,040 --> 01:17:04,200
validation for hyperparameters tuning, okay.

1686
01:17:04,200 --> 01:17:08,000
So is it something that everybody kind of is familiar with

1687
01:17:08,000 --> 01:17:10,080
or somebody, yeah, okay.

1688
01:17:10,080 --> 01:17:11,520
This is a standard common sense.

1689
01:17:11,520 --> 01:17:12,920
So train that split.

1690
01:17:13,920 --> 01:17:15,440
Sometimes your data is small.

1691
01:17:15,440 --> 01:17:19,680
So you do cross-validation, which means you do K-fault,

1692
01:17:20,840 --> 01:17:22,320
sort of K-fault cross-validation.

1693
01:17:22,320 --> 01:17:24,880
So you split the data into K chunks.

1694
01:17:24,880 --> 01:17:28,040
And then for each chunks, you do train on some part

1695
01:17:28,040 --> 01:17:30,040
and test on the other part, right?

1696
01:17:30,040 --> 01:17:31,280
So you basically do five,

1697
01:17:31,280 --> 01:17:32,760
here's a five-fold cross-validation.

1698
01:17:32,760 --> 01:17:35,560
So some, you know, in first fault,

1699
01:17:35,560 --> 01:17:38,520
this part is test data, this part is and so on, so on, so on.

1700
01:17:38,520 --> 01:17:40,120
So cross-validation is a thing,

1701
01:17:40,120 --> 01:17:42,120
standard in machine learning.

1702
01:17:42,160 --> 01:17:43,000
To be honest, you know,

1703
01:17:43,000 --> 01:17:44,960
all these data sets we saw before,

1704
01:17:44,960 --> 01:17:49,320
like IMDB, SNLI, SuperGLUE and so on,

1705
01:17:49,320 --> 01:17:53,320
they have fixed frame depth test.

1706
01:17:53,320 --> 01:17:54,960
They don't do cross-validation.

1707
01:17:54,960 --> 01:17:55,920
For which reason?

1708
01:17:56,800 --> 01:17:59,160
Because you can really compare performance

1709
01:17:59,160 --> 01:18:03,080
on the same test data and on the same validation data.

1710
01:18:03,080 --> 01:18:04,400
If you do cross-validation randomly,

1711
01:18:04,400 --> 01:18:05,800
then you, oh, there's some randomness.

1712
01:18:05,800 --> 01:18:07,520
So, you know, you don't want to do it.

1713
01:18:07,520 --> 01:18:08,520
If you have small data

1714
01:18:08,520 --> 01:18:09,800
and you're playing around with new tasks,

1715
01:18:09,800 --> 01:18:11,960
maybe you want to do cross-validation.

1716
01:18:11,960 --> 01:18:12,840
Any questions?

1717
01:18:13,920 --> 01:18:14,760
Yes.

1718
01:18:16,520 --> 01:18:19,240
Yeah, so the standard tasks,

1719
01:18:19,240 --> 01:18:23,840
they have this frame, frame depth test.

1720
01:18:23,840 --> 01:18:25,080
They don't care about cross-validation

1721
01:18:25,080 --> 01:18:27,160
because in cross-validation,

1722
01:18:27,160 --> 01:18:29,080
yeah, you have to do some random splits

1723
01:18:29,080 --> 01:18:30,280
and the randomness will be different

1724
01:18:30,280 --> 01:18:31,440
from one person to another.

1725
01:18:31,440 --> 01:18:35,800
So it's better to have fixed, right?

1726
01:18:35,800 --> 01:18:37,960
Okay, any other question?

1727
01:18:37,960 --> 01:18:40,440
Okay, so I'm just, I put it just for reference there.

1728
01:18:40,440 --> 01:18:42,520
Cross-validation is a thing.

1729
01:18:42,520 --> 01:18:44,320
But now let's look into an evaluation

1730
01:18:44,320 --> 01:18:46,400
of test classification.

1731
01:18:46,400 --> 01:18:48,680
So classification is easy because we have,

1732
01:18:48,680 --> 01:18:51,080
let's, if we have two classes,

1733
01:18:51,080 --> 01:18:55,480
we can call them, we call them positive or negative,

1734
01:18:55,480 --> 01:18:57,480
or you can map everything into these two classes.

1735
01:18:57,480 --> 01:19:01,520
And then the thing is you can, the predictions here,

1736
01:19:01,520 --> 01:19:04,120
you can put them in a thing called confusion matrix,

1737
01:19:05,080 --> 01:19:07,560
where it, what it means is that it's a table.

1738
01:19:07,560 --> 01:19:09,240
So here, these will be numbers.

1739
01:19:11,280 --> 01:19:14,600
And then it's basically saying how many times

1740
01:19:14,600 --> 01:19:18,800
in my test set, something which was actually negative

1741
01:19:18,800 --> 01:19:22,200
was actually predict, it was predictive as negative.

1742
01:19:22,200 --> 01:19:26,320
And you make a, yeah, plus one here.

1743
01:19:26,320 --> 01:19:29,320
And then you collect the table for all your test data

1744
01:19:29,320 --> 01:19:31,440
and get a confusion matrix.

1745
01:19:31,440 --> 01:19:34,440
So one thing to remember is that, you know,

1746
01:19:34,440 --> 01:19:37,240
the ordering of columns here is arbitrary.

1747
01:19:37,760 --> 01:19:41,040
You know, it doesn't have to start with negative here

1748
01:19:41,040 --> 01:19:41,880
and positive here.

1749
01:19:41,880 --> 01:19:44,560
You can just flip them and flip the rows as well,

1750
01:19:44,560 --> 01:19:46,760
which will flip the true negatives, false positives,

1751
01:19:46,760 --> 01:19:48,200
false negatives, and true positives.

1752
01:19:48,200 --> 01:19:50,120
So it's super messy.

1753
01:19:50,120 --> 01:19:52,440
So anytime, I mean, you don't have to remember exactly

1754
01:19:52,440 --> 01:19:56,160
what is true, well, maybe it's not a bad thing to remember,

1755
01:19:56,160 --> 01:20:01,160
but I don't remember it either because you can flip that

1756
01:20:02,120 --> 01:20:03,800
and then just, yeah, what's left, right?

1757
01:20:03,800 --> 01:20:05,280
It's true, negative, I don't know.

1758
01:20:05,280 --> 01:20:07,200
So Wikipedia is your friend.

1759
01:20:08,200 --> 01:20:09,840
But just, you know, be aware of that.

1760
01:20:09,840 --> 01:20:12,680
So this is just one example how to do that.

1761
01:20:12,680 --> 01:20:16,600
So an example here would be, we have an classifier.

1762
01:20:16,600 --> 01:20:20,800
So here's an example of some disease detection.

1763
01:20:20,800 --> 01:20:23,320
So some binary task, negative and positive.

1764
01:20:23,320 --> 01:20:25,480
I do have a disease or I don't have a disease.

1765
01:20:25,480 --> 01:20:28,200
And these are the columns on the confusion matrix, right?

1766
01:20:28,200 --> 01:20:33,200
So it means that from, in my test data from 168 plus 33,

1767
01:20:37,360 --> 01:20:41,080
negative examples were classified as negative 168

1768
01:20:41,080 --> 01:20:43,000
and positive as 33.

1769
01:20:43,840 --> 01:20:47,080
So this is the confusion matrix, how much confused I were.

1770
01:20:47,080 --> 01:20:49,080
Any question to the confusion matrix?

1771
01:20:49,080 --> 01:20:50,000
Is it clear to everybody?

1772
01:20:50,000 --> 01:20:53,000
It's something you should spend some time kind of,

1773
01:20:53,000 --> 01:20:54,440
you know, working around and understanding

1774
01:20:54,440 --> 01:20:56,320
what confusion matrix is to understand

1775
01:20:56,320 --> 01:20:57,440
how to make evaluation,

1776
01:20:57,440 --> 01:20:59,440
but it's also not a rocket science.

1777
01:20:59,440 --> 01:21:02,840
So any questions, confusion matrix?

1778
01:21:04,600 --> 01:21:05,800
Okay, good.

1779
01:21:05,800 --> 01:21:10,120
So you have the confusion matrix for the binary task here

1780
01:21:10,120 --> 01:21:11,320
because we have two classes.

1781
01:21:11,320 --> 01:21:13,960
If we have three classes, I'll show later how it looks.

1782
01:21:13,960 --> 01:21:18,040
So one measure here is the accuracy.

1783
01:21:18,040 --> 01:21:19,560
So what is it saying?

1784
01:21:19,560 --> 01:21:21,120
It's only how accurate I was.

1785
01:21:21,120 --> 01:21:24,600
So the predictions which were correct over all predictions.

1786
01:21:24,600 --> 01:21:27,920
This is super fancy formula with some and blah, blah, blah.

1787
01:21:27,920 --> 01:21:31,080
Yeah, so basically you take the diagonal

1788
01:21:31,080 --> 01:21:32,360
of your confusion matrix

1789
01:21:33,360 --> 01:21:36,920
and divides by the total number of things.

1790
01:21:36,920 --> 01:21:40,240
So basically this is an identity function,

1791
01:21:40,240 --> 01:21:42,920
which means my prediction was something

1792
01:21:42,920 --> 01:21:44,800
and my gold label was something.

1793
01:21:44,800 --> 01:21:48,160
And if they're equal, this is plus one.

1794
01:21:48,160 --> 01:21:51,880
So this is one, and if they're not equal, this is zero.

1795
01:21:51,880 --> 01:21:53,680
So I'm computing all the matches here

1796
01:21:53,680 --> 01:21:58,080
and divide by all the numbers of instances in my test data.

1797
01:21:58,080 --> 01:21:59,720
Okay, and this is accuracy.

1798
01:21:59,720 --> 01:22:00,960
So here's the workout example,

1799
01:22:00,960 --> 01:22:03,680
which I suggest you to compute

1800
01:22:03,680 --> 01:22:05,360
and just play around with numbers.

1801
01:22:05,360 --> 01:22:07,760
The accuracy doesn't care about the other numbers here.

1802
01:22:07,760 --> 01:22:11,120
So these false predictions, whatever happens here,

1803
01:22:11,120 --> 01:22:12,360
it's not important.

1804
01:22:12,360 --> 01:22:13,360
You can flip these numbers

1805
01:22:13,360 --> 01:22:15,240
and the accuracy will be the same.

1806
01:22:15,240 --> 01:22:16,880
So everybody understands accuracy?

1807
01:22:18,560 --> 01:22:21,520
Basically how many times I go to drive in the test set?

1808
01:22:22,600 --> 01:22:24,240
That's as simple as it is.

1809
01:22:24,240 --> 01:22:25,680
And it's between zero and one.

1810
01:22:25,680 --> 01:22:30,680
So 100% is one or 100% is 100% accuracy is the best system.

1811
01:22:30,680 --> 01:22:31,760
It works perfectly.

1812
01:22:33,360 --> 01:22:37,360
If you have binary tasks and it's balanced data set,

1813
01:22:37,360 --> 01:22:39,360
50% would be like the random accuracy.

1814
01:22:41,000 --> 01:22:43,800
So accuracy has some issues.

1815
01:22:43,800 --> 01:22:46,240
If you have skewed data set, it won't work that well.

1816
01:22:46,240 --> 01:22:48,520
So we have another metrics

1817
01:22:48,520 --> 01:22:51,000
which are called precision recall and F1 score.

1818
01:22:51,000 --> 01:22:52,240
Is there anyone who hasn't heard

1819
01:22:52,240 --> 01:22:54,160
about precision recall and F1 score?

1820
01:22:55,320 --> 01:22:56,680
Okay, good.

1821
01:22:56,680 --> 01:23:01,040
So it's computing.

1822
01:23:01,040 --> 01:23:02,920
So what is precision doing?

1823
01:23:02,920 --> 01:23:05,360
And now we have an example of precision for class positive

1824
01:23:05,360 --> 01:23:08,000
because it will be different for class negative

1825
01:23:08,000 --> 01:23:11,480
because these kind of ordering is arbitrary.

1826
01:23:12,400 --> 01:23:14,080
So for class positive, I'm here

1827
01:23:15,080 --> 01:23:16,960
and I wanna compute a precision for that.

1828
01:23:16,960 --> 01:23:18,180
So I'm saying,

1829
01:23:18,180 --> 01:23:23,180
whatever I predicted here as positive.

1830
01:23:27,300 --> 01:23:29,780
So I predicted something positive.

1831
01:23:31,220 --> 01:23:33,460
I predicted positive the correct things

1832
01:23:33,460 --> 01:23:37,420
and predicted as positive something which wasn't positive.

1833
01:23:37,420 --> 01:23:39,300
So I'm saying how many times

1834
01:23:39,300 --> 01:23:43,460
did I correctly predict it positive if I set it positive?

1835
01:23:43,460 --> 01:23:45,140
So I'm taking the true positive

1836
01:23:46,140 --> 01:23:50,020
over this sum.

1837
01:23:51,180 --> 01:23:52,860
So saying like how precise I was.

1838
01:23:52,860 --> 01:23:55,700
My classifier is set, this is positive.

1839
01:23:56,940 --> 01:23:58,940
Which part of that was really positive?

1840
01:23:59,780 --> 01:24:02,100
This is what precision is doing.

1841
01:24:02,100 --> 01:24:05,700
Recall is saying, yeah, I have my data here.

1842
01:24:05,700 --> 01:24:09,860
So all of this in my data set

1843
01:24:09,860 --> 01:24:13,740
is which is positive in my data.

1844
01:24:13,820 --> 01:24:15,940
These are actually positive in my data set.

1845
01:24:17,140 --> 01:24:20,180
How much I found of them.

1846
01:24:21,100 --> 01:24:26,020
So this is the, I take the true positive

1847
01:24:26,020 --> 01:24:30,140
and divide by the sum of all of these positive

1848
01:24:30,140 --> 01:24:31,380
in my data set.

1849
01:24:31,380 --> 01:24:35,500
So I have 500 documents which are positive in my data set

1850
01:24:35,500 --> 01:24:38,540
and I found only 100 of them.

1851
01:24:39,380 --> 01:24:42,820
So my recall is just one over five.

1852
01:24:42,820 --> 01:24:44,620
I didn't find all.

1853
01:24:44,620 --> 01:24:46,740
And these two are kind of complimentary

1854
01:24:47,820 --> 01:24:49,380
and you can combine them into one score

1855
01:24:49,380 --> 01:24:52,140
which is called the F1 measure, F1 score.

1856
01:24:52,140 --> 01:24:56,060
And it's this harmonic mean of these two measures.

1857
01:24:56,060 --> 01:24:59,020
So you might seen it before,

1858
01:24:59,020 --> 01:25:01,740
something to know, to remember and know how it works.

1859
01:25:01,740 --> 01:25:02,860
Why is it important?

1860
01:25:02,860 --> 01:25:07,540
So it's important because it takes into account

1861
01:25:07,540 --> 01:25:09,180
two different versions of your classifier.

1862
01:25:09,180 --> 01:25:11,300
You wanna maybe precise classifier,

1863
01:25:11,300 --> 01:25:13,620
you wanna maybe more recolor in the classifier

1864
01:25:13,620 --> 01:25:16,180
or you wanna combine into one measure.

1865
01:25:16,180 --> 01:25:17,060
Any questions?

1866
01:25:19,500 --> 01:25:20,860
Great.

1867
01:25:20,860 --> 01:25:24,980
So the confusion matrix here is for just for two classes.

1868
01:25:24,980 --> 01:25:27,420
Obviously you might have multi-class problem

1869
01:25:27,420 --> 01:25:29,620
and here this is taken from the Reuters data sets

1870
01:25:29,620 --> 01:25:34,420
which is newswire classification in two different topics.

1871
01:25:34,420 --> 01:25:36,420
And here the confusion matrix is basically the same.

1872
01:25:36,420 --> 01:25:39,980
So you have the true class and here are the predictions.

1873
01:25:40,020 --> 01:25:42,300
And if we are compute to create the accuracy,

1874
01:25:42,300 --> 01:25:44,260
so what's the accuracy of the classifier?

1875
01:25:44,260 --> 01:25:46,820
It's basically sum of the diagonal.

1876
01:25:49,700 --> 01:25:51,260
Oh wow, sorry.

1877
01:25:52,180 --> 01:25:56,140
Sum of the diagonal over all these,

1878
01:25:56,140 --> 01:26:00,420
sum of all these things would be the accuracy.

1879
01:26:00,420 --> 01:26:03,060
I mean what's going, what's right

1880
01:26:03,060 --> 01:26:06,140
and over all the instances in my test data.

1881
01:26:07,180 --> 01:26:09,620
How would you turn this into precision and recall?

1882
01:26:10,100 --> 01:26:11,500
Okay, this is interesting because precision and recall,

1883
01:26:11,500 --> 01:26:14,380
we define like true positives, true negatives,

1884
01:26:14,380 --> 01:26:15,500
but these were just two classes.

1885
01:26:15,500 --> 01:26:19,460
So how do I do it for a bigger matrix?

1886
01:26:19,460 --> 01:26:22,420
I have one, two, three, four, five, six classes.

1887
01:26:22,420 --> 01:26:23,700
So what I'm going to do?

1888
01:26:26,220 --> 01:26:30,020
Yeah, so basically you take class of your interest,

1889
01:26:30,020 --> 01:26:35,020
which will be interest maybe and merge the rest

1890
01:26:35,020 --> 01:26:38,020
and the rest will be merged into one row and one column.

1891
01:26:38,020 --> 01:26:41,220
And this is how you create basically two by two matrix.

1892
01:26:42,420 --> 01:26:43,780
This is how you compute it.

1893
01:26:47,420 --> 01:26:50,540
And then you compute this for each class

1894
01:26:50,540 --> 01:26:55,540
and then you can take an average of these F1 scores

1895
01:26:56,300 --> 01:26:58,020
or do some other things.

1896
01:26:58,020 --> 01:27:00,380
So this is kind of technical details,

1897
01:27:00,380 --> 01:27:01,220
but it might get tricky.

1898
01:27:01,220 --> 01:27:03,100
So always report what you do.

1899
01:27:04,820 --> 01:27:05,980
I recommend this paper.

1900
01:27:05,980 --> 01:27:07,580
If you're interested in more like how tricky

1901
01:27:08,140 --> 01:27:10,580
this could be, have a look at this paper

1902
01:27:10,580 --> 01:27:13,220
because all these tiny details make a difference

1903
01:27:13,220 --> 01:27:14,500
whether you have the accuracy of,

1904
01:27:14,500 --> 01:27:19,140
or micro F1 score of 80 or 85 or 90,

1905
01:27:19,140 --> 01:27:22,060
make sure you're comparing apples to apples.

1906
01:27:22,060 --> 01:27:23,660
Okay, so this is the evaluations.

1907
01:27:23,660 --> 01:27:26,460
Okay, any questions so far?

1908
01:27:26,460 --> 01:27:28,180
So if you've seen this before machine learning,

1909
01:27:28,180 --> 01:27:31,180
it's nothing new, like the same stuff will run over.

1910
01:27:32,140 --> 01:27:33,300
Evaluating this generation.

1911
01:27:33,300 --> 01:27:34,380
Okay, we're running out of time,

1912
01:27:34,380 --> 01:27:37,220
but let me just, okay, seven minutes and we're good.

1913
01:27:37,220 --> 01:27:38,660
It's okay?

1914
01:27:38,660 --> 01:27:40,500
Good, yeah, we're almost there.

1915
01:27:42,180 --> 01:27:46,140
Yes, so evolution of text generation is hard.

1916
01:27:49,220 --> 01:27:50,060
Why?

1917
01:27:50,060 --> 01:27:51,060
Because you have different metrics

1918
01:27:51,060 --> 01:27:53,860
and I'll let me just scare you off a little bit.

1919
01:27:53,860 --> 01:27:56,100
So this is just a part of one large table

1920
01:27:56,100 --> 01:27:58,140
of different metrics for text generation.

1921
01:27:59,700 --> 01:28:04,700
So evaluating text generation is a matter of research

1922
01:28:05,620 --> 01:28:07,500
and not all these metrics here,

1923
01:28:07,500 --> 01:28:10,580
so there could be like, I don't know how many, 40 metrics

1924
01:28:10,580 --> 01:28:12,260
and they have pros and cons.

1925
01:28:12,260 --> 01:28:15,780
So it's part, it's basically a research topic.

1926
01:28:15,780 --> 01:28:19,140
Classification is easy, match or not match, cool.

1927
01:28:19,140 --> 01:28:21,340
Generation, yeah, you can translate it

1928
01:28:21,340 --> 01:28:24,020
and summarization and oh, it's hard.

1929
01:28:24,020 --> 01:28:24,840
So there's metrics.

1930
01:28:24,840 --> 01:28:28,540
I'm gonna show you two of them, which are like super easy,

1931
01:28:28,540 --> 01:28:31,060
but you don't have to remember the formulas

1932
01:28:31,060 --> 01:28:33,420
because if you were to implement it,

1933
01:28:33,420 --> 01:28:35,220
then you better look up the papers.

1934
01:28:36,140 --> 01:28:38,780
So one of them, but you should know the name,

1935
01:28:38,780 --> 01:28:43,180
is BLEU, B-L-E-U, Bilingual Evolution Understudy.

1936
01:28:43,180 --> 01:28:44,860
It's like the first and most popular metrics

1937
01:28:44,860 --> 01:28:49,380
for machine translation and it computes the engram overlap

1938
01:28:49,380 --> 01:28:51,660
between the reference and the hypothesis.

1939
01:28:51,660 --> 01:28:54,020
So you have a reference translation

1940
01:28:54,020 --> 01:28:55,740
and you have the hypothesis which is a translation

1941
01:28:55,740 --> 01:28:58,500
of your machine translation system

1942
01:28:58,500 --> 01:29:00,660
and you're basically comparing how much

1943
01:29:00,660 --> 01:29:02,900
there is an overlap of engrams.

1944
01:29:03,380 --> 01:29:04,220
What is an engram?

1945
01:29:05,660 --> 01:29:06,500
What is a bigram?

1946
01:29:06,500 --> 01:29:07,420
What is a bigram?

1947
01:29:09,180 --> 01:29:11,540
There are two characters in each of those

1948
01:29:11,540 --> 01:29:14,060
combined as one token.

1949
01:29:14,060 --> 01:29:15,340
Two characters, I would say,

1950
01:29:15,340 --> 01:29:16,860
this would be like character bigram.

1951
01:29:16,860 --> 01:29:18,860
So just standard engram would be two tokens

1952
01:29:18,860 --> 01:29:19,680
next to each other.

1953
01:29:19,680 --> 01:29:20,520
It's a bigram.

1954
01:29:20,520 --> 01:29:23,860
For trigram would be three tokens next to each other, okay?

1955
01:29:23,860 --> 01:29:27,060
So you would compare like pairs of words, the bigrams

1956
01:29:27,060 --> 01:29:29,300
and how much there is an overlap.

1957
01:29:29,300 --> 01:29:30,420
Yeah, it looks easy.

1958
01:29:31,420 --> 01:29:34,420
There is one score of the corpus,

1959
01:29:34,420 --> 01:29:37,900
but the drawback, it's just a precision based.

1960
01:29:37,900 --> 01:29:40,860
So you don't count for recall

1961
01:29:40,860 --> 01:29:43,780
and there's an exact engram matching.

1962
01:29:43,780 --> 01:29:46,140
So there has to be like exact translation

1963
01:29:46,140 --> 01:29:49,380
and in translation, you can use some different spelling

1964
01:29:49,380 --> 01:29:51,900
maybe or different kind of wording of the same thing.

1965
01:29:51,900 --> 01:29:54,780
So if there is no match, the blue score doesn't work.

1966
01:29:56,100 --> 01:29:59,260
So there's another metric which is oriented on recall

1967
01:29:59,260 --> 01:30:00,900
and it's rouge.

1968
01:30:00,900 --> 01:30:03,780
If you're wondering why all these machine translation

1969
01:30:03,780 --> 01:30:08,780
evaluation metrics are frame sounding, I have no idea.

1970
01:30:09,780 --> 01:30:10,980
It's maybe the part of the creative.

1971
01:30:10,980 --> 01:30:12,940
If you're like, you know, all the transformers like Bert,

1972
01:30:12,940 --> 01:30:16,860
Roberta, Camembert, you know, all these on streets things.

1973
01:30:18,140 --> 01:30:19,180
Maybe it was fun.

1974
01:30:19,180 --> 01:30:20,980
So it's rouge, so rouge.

1975
01:30:20,980 --> 01:30:23,420
Recall-oriented understanding of gisting evaluation.

1976
01:30:23,420 --> 01:30:24,540
It's a terrible name.

1977
01:30:24,540 --> 01:30:25,660
It's similar to blue.

1978
01:30:25,660 --> 01:30:27,400
I mean, there's different variants,

1979
01:30:27,400 --> 01:30:28,700
but counting in the engram matches

1980
01:30:29,180 --> 01:30:30,020
between the hypothesis and reference.

1981
01:30:30,020 --> 01:30:32,340
So it's the same, but it's recall-based measure.

1982
01:30:32,340 --> 01:30:34,420
So there's different formula and it's just,

1983
01:30:34,420 --> 01:30:39,260
it has some other kind of properties, right?

1984
01:30:39,260 --> 01:30:41,880
And it's measuring the longest common subsequence

1985
01:30:41,880 --> 01:30:43,140
between a pair of sentences.

1986
01:30:43,140 --> 01:30:44,620
You have a question?

1987
01:30:44,620 --> 01:30:47,460
Does this become order in the engram?

1988
01:30:47,460 --> 01:30:48,900
Does that maybe some-

1989
01:30:48,900 --> 01:30:51,180
The ordering of engrams, whether it counts,

1990
01:30:51,180 --> 01:30:55,140
well, it doesn't matter where the engram occurs, you know?

1991
01:30:55,140 --> 01:30:57,280
So you can have the engram in the reference at the beginning

1992
01:30:57,280 --> 01:30:58,580
and the hypothesis at the end,

1993
01:30:58,580 --> 01:31:00,480
because you can translate a sentence.

1994
01:31:02,220 --> 01:31:06,080
Yesterday, I was at cinema, or I was at cinema yesterday.

1995
01:31:07,080 --> 01:31:08,760
And you have yesterday at the beginning and the end.

1996
01:31:08,760 --> 01:31:11,900
So there's like unigram overlap, which is fine.

1997
01:31:11,900 --> 01:31:13,720
So it takes this into account, like,

1998
01:31:13,720 --> 01:31:17,640
or the bigram is more like context, contextualized.

1999
01:31:17,640 --> 01:31:18,920
Okay, another question?

2000
01:31:19,860 --> 01:31:22,800
So remember, blue and rouge are the standard metrics

2001
01:31:22,800 --> 01:31:25,080
and they have pros and cons, mostly cons.

2002
01:31:25,080 --> 01:31:27,520
So that's why there's already different metrics

2003
01:31:27,520 --> 01:31:28,360
which are better.

2004
01:31:29,440 --> 01:31:32,720
And it's super hard, I mean, it's easy to hack,

2005
01:31:32,720 --> 01:31:33,860
well, it's not easy to hack them,

2006
01:31:33,860 --> 01:31:37,240
but if you improve blue score, it doesn't matter.

2007
01:31:37,240 --> 01:31:40,480
It doesn't mean that human perception of your evaluation,

2008
01:31:40,480 --> 01:31:43,740
of your machine translation, is really better.

2009
01:31:43,740 --> 01:31:45,700
So there is some correlation between the blue scores

2010
01:31:45,700 --> 01:31:47,920
and how humans perceive the evaluation,

2011
01:31:47,920 --> 01:31:49,460
but it's not 100%.

2012
01:31:49,460 --> 01:31:50,880
That's why there is research on that,

2013
01:31:50,880 --> 01:31:53,760
because you wanna have better translation for humans,

2014
01:31:53,760 --> 01:31:55,400
not just because of blue score.

2015
01:31:55,400 --> 01:31:57,880
You know, the advantage of the blue score is

2016
01:31:57,880 --> 01:31:59,200
you can do it automatically.

2017
01:31:59,200 --> 01:32:00,380
You don't need a human to say,

2018
01:32:00,380 --> 01:32:03,440
oh, this is better translation than the other.

2019
01:32:03,440 --> 01:32:06,240
Eventually, in language generation,

2020
01:32:06,240 --> 01:32:08,360
you want humans to look into the generated text.

2021
01:32:08,360 --> 01:32:10,560
So, you know, kind of, but it's costly

2022
01:32:10,560 --> 01:32:11,700
and time consuming and so on.

2023
01:32:11,700 --> 01:32:13,300
So that's why there are metrics.

2024
01:32:14,520 --> 01:32:16,840
So there's some caveats of NLP benchmarking.

2025
01:32:16,840 --> 01:32:20,160
And I listed like, I guess, two or three,

2026
01:32:20,160 --> 01:32:21,120
and then we're done.

2027
01:32:22,120 --> 01:32:25,480
So we talked about the gold data paradigm.

2028
01:32:25,480 --> 01:32:29,200
So, and it's easy, like we have like one ground truth.

2029
01:32:30,140 --> 01:32:32,340
And it makes sense when humans highly agree on the answer.

2030
01:32:32,340 --> 01:32:34,460
So if the annotation task would be,

2031
01:32:34,460 --> 01:32:35,980
does this image contain a bird,

2032
01:32:35,980 --> 01:32:37,520
or is it, is learn a verb,

2033
01:32:37,520 --> 01:32:38,800
or what is the capital of Italy,

2034
01:32:38,800 --> 01:32:40,600
people will agree, and this is easy.

2035
01:32:40,600 --> 01:32:41,680
We have gold standard.

2036
01:32:42,760 --> 01:32:47,760
But if, for example, something which is more,

2037
01:32:48,120 --> 01:32:50,800
yeah, which is more subjective,

2038
01:32:50,800 --> 01:32:54,160
is for example, is this comments on Facebook toxic or not?

2039
01:32:54,160 --> 01:32:55,640
Well, people will disagree maybe,

2040
01:32:55,640 --> 01:32:58,240
because it's depending on your subjectivity.

2041
01:32:58,240 --> 01:33:01,740
And, or understanding of the language.

2042
01:33:01,740 --> 01:33:03,920
And we kind of tend to, you know,

2043
01:33:03,920 --> 01:33:07,200
kind of get rid of these hard cases

2044
01:33:07,200 --> 01:33:10,400
or non-agreed upon things.

2045
01:33:10,400 --> 01:33:13,200
But now there's research on that actually

2046
01:33:13,200 --> 01:33:16,860
these human label variation is an opportunity,

2047
01:33:16,860 --> 01:33:17,700
not a problem.

2048
01:33:18,580 --> 01:33:19,860
So this is something which we should maybe

2049
01:33:19,860 --> 01:33:21,540
took into account and not just pretend,

2050
01:33:21,540 --> 01:33:22,940
oh, there's one gold truth.

2051
01:33:23,860 --> 01:33:25,480
So we should have maybe some variation.

2052
01:33:25,480 --> 01:33:27,860
This is basically a part of current research

2053
01:33:27,860 --> 01:33:30,580
by the group of Barbara Plunk in Munich.

2054
01:33:31,660 --> 01:33:34,360
So remember, if there is a gold standard,

2055
01:33:34,360 --> 01:33:36,420
well, yeah, somebody has to create a gold standard,

2056
01:33:36,420 --> 01:33:39,660
and they took some arbitrary decisions on the way.

2057
01:33:39,660 --> 01:33:44,120
So maybe, you know, it was a hard choice.

2058
01:33:44,120 --> 01:33:46,520
Human annotators are biased,

2059
01:33:46,520 --> 01:33:48,360
because the data sets are constructed

2060
01:33:48,360 --> 01:33:51,600
by small number of annotators and humans are biased.

2061
01:33:51,600 --> 01:33:54,680
So if you, you know, you go to mechanical Turk,

2062
01:33:55,680 --> 01:33:56,680
and at the end of the day,

2063
01:33:56,680 --> 01:33:58,840
you have maybe a hundred workers working for you,

2064
01:33:58,840 --> 01:34:02,680
all these 600,000 Stanford and allied, you know,

2065
01:34:02,680 --> 01:34:06,160
data points was done by, I don't know, 200 workers,

2066
01:34:06,160 --> 01:34:08,760
which means you get some bias of these people

2067
01:34:08,760 --> 01:34:09,980
into the data set.

2068
01:34:09,980 --> 01:34:11,320
So people are biased.

2069
01:34:13,000 --> 01:34:15,760
And it has some consequences maybe

2070
01:34:15,760 --> 01:34:19,440
if you let the data create by somebody else,

2071
01:34:19,440 --> 01:34:22,200
so by annotators that did not contribute to the training set

2072
01:34:22,200 --> 01:34:24,240
kind of influences your performance of the model.

2073
01:34:24,240 --> 01:34:26,240
So you have some bias in the data,

2074
01:34:26,240 --> 01:34:28,240
which is inherent to the humans.

2075
01:34:28,240 --> 01:34:30,140
Yeah, take that into mind as well.

2076
01:34:31,640 --> 01:34:33,220
We might have artifacts in the, sorry,

2077
01:34:33,220 --> 01:34:36,360
artifacts in the data set, so some spurious correlation.

2078
01:34:36,360 --> 01:34:38,460
So we created data sets,

2079
01:34:38,460 --> 01:34:40,120
which I'm not going to introduce in the detail,

2080
01:34:40,120 --> 01:34:45,120
but there was a, so logical reasoning and arguments,

2081
01:34:46,320 --> 01:34:48,220
which is, I don't have time to go into detail,

2082
01:34:48,220 --> 01:34:52,300
but we have, we had a lot of negations there.

2083
01:34:52,300 --> 01:34:54,240
And some other researchers look at our data set

2084
01:34:54,240 --> 01:34:58,740
and said like, yeah, if you delete the negation,

2085
01:34:59,840 --> 01:35:03,040
basically all the performance of your classifier

2086
01:35:03,040 --> 01:35:04,880
will just drop to random,

2087
01:35:04,880 --> 01:35:07,120
because the classifiers are just picking up

2088
01:35:07,120 --> 01:35:09,920
on these artifacts or spurious correlation in data,

2089
01:35:09,920 --> 01:35:13,360
and if they see a negation don't or not,

2090
01:35:13,360 --> 01:35:16,160
they just kind of predict the final class,

2091
01:35:16,160 --> 01:35:17,240
but it doesn't have to do anything

2092
01:35:17,240 --> 01:35:19,720
with understanding the actual task.

2093
01:35:19,720 --> 01:35:22,220
It's just picking up on artifacts.

2094
01:35:22,220 --> 01:35:24,120
So it's hard to do, create a data set

2095
01:35:24,120 --> 01:35:26,400
which there's no artifacts,

2096
01:35:26,400 --> 01:35:28,200
and we have to take, you know, think about it,

2097
01:35:28,200 --> 01:35:30,160
like, yeah, solving the task or solving the artifact.

2098
01:35:30,160 --> 01:35:31,160
So it's important.

2099
01:35:32,160 --> 01:35:33,780
Okay, any questions?

2100
01:35:35,840 --> 01:35:36,680
Good.

2101
01:35:37,980 --> 01:35:42,980
So remember, we have vast amount of tasks

2102
01:35:43,140 --> 01:35:47,140
and data sets, data quality matters,

2103
01:35:47,140 --> 01:35:48,960
understanding the data annotators

2104
01:35:48,960 --> 01:35:51,740
and the task matters as well,

2105
01:35:51,740 --> 01:35:54,620
and we should be familiar

2106
01:35:54,620 --> 01:35:57,500
with the common evolution tasks metrics,

2107
01:35:57,500 --> 01:35:59,740
because then if you read some papers,

2108
01:35:59,740 --> 01:36:03,060
you know, all human level performance on SNLI,

2109
01:36:03,060 --> 01:36:04,300
you will know what it means,

2110
01:36:04,300 --> 01:36:06,680
and you should know what kind of,

2111
01:36:06,680 --> 01:36:08,840
who created data set, for what,

2112
01:36:09,720 --> 01:36:14,520
and what was the kind of bias in the data as well, maybe.

2113
01:36:14,520 --> 01:36:15,560
And getting better scores

2114
01:36:15,560 --> 01:36:16,800
is just the beginning of the story.

2115
01:36:16,800 --> 01:36:19,600
So getting better blue translation, yeah, nice.

2116
01:36:19,600 --> 01:36:22,800
Crunching numbers, cool, but does it do any better?

2117
01:36:22,800 --> 01:36:24,960
So take this into mind.

2118
01:36:24,960 --> 01:36:28,960
And evaluating text generation is an art.

2119
01:36:30,080 --> 01:36:31,780
Having said that, thank you very much.

2120
01:36:31,780 --> 01:36:32,880
I'll see you in a week.

2121
01:36:34,040 --> 01:36:35,640
Oh, sorry, okay, there is two questions.

2122
01:36:35,640 --> 01:36:38,020
Sorry, no, no, no, not yet, two questions.

2123
01:36:39,480 --> 01:36:41,960
The practice class is scheduled for right after this.

2124
01:36:41,960 --> 01:36:45,640
Oh, okay, so the practice class, nothing today, and more,

2125
01:36:45,640 --> 01:36:48,340
so this will be basically hybrid or online,

2126
01:36:48,340 --> 01:36:51,140
so there won't be like practice class in person, okay?

2127
01:36:52,360 --> 01:36:53,400
That was it?

2128
01:36:53,400 --> 01:36:54,400
That's it, thank you.

2129
01:36:54,400 --> 01:36:55,240
Thanks.

