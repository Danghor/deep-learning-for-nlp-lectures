\begin{frame}{Language Models (LMs)}
    \begin{itemize}
        \item<1-> language modeling is the task of assigning a probability to a sentence in a language
        \item<2-> what is the probability of seeing the sentence ``The cat sat on the mat.”
        \item<3-> ideal performance at language modeling is to predict the next token in a sequence with a number of guesses that is the identical to or lower than the number of guesses required by a human expert
        \item<4-> even without achieving human-level performance, language modeling is a crucial component in real-world NLP applications such as conversational AI, machine-translation, text summarization, ...
    \end{itemize}
\end{frame}
\begin{frame}{Language Models (LMs)}
\begin{itemize}
    \item<1-> assume a sequence of words $w_{1:n} = w_1 w_2 ... w_{n-1} w_n$
    \item<2-> $P(w_{1:n}) = P(w_1) P(w_2|w_1) P(w_3|w_{1:2})P(w_4|w_{1:3}) ... P(w_n|w_{1:n-1}) $
    \item<3-> each word is predicted conditioned on the preceding words
    \item<4-> estimating the probability of the last token needs to be conditioned on $n-1$ preceding words which is computationally expensive
    \item<5-> markov-assumption: the future is independent of the past given the present  
\end{itemize}
    
\end{frame}
\begin{frame}{Language Models (LMs)}
\begin{itemize}
    \item<1-> $k$th order markov-assumption assumes that the next word in a sequence depends only on the last $k$ words
    \begin{equation*}
        P(w_{i}|w_{1:i-1}) \approx P(w_i | w_{(i-1)-k:i-1})
    \end{equation*}
    \item<2-> probability of a sequence of tokens $w_{1:n}$
    \begin{equation*}
        P(w_{1:n}) \approx \prod_{i=1}^{n} P(w_i | w_{i-k:i-1})
    \end{equation*}
    \item<3-> to make it computationally-friendly for computers (What is the problem with the above format?)
    \begin{equation*}
        \text{log}_2P(w_{1:n}) \approx \sum_{i=1}^{n} \text{log}_2 \left(P(w_i | w_{i-k:i-1} )\right)
    \end{equation*}
\end{itemize}
\end{frame}
\begin{frame}{Evaluating LMs}
    \begin{itemize}
        \item<1-> the perplexity metric over an unseen sentence indicates how well a LM predicts the likelihood of the sentence
        \begin{equation*}
            \text{prep}_{w_{1:n}} (\text{LM}) = 2^{-\frac{1}{n} \sum_{i=1}^{n} \text{log}_2 \text{LM}({w_i|w_{1:i-1}})}
        \end{equation*}
        \item<2-> in this way, we can compare several language models with one another
        \item<3-> low perplexity values indicate a better language model as it assigns high probabilities to the unseen sentences 
        \item<4-> perplexities of two language models are only comparable with respect to the same evaluation dataset
    \end{itemize}
\end{frame}
\begin{frame}{Estimating Probabilities}
\begin{itemize}
    \item<1-> count-based: The estimates can be derived from corpus counts. 
    \item<2-> let $\#(w_{i:j})$ be the count of the sequence of words $w_{i:j}$ in a corpus
    \item<3-> the maximum likelihood estimate (MLE) of $P(w_{i}|w_{i-1-k:i-1})$
    \begin{equation*}
        P(w_{i}|w_{i-1-k:i-1}) = \frac{\#(w_{i-1-k:i})}{\#(w_{i-1-k:i-1})}
    \end{equation*}
    \item<4-> example: $w_1w_2w_3 =$ the cat sat
    \begin{equation*}
        P(w_{3}|w_{1:2}) = \frac{\#(\text{the cat sat})}{\#(\text{the cat})}
    \end{equation*}
\end{itemize}
\end{frame}
\begin{frame}{Estimating Probabilities}
 \begin{itemize}
     \item<1-> what if $\#(w_{i:j}) =0 $?  
     \item<2-> then the probability estimation is $0$, which translates to an infinite perplexity 
     \item<3-> one way of avoiding zero-probability N-grams is to use smoothing techniques
     \item<4-> additive smoothing: assume $|V|$ is  the vocabulary size and $0<\alpha \leq 1$
     \begin{equation*}
         P(w_{i}|w_{i-1-k:i-1}) = \frac{\#(w_{i-1-k:i}) + \alpha}{\#(w_{i-1-k:i-1}) + \alpha|V|}
     \end{equation*}
     
 \end{itemize}   
\end{frame}
\begin{frame}{Pro and Cons of Discussed LMs}
    \begin{itemize}
        \item<1-> easy to train, scale to large corpora, and work well in practice
        \item<2-> scaling to larger N-grams is a problem for MLE-based language models. 
        \item<3-> the large number of words in the vocabulary means that statistics for larger N-grams will be sparse
        \item<4-> MLE-based language models suffer from lack of generalization across contexts
        \item<5-> having observed ``black car'' and ``blue car'' does not influence our estimates of the sequence ``red car'' if we haven’t seen it before
    \end{itemize}
\end{frame}
\begin{frame}{Neural Language Models}
    \begin{itemize}
        \item<1-> we can use neural models to estimate probabilities for an LM
        \item<2-> in this way we can overcome the shortcomings of the MLE-based LMs because neural networks 
        \begin{itemize}
            \item<3-> they allow conditioning on increasingly large context sizes with only a linear increase in the number of parameters
            \item<4-> they support generalization across different contexts
        \end{itemize}
        \item<5-> we focus on the neural LM that was introduced by Bengio et al. (2003)
    \end{itemize}
\end{frame}
\begin{frame}{Neural Language Models}
    \begin{itemize}
        \item<1-> let $w_{1:k}$ be the given context
        \item<2-> we want to estimate $P(w_{k+1}|w_{1:k})$ 
        \item<3-> we design an MLP neural model, which takes $w_{1:k}$ as input and returns $P(w_{k+1})$ over all words in vocabulary $V$ as output
        \begin{equation*}
            x  = [v(w_1), v(w_2), ..., v(w_k)]
        \end{equation*}
        \begin{equation*}
            h^{(1)} = g(xW^{(1)}+b^{(1)})
        \end{equation*}
        \begin{equation*}
            P(w_{k+1}) = \text{softmax}(h^{(1)}W^{(2)}+b^{(2)})
        \end{equation*}
        \item<4-> the training examples are simply word k-grams from the training set, where the identities of the first $k-1$ words are used as features, and the last word is used as the target label for the classification
        \item<5-> loss function: cross-entropy loss
    \end{itemize}
\end{frame}
\begin{frame}{Neural LMs for Generating Language}
    \begin{itemize}
        \item<1-> assume that we are given $w_{1:k}$ as context
        \item<2-> we are asked to predict the next word $w_{k+1}$ from vocabulary $V$
        \item<3-> we feed $w_{1:k}$ to our trained MLP-based LM 
        \item<4-> our LM returns $P(w_{k+1})$ of each word in $V$ 
        \item<5-> we pick up the word with the maximum probability to generate the next word
        \item<6-> we add the the predicted word to the context and repeat the above procedure
    \end{itemize}
\end{frame}