\begin{frame}{Training as Optimization}
    \begin{itemize}
        \item<1-> we learned that the goal of training is to minimize a loss function (and a regularization term)
        \begin{equation*}
            \hat{\Theta} =\text{argmin}_{\Theta} \left( \mathcal{L}(\hat{y},y)  + \lambda R(\Theta) \right)
        \end{equation*}
        \item<2-> training = Solving an optimization problem
        \item<3-> how to find parameter values that minimize loss?
    \end{itemize}
\end{frame}
\begin{frame}{Gradient-based Optimization}
   
  \only<1-1>{
    \begin{figure}
        \centering
        \includegraphics[scale=0.2]{final/lecture03/figures/sgd1.png}
    \end{figure}
    }
    \only<2-2>{
        \begin{figure}
        \centering
        \includegraphics[scale=0.2]{final/lecture03/figures/sgd2.png}
    \end{figure}
    }
    \only<3-3>
    {
      \begin{figure}
        \centering
        \includegraphics[scale=0.2]{final/lecture03/figures/sgd2.png}
    \end{figure}      
    }
    \only<4-4>
    {
     \begin{figure}
        \centering
        \includegraphics[scale=0.2]{final/lecture03/figures/sgd4.png}
    \end{figure}
    }
\vspace*{\fill}
\textit{\tiny{(Taken from: 
\url{https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent})}}
   
\end{frame}
\begin{frame}{Gradient-based Optimization}
    \begin{itemize}
        \item<1-> we repeatedly compute an estimate of the loss over the training set
        \item<2-> we compute the gradients of the parameters with respect to the loss estimate
        \item<3-> we move the parameter values in the opposite directions of the gradient
    \end{itemize}
\end{frame}
\begin{frame}{(Online) Stochastic Gradient Descent}
\centering
\begin{figure}
    \includegraphics[scale=0.25]{final/lecture03/figures/sgd.png}
\end{figure}
\vspace*{\fill}
\textit{\tiny{(Taken from: Neural Network Methods for Natural Language Processing, Yoav Goldberg)}}
\end{frame}
\begin{frame}{\Large{(Minibatch) Stochastic Gradient Descent}}
\centering
\begin{figure}
    \includegraphics[scale=0.20]{final/lecture03/figures/msgd.png}
\end{figure}
\vspace*{\fill}
\textit{\tiny{(Taken from: Neural Network Methods for Natural Language Processing, Yoav Goldberg)}}
\end{frame}
\begin{frame}{Why Minibatch SGD?}
    \begin{itemize}
        \item<1-> it's not expensive: while computing loss function gradient on all training set can be computationally
expensive 
        \item<2-> it converges faster to a good solution than full-batch learning, in which we use all training set to compute gradient
        \item<3-> smaller mini-batch sizes lead often to better solutions (generalize better)
    \end{itemize}
\end{frame}
\begin{frame}{Stochastic Gradient Descent (SGD)}
    \begin{itemize}
        \item<1-> gradient descent does not always lead to best solutions
        \begin{figure}
            \includegraphics[scale=0.3]{final/lecture03/figures/sgd5.png}
        \end{figure}
        \item<2-> why? 
        \item<3-> SGD is sensitive to the learning rate and initial parameter values (starting point)
    \end{itemize}
\end{frame}
\begin{frame}{Small Learning Rate}
         \begin{figure}
        \centering
         \includegraphics[scale=0.4]{final/lecture03/figures/small_lr.png}
        \end{figure}
\vspace*{\fill}
\textit{\tiny{(Taken from: 
\url{https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent})}}     
\end{frame}
\begin{frame}{Large Learning Rate}
         \begin{figure}
        \centering
          \includegraphics[scale=0.4]{final/lecture03/figures/large_lr.png}
        \end{figure}
\vspace*{\fill}
\textit{\tiny{(Taken from: 
\url{https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent})}}
\end{frame}
\begin{frame}{Adaptive Learning Rate}
         \begin{figure}
        \centering
        \includegraphics[scale=0.40]{final/lecture03/figures/adaptive_lr.png}
        \end{figure}
\vspace*{\fill}
\textit{\tiny{(Taken from: 
\url{https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent})}}
\end{frame}
\begin{frame}{Stochastic Gradient Descent (SGD)}
    \begin{itemize}
        \item<1-> Use adaptive learning rate algorithms 
        \begin{itemize}
            \item<1->  AdaGrad [Duchi et al., 2011], 
            \item<1-> AdaDelta [Zeiler, 2012], 
            \item<1-> RMSProp [Tieleman and Hinton, 2012], 
            \item<1-> Adam [Kingma and Ba, 2014] 
        \end{itemize}
        \item<2-> these methods train usually faster than SGD
        \item<3-> found solution is often not as good as that by SGD $\rightarrow$ First train with Adam, fine-tune with SGD
        \item<4-> they use different initial parameter values in different runs of experiments and report the average of scores
    \end{itemize}
\end{frame}
\begin{frame}{Backpropagation}
\begin{itemize}
    \item<1-> a fancy name for a recursive algorithm that computes the derivatives of a nested functions using the chain rule, while caching intermediary derivatives 
    \item<2-> chain rule: Assume $y = f(g(x))$
    \begin{equation*}
        \frac{\partial y}{\partial x} = \frac{\partial f}{\partial g}\times \frac{\partial g}{\partial x}   = \frac{\partial f}{\partial g}\frac{\partial g}{\partial x}
    \end{equation*}
\end{itemize}
\end{frame}
\begin{frame}{Backpropagation}
\begin{itemize}
    \item<1-> consists of two steps
    \begin{itemize}
        \item<2-> forward pass $\rightarrow$ use current parameter values to compute the loss value
        \item<3-> backward pass $\rightarrow$ use the gradient of the loss to update the parameter values
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Model: Multilayer Perceptron (MLP) }
    \centering
    \scalebox{0.5}{
        \begin{tikzpicture}
        \uncover<1->{
        \node[] (x) at (0,0) {$x$};
        \node[draw, minimum height=2, minimum width=3] (l1) at (0,1) {$ f \left( x W^{(1)} \right) $};
        \draw[->, thick] (x) -- (l1);
        
         \node[] (h1) at (0,4) {$h^{(1)}$}; 
        \draw[->, thick] (l1) -- (h1);
        
        \node[draw, minimum height=2, minimum width=3] (l2) at (0,5) {$ f \left( h^{(1)} W^{(2)} \right) $};
        \draw[->, thick] (h1) -- (l2);
        
        
         \node[] (h2) at (0,8) {$h^{(2)}$}; 
        \draw[->, thick] (l2) -- (h2);
        
        \node[draw, minimum height=2, minimum width=3] (l3) at (0,9) {$ f \left( h^{(2)} W^{(3)} \right) $};
        \draw[->, thick] (h2) -- (l3);
        
        \node[] (y_hat) at (0,12) {$\hat{y}$}; 
        \draw[->, thick] (l3) -- (y_hat);
        
     \node[draw, white] (dummy-node) at (22,0) {};
    }
    \end{tikzpicture}
    }
\end{frame}
\begin{frame}{Backpropagation: Forward Pass}
    \centering
    \scalebox{0.5}{
        \begin{tikzpicture}
        \uncover<1->{
        \node[] (x) at (0,0) {x};
        \node[draw, minimum height=2, minimum width=3] (l1) at (0,1) {$ x W^{(1)} $};
        \draw[->, thick] (x) -- (l1);
        \node[] (z1) at (0,2) {$z^{(1)}$}; 
        \draw[->, thick] (l1) -- (z1);
        }
        \uncover<2->{
        \node[draw, minimum height=2, minimum width=3] (f1) at (0,3) {$ f\left( z^{(1)} \right)$};
        \draw[->, thick] (z1) -- (f1);
        \node[] (h1) at (0,4) {$h^{(1)}$}; 
        \draw[->, thick] (f1) -- (h1);
        }
        \uncover<3->{
        
        \node[draw, minimum height=2, minimum width=3] (l2) at (0,5) {$ h^{(1)} W^{(2)} $};
        \draw[->, thick] (h1) -- (l2);
        \node[] (z2) at (0,6) {$z^{(2)}$}; 
        \draw[->, thick] (l2) -- (z2);
        
        \node[draw, minimum height=2, minimum width=3] (f2) at (0,7) {$ f\left( z^{(2)} \right)$};
        \draw[->, thick] (z2) -- (f2);
        \node[] (h2) at (0,8) {$h^{(2)}$}; 
        \draw[->, thick] (f2) -- (h2);
   
        }
    \uncover<4->{
        
        \node[draw, minimum height=2, minimum width=3] (l3) at (0,9) {$ h^{(2)} W^{(3)} $};
        \draw[->, thick] (h2) -- (l3);
        \node[] (z3) at (0,10) {$z^{(3)}$}; 
        \draw[->, thick] (l3) -- (z3);
        
        \node[draw, minimum height=2, minimum width=3] (f3) at (0,11) {$ f\left( z^{(3)} \right)$};
        \draw[->, thick] (z3) -- (f3);
        \node[] (y_hat) at (0,12) {$\hat{y}$}; 
        \draw[->, thick] (f3) -- (y_hat);
        }
        
    \uncover<5->
        {
        \node[draw] (loss) at (0,13) {$\mathcal{L}(y,\hat{y})$};
        \draw[->, thick, dashed] (y_hat) -- (loss);
        \node[] (l) at (0,14) {$\ell$}; 
        \draw[->, thick] (loss) -- (l);
        }  

     \node[draw, white] (dummy-node) at (22,0) {};
    \end{tikzpicture}
    }
\end{frame}
\begin{frame}{Backpropagation: Backward Pass}
    \centering
    \scalebox{0.55}{
        \begin{tikzpicture}
    
        \node[] (x) at (0,0) {x};
        \node[draw, minimum height=2, minimum width=3] (l1) at (0,1) {$ x W^{(1)} $};
        \draw[->, thick] (x) -- (l1);
        \node[] (z1) at (0,2) {$z^{(1)}$}; 
        \draw[->, thick] (l1) -- (z1);
        \node[draw, minimum height=2, minimum width=3] (f1) at (0,3) {$ f\left( z^{(1)} \right)$};
        \draw[->, thick] (z1) -- (f1);
        \node[] (h1) at (0,4) {$h^{(1)}$}; 
        \draw[->, thick] (f1) -- (h1);
        
        \node[draw, minimum height=2, minimum width=3] (l2) at (0,5) {$ h^{(1)} W^{(2)} $};
        \draw[->, thick] (h1) -- (l2);
        \node[] (z2) at (0,6) {$z^{(2)}$}; 
        \draw[->, thick] (l2) -- (z2);
        
        \node[draw, minimum height=2, minimum width=3] (f2) at (0,7) {$ f\left( z^{(2)} \right)$};
        \draw[->, thick] (z2) -- (f2);
        \node[] (h2) at (0,8) {$h^{(2)}$}; 
        \draw[->, thick] (f2) -- (h2);

        \node[draw, minimum height=2, minimum width=3] (l3) at (0,9) {$ h^{(2)} W^{(3)} $};
        \draw[->, thick] (h2) -- (l3);
        \node[] (z3) at (0,10) {$z^{(3)}$}; 
        \draw[->, thick] (l3) -- (z3);
        
        \node[draw, minimum height=2, minimum width=3] (f3) at (0,11) {$ f\left( z^{(3)} \right)$};
        \draw[->, thick] (z3) -- (f3);
        \node[] (y_hat) at (0,12) {$\hat{y}$}; 
        \draw[->, thick] (f3) -- (y_hat);
        

        \node[draw] (loss) at (0,13) {$\mathcal{L}(y,\hat{y})$};
        \draw[->, thick, dashed] (y_hat) -- (loss);
        \node[] (l) at (0,14) {$\ell$}; 
        \draw[->, thick] (loss) -- (l);
         
        \uncover<1->
        {
        \node[draw] (dl-dy_hat) at (10,13) {$\frac{\partial \ell }{\partial \hat{y}} $};
        }
        \uncover<2->
        {
        \node[] (d1) at (10,12) {$d^{(1)}$};
        \draw[->, thick, dashed] (dl-dy_hat) -- (d1); 
        }
        \uncover<3-3>
        {
        \node[draw] (dl-dz3) at (10,11) {$\frac{\partial \ell }{\partial z^{(3)}}$};
        }
        
        \uncover<4-4>
        {
        \node[draw] (dl-dz3) at (10,11) {$\frac{\partial \ell }{\partial z^{(3)}} = 
        \frac{\partial \ell }{\partial \hat{y}} \frac{\partial \hat{y} }{\partial z^{(3)}}
        $};
        }
        \uncover<5->
        {
        
        \node[draw] (dl-dz3) at (10,11) {$\frac{\partial \ell }{\partial z^{(3)}} = 
        \frac{\partial \ell }{\partial \hat{y}} \frac{\partial \hat{y} }{\partial z^{(3)}} = d^{(1)}\frac{\partial \hat{y} }{\partial z^{(3)}}
        $};
        \draw[->, thick, dashed] (d1) -- (dl-dz3);
        }
        \uncover<6->
        {
        \node[] (d2) at (10,10) {$d^{(2)}$};
        \draw[->, thick, dashed] (dl-dz3) -- (d2); 
        }
        
        \uncover<7-7>
        {
        \node[draw] (dl-dh2) at (10,9) {$\frac{\partial \ell}{ \partial h^{(2)}} 
        $};
        }
        
        \uncover<8-8>
        {
        \node[draw] (dl-dh2) at (10,9) {$\frac{\partial \ell}{ \partial h^{(2)}} = \frac{\partial \ell}{ \partial z^{(3)}} \frac{\partial z^{(3)}}{ \partial h^{(2)}}
        $};
        }
        \uncover<9->
        {
       
        \node[draw] (dl-dh2) at (10,9) {$\frac{\partial \ell}{ \partial h^{(2)}} = \frac{\partial \ell}{ \partial z^{(3)}} \frac{\partial z^{(3)}}{ \partial h^{(2)} } = d^{(2)} w^{(3)}
        $};
         \draw[->, thick, dashed] (d2) -- (dl-dh2);
        }
        \uncover<10->
        {
        \node[] (d3) at (10,8) {$d^{(3)}$};
        \draw[->, thick, dashed] (dl-dh2) -- (d3); 
        }        
        
        
        
        \uncover<11-11>
        {
        \node[draw] (dl-dw3) at (15,9) {$\frac{\partial \ell}{ \partial w^{(3)}} 
        $};
        }
         \uncover<12-12>
        {
        \node[draw] (dl-dw3) at (15,9) {$\frac{\partial \ell}{ \partial w^{(3)}} = \frac{\partial \ell}{ \partial z^{(3)}} \frac{\partial z^{(3)}}{ \partial w^{(3)}}
        $};
        }
        \uncover<13-23>
        {
        \node[draw] (dl-dw3) at (15,9) {$\frac{\partial \ell}{ \partial w^{(3)}} = \frac{\partial \ell}{ \partial z^{(3)}} \frac{\partial z^{(3)}}{ \partial w^{(3)} } = d^{(2)} h^{(2)}
        $};
        }
       \uncover<14->
        {
       
        \node[draw] (dl-dz2) at (10,7) {$\frac{\partial \ell}{ \partial z^{(2)}} = \frac{\partial \ell}{ \partial h^{(2)}} \frac{\partial h^{(2)}}{ \partial z^{(2)} } = d^{(3)} \frac{\partial h^{(2)}}{ \partial z^{(2)} }
        $};
         \draw[->, thick, dashed] (d3) -- (dl-dz2);
        } 
        \uncover<15->
        {
        \node[] (d4) at (10,6) {$d^{(4)}$};
        \draw[->, thick, dashed] (dl-dz2) -- (d4); 
        } 
        
        \uncover<16->
        {
     
        \node[draw] (dl-dh1) at (10,5) {$\frac{\partial \ell}{ \partial h^{(1)}} = \frac{\partial \ell}{ \partial z^{(2)}} \frac{\partial z^{(2)}}{ \partial h^{(1)} } = d^{(4)} w^{(2)}
        $};
           \draw[->, thick, dashed] (d4) -- (dl-dh1);
        }
        \uncover<17-23>
        {
        \node[draw] (dl-dw2) at (15,5) {$\frac{\partial \ell}{ \partial w^{(2)}} = \frac{\partial \ell}{ \partial z^{(2)}} \frac{\partial z^{(2)}}{ \partial w^{(2)} } = d^{(4)} h^{(1)}
        $};
        }
        \uncover<18->
        {
        \node[] (d5) at (10,4) {$d^{(5)}$};
        \draw[->, thick, dashed] (dl-dh1) -- (d5); 
        }  
        
      \uncover<19->
        {
       
        \node[draw] (dl-dz1) at (10,3) {$\frac{\partial \ell}{ \partial z^{(1)}} = \frac{\partial \ell}{ \partial h^{(1)}} \frac{\partial h^{(1)}}{ \partial z^{(1)} } = d^{(5)} \frac{\partial h^{(1)}}{ \partial z^{(1)} }
        $};
         \draw[->, thick, dashed] (d5) -- (dl-dz1);
        } 
        \uncover<20->
        {
        \node[] (d6) at (10,2) {$d^{(6)}$};
        \draw[->, thick, dashed] (dl-dz1) -- (d6); 
        } 
        \uncover<21->
        {
         
        \node[draw] (dl-dx) at (10,1) {$\frac{\partial \ell}{ \partial x} = \frac{\partial \ell}{ \partial z^{(1)}} \frac{\partial z^{(1)}}{ \partial x } = d^{(6)} w^{(1)}
        $};
        \draw[->, thick, dashed] (d6) -- (dl-dx);
        }
        \uncover<22-23>
        {
       
        \node[draw] (dl-dw1) at (15,1) {$\frac{\partial \ell}{ \partial w^{(1)}} = \frac{\partial \ell}{ \partial z^{(1)}} \frac{\partial z^{(1)}}{ \partial w^{(1)} } = d^{(6)} x
        $};
        }
       \uncover<23->
       {
        \node[] (question) at (15,0) {\textcolor{myblue}{Which gradients will be used for SGD?}};
       }
        \uncover<24->
        {
        \node[draw,myblue] (dl-dw3) at (15,9) {$\frac{\partial \ell}{ \partial w^{(3)}} = \frac{\partial \ell}{ \partial z^{(3)}} \frac{\partial z^{(3)}}{ \partial w^{(3)} } = d^{(2)} h^{(2)}
        $};
        }
        \uncover<24->
        {
        \node[draw,myblue] (dl-dw2) at (15,5) {$\frac{\partial \ell}{ \partial w^{(2)}} = \frac{\partial \ell}{ \partial z^{(2)}} \frac{\partial z^{(2)}}{ \partial w^{(2)} } = d^{(4)} h^{(1)}
        $};
        }
        \uncover<24->
        {
       
        \node[draw,myblue] (dl-dw1) at (15,1) {$\frac{\partial \ell}{ \partial w^{(1)}} = \frac{\partial \ell}{ \partial z^{(1)}} \frac{\partial z^{(1)}}{ \partial w^{(1)} } = d^{(6)} x
        $};
        }
    \end{tikzpicture}
    }
\end{frame}

\begin{frame}{Backprop + SGD}
\begin{itemize}
    \item<1-> the output of backprop is gradient of parameters of a neural model
    \item<2-> once we have the gradients we can use SGD rule to update the parameter values
\end{itemize}
\end{frame}
\begin{frame}[fragile]{A Simple Training Loop in PyTorch}
\begin{lstlisting}[language=Python]
optimizer = SGD(model_params, lr)

for epoch in range(num_epochs):
    for x,y in  data_batches:
        
        y_hat = model(x) 
        loss = loss_func(y_hat, y)
        
        optimizer.zero_grad() 
        loss.backward()
        
        optimizer.step()
\end{lstlisting}
\end{frame}


%%%
% Following is what I used for the video
%%%
% \begin{frame}{Backpropagation: Forward Pass}
%     \centering
%     \begin{tikzpicture}
%         \uncover<1->{
%         \node[] (x) at (0,0) {x};
%         \node[draw, minimum height=2, minimum width=3] (l1) at (0,1) {$f \left( xW^{(1)} \right)$};
%         \draw[->, thick] (x) -- (l1);
%         \node[] (h1) at (0,2) {$h^{(1)}$}; 
%         \draw[->, thick] (l1) -- (h1);
%         }
%     \uncover<2->{
%         \node[draw, minimum height=2, minimum width=3] (l2) at (0,3) {$ f\left( h^{(1)}W^{(2)} \right)$};
%         \draw[->, thick] (h1) -- (l2);
%         \node[] (h2) at (0,4) {$h^{(2)}$}; 
%         \draw[->, thick] (l2) -- (h2);
%         }
%      \uncover<3->{
%         \node[draw, minimum height=2, minimum width=3] (l3) at (0,5) {$ f \left( h^{(2)}W^{(3)} \right)$};
%         \draw[->, thick] (h2) -- (l3);
%         \node[] (y_hat) at (0,6) {$\hat{y}$}; 
%         \draw[->, thick] (l3) -- (y_hat);
%         }
%         \uncover<4->
%         {
%         \node[] (comma) at (0,6.5) {$,$};
%         \node[] (y) at (0,7) {$y$}; 
%         \node[draw, dashed,thick, circle] (loss) at (5,7) {$L(y,\hat{y})$};
%         \draw[->, thick, dashed] (y) -- (loss);
%         \draw[->, thick, dashed] (y_hat) -- (loss);
%         }
        
%     \end{tikzpicture}
% \end{frame}
% \begin{frame}{Backpropagation: Backward Pass}
%     \centering
%     \begin{tikzpicture}
%         \uncover<1->{
%         \node[] (x) at (0,0) {x};
%         \node[draw, minimum height=2, minimum width=3] (l1) at (0,1) {$f \left( xW^{(1)} \right)$};
%         \draw[->, thick] (x) -- (l1);
%         \node[] (h1) at (0,2) {$h^{(1)}$}; 
%         \draw[->, thick] (l1) -- (h1);
%         }
%     \uncover<1->{
%         \node[draw, minimum height=2, minimum width=3] (l2) at (0,3) {$ f \left( h^{(1)}W^{(2)} \right)$};
%         \draw[->, thick] (h1) -- (l2);
%         \node[] (h2) at (0,4) {$h^{(2)}$}; 
%         \draw[->, thick] (l2) -- (h2);
%         }
%      \uncover<1->{
%         \node[draw, minimum height=2, minimum width=3] (l3) at (0,5) {$ f \left( h^{(2)}W^{(3)} \right)$};
%         \draw[->, thick] (h2) -- (l3);
%         \node[] (y_hat) at (0,6) {$\hat{y}$}; 
%         \draw[->, thick] (l3) -- (y_hat);
%         }
%         \uncover<1->
%         {
%         \node[] (comma) at (0,6.5) {$,$};
%         \node[] (y) at (0,7) {$y$}; 
%         \node[draw, dashed,thick, circle] (loss) at (5,7) {$L(y,\hat{y})$};
%         \draw[->, thick, dashed] (y) -- (loss);
%         \draw[->, thick, dashed] (y_hat) -- (loss);
%         }
%         \uncover<2->
%         {
        
%         \node[] (d3) at (5,5.75) {\textcolor{blue}{$d^{(3)} = \frac{\partial L}{ \partial \hat{y}} = \frac{\partial L}{ \partial f}$ }}; 
%         }
%         \uncover<3-3>
%         {
%         \node[] (d2_grad) at (5,5) 
%         {$
%          \frac{\partial L}{\partial W^{(3)}} = \frac{\partial L}{\partial f} \frac{\partial f}{\partial W^{(3)}}
%         $}; 
%         \draw[dashed, thick,->] (loss) -- (d2_grad);
%         }
        
%         \uncover<4->
%         {
%         \node[] (d3) at (5,5) 
%         {$
%          \frac{\partial L}{\partial W^{(3)}} = 
%         \frac{\partial L}{\partial f} \frac{\partial f}{\partial W^{(3)}} =
%         \frac{\partial L}{\partial f} h^{(2)} = d^{(3)}h^{(2)}
%         $}; 
%         \draw[dashed, thick,->] (loss) -- (d2_grad);
%         }
        
%         \uncover<5->
%         {
        
%         \node[] (d2) at (5,4.25) 
%         {
%       \textcolor{blue}{ $d^{(2)} = d^{(3)}W^{(3)}$}
%         };
%         }
%         \uncover<6-6>
%         {
%         \node[] (d2) at (5,3.5) 
%         { 
        
%         $
%         \frac{\partial L}{\partial W^{(2)}} = 
%         \frac{\partial L}{\partial f} \frac{\partial f}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial W^{(2)}} = 
%         $             

%         }; 
%       \draw[->, dashed, thick] (d3) -- (5,3.75);
%         }
%     \uncover<7->
%         {
        
%         \node[] (d2) at (5,3.5) 
%         { 
%         \begin{tabular}{l}
%         \\
%         \\
%         $
%          \frac{\partial L}{\partial W^{(2)}} = 
%         \frac{\partial L}{\partial f} \frac{\partial f}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial W^{(2)}} = 
%         $             
%         \\
%         $ 
%         \frac{\partial L}{\partial f} W^{(3)} \frac{\partial h^{(2)}}{\partial W^{(2)}} = 
%         \frac{\partial L}{\partial f} W^{(3)} h^{(1)} = d^{(2)}h^{(1)}
%         $
   
%         \end{tabular}
%         }; 
%       \draw[->, dashed, thick] (d3) -- (5,3.75);
        
%         }
        
%     \uncover<8->
%     {
%     \node[] (d2) at (5,2) 
%         {
%       \textcolor{blue}{ $d^{(1)} = d^{(2)}W^{(2)}$}
%         };
%     }
%     \uncover<9->
%         {
        
%         \node[] (d1) at (5.5,1) 
%         { 
%         $
%          \frac{\partial L}{\partial W^{(1)}} = 
%         \frac{\partial L}{\partial f} \frac{\partial f}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial h^{(1)}}
%         \frac{\partial h^{(1)}}{\partial W^{(1)}}
%         = 
%         \frac{\partial L}{\partial f} W^{(3)}  W^{(2)} x = d^{(1)} x 
%         $
 
%         }; 
%          \draw[->, dashed, thick] (5,2.5) -- (5,1.5);
%         }
%     \end{tikzpicture}
% \end{frame}

