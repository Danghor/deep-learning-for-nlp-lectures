\begin{frame}{Feature Vectors}
    \begin{itemize}
        \item<1-> the output of a learning algorithm is a $f()$
        \item<2-> $f$ takes as input a vector $x$ with dimension $d_{in}$
        \item<3-> $f$ returns as output a vector $\hat{y}$ with dimension $d_{out}$
        \item<4-> how can we map textual features to a vector?
    \end{itemize}
\end{frame}
\begin{frame}{Different Types of Features}
    \begin{itemize}
        \item<1-> numerical features 
        \begin{itemize}
            \item<1-> the value of a feature is a number 
            \item<2-> e.g., the frequency of a word in a text
        \end{itemize}
        \item<3-> categorical features
        \begin{itemize}
            \item<3-> the value of a feature is from a set of values
            \item<4-> what is the POS of a word? 
        \end{itemize}
        
        \item<5-> how to encode categorical features?
        \begin{itemize}
            \item<6-> one-hot encodings
            \item<7-> dense embeddings
        \end{itemize}
        
    \end{itemize}
\end{frame}
\begin{frame}{One-hot Encodings}  
    \begin{itemize}
        \item<1-> let's assume that values of a feature is in categories $\{ v_0, v_1, v_2 \}$
        \item<2-> we encode the space of feature values via vectors in which 
            \begin{itemize}
                \item<3-> each entry is either $0$ or $1$
                \item<4-> each entry is associated with one of the categories
                \item<5-> only one item in a vector can be $1$, the rest should be $0$
            \end{itemize}
        \item<6-> for example:
        \begin{itemize}
            \item<6-> $v_0 = [1, 0, 0]$
            \item<6-> $v_1 = [0, 1, 0]$
            \item<6-> $v_2 = [0, 0, 1]$
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}{Example: Categorical Labels}
    \begin{itemize}
        \item<1-> let $f()$ take a vector $x$ which encodes a text and also return a vector $\hat{y}$ representing the topic of the text which can be from $\{$news, sport, science, politic$\}$
        \item<2-> how can we represent topic labels using one-hot encoding? 
        \begin{itemize}
            \item<3-> news = $[1,0,0,0]$
            \item<4-> sport = $[0,1,0,0]$
            \item<5-> science = $[0,0,1,0]$
            \item<6-> politic = $[0,0,0,1]$
        \end{itemize}
        \item<7-> what is the length of one-hot vectors for a feature with $k$ categories?  
        
    \end{itemize}
\end{frame}
\begin{frame}{Example: Word Encodings}
    \begin{itemize}
        \item<1-> let $V$ be vocabulary of a language 
        \item<2-> we take $V$ as categories a word can take in a text
        \item<3-> so words can be encoded by one-hot vectors
        \item<4-> let $V= \{v_0, v_1, v_2,...,v_{|V|-1} \}$
        \begin{itemize}
            \item<5-> $v_0 = [1,0,0,..., 0]$
            \item<6-> $v_1 = [0,1,0,..., 0]$
            \item<7-> $v_2 = [0,0,1,..., 0]$
            \item<7-> $...$
            \item<7-> $v_{|V|-1} = [0,0,0,..., 1]$
        \end{itemize}
        \item<8-> we can easily represent a text via BOW and then represent each word in BOW with its one-hot vector
        \item<9-> how can we define vocabulary $V$ given a corpus $D$?
    \end{itemize}
\end{frame}

\begin{frame}{Comment}
    \centering
    \textcolor{myblue}{\Large{\textbf{pause}}}
\end{frame}


\begin{frame}{One-hot Encodings}
\begin{itemize}
    \item<1-> word vectors are very sparse 
    \item<2-> semantic relations between words are not encoded in word vectors
    \item<3-> it's better to use one-hot representations for a few distinct features where we expect no correlation between features
\end{itemize}\end{frame}

\begin{frame}{Dense Encodings}
    \begin{itemize}
        \item<1-> a categorical feature is embedded as a vector in a $d$ dimensional space
        \item<2-> assuming categories $C = \{ c_0,c_1,..., c_{|C|-1} \}$, $d=4$
        \begin{itemize}
            \item<3-> $c_0 = [+0.1,-0.2,+0.3,+0.5]$
            \item<4-> $c_1 = [-0.2-0.1,+0.1,+0.2]$
            \item<4-> ...
            \item<4-> $c_{|C|-1} = [+0.2,-0.2,-0.1,+0.3]$
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}{Example: Word Encodings}
    \begin{itemize}
        \item<1-> assuming vocabulary $V = \{ v_0,v_1,..., v_{|V|-1} \}$, $d=4$
        \begin{itemize}
            \item<2-> $v_0 = [+0.1,-0.2,+0.3,+0.5]$
            \item<2-> $v_1 = [-0.2-0.1,+0.1,+0.2]$
            \item<2-> ...
            \item<2-> $v_{|V|-1} = [+0.2,-0.2,-0.1,+0.3]$
        \end{itemize}
        \item<3-> in the one-hot method we represent each word with a sparse vector of size $|V|$
        \item<4-> in dense encoding method we represent each word with a dense vector with a small size $d$
    \end{itemize}
\end{frame}
\begin{frame}{Dense Encodings}
    \begin{itemize}
        \item<1-> dimensionality of vectors is $d$
        \item<2-> model training will cause similar features to have similar vectors
        \item<3-> it's mainly useful when we expect some correlations between features $\rightarrow$ ``cat'' and ``dog'' are semantically related
        \item<4-> is also useful when we have a large number of features $\rightarrow$ for example vocabulary
    \end{itemize}
\end{frame}