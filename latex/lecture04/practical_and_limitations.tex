 \begin{frame}{Pre-Trained Embeddings}
\begin{itemize}
    \item Word2Vec
    \begin{itemize}
        \item trained on Google News (100 billion tokens)
    \end{itemize}
    \item GloVe
        \begin{itemize}
            \item trained on Wikipedia (6 billion tokens)
            \item trained on CommonCrawl (42 and 840 billion tokens)
            \item trained on Twitter (27 million tokens)
        \end{itemize}
    \item many other pre-trained embeddings for different languages 
        \begin{itemize}
            \item \url{https://fasttext.cc/docs/en/crawl-vectors.html}
        \end{itemize}
\end{itemize}
\end{frame}
 \begin{frame}{Practical Hints}
     \begin{itemize}
         \item<1-> always try out different word embeddings (consider them as a hyperparameter)
         \item<2-> results may vary drastically with different embeddings
         \item<3-> consider source of corpora used to train word embeddings (larger is not always better, a smaller but more domain-focused can be more effective)
         \item<4-> consider what contexts  were used to define similarities 
         \item<5-> it's better to use the same tokenization and text normalization methods that  were used for creating word embeddings
     \end{itemize}
 \end{frame}

 \begin{frame}{Embedding Layers in PyTorch}
     \begin{itemize}
         \item<1-> an embedding layer (a.k.a lookup table) maps a sequence of word IDs to a sequence of embedding vectors
         \item<2-> it uses a lookup table, which is a matrix with size $|V|\times d_{emb}$
         \item<3-> the $i$'th row of this matrix contains embeddings of $i$'th word in vocabulary $V$
         \item<4-> so the only thing we need to do is to replace a word with its index in vocabulary $\rightarrow$ a dictionary does this easily (word\_to\_ix)
     \end{itemize}
     
 \end{frame}

 \begin{frame}[fragile]{Embedding Layers in PyTorch}
    \tiny
     \begin{lstlisting}[language=Python]
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    
    torch.manual_seed(1)
    
    word_to_ix = {"hello": 0, "world": 1}
    
    embeds = nn.Embedding(2, 5) 

    lookup_tensor = torch.tensor([word_to_ix["hello"]], dtype=torch.long)
    
    hello_embed = embeds(lookup_tensor)
    
    print(hello_embed)
    
    \end{lstlisting}

 \end{frame}
 
 \begin{frame}[fragile]{Embedding Layers in PyTorch}
  \tiny
     \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(EmbeddingLayer, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
    
    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        return embeds
\end{lstlisting}        

 \end{frame}
 
 \begin{frame}[fragile]{Embedding Layers in PyTorch}
  \tiny
     \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(EmbeddingLayer, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
    
    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        return embeds

if __name__ == '__main__':
    model = EmbeddingLayer(voc_size=10, emb_size=3)
    inputs = [1,5]
    inputs_tenseor = torch.tensor(inputs, dtype=torch.long)
    
    emb_vectors = model(inputs_tenseor)
    
    print(f"the shape of emb vectors is {emb_vectors.shape}")            
\end{lstlisting}
 \end{frame}

 \begin{frame}[fragile]{Using Pretrained Embedding Layers  \\ in PyTorch}
  \tiny
     \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, weights_matrix):
        super(EmbeddingLayer, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        self.embeddings.load_state_dict({'weight': weights_matrix})
    
     
    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        return embeds
\end{lstlisting}
 \end{frame}
 
 \begin{frame}[fragile]{Freezing Embedding Layers \\ in PyTorch}
 
  \tiny
     \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, weights_matrix, freeze=False):
        super(EmbeddingLayer, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        self.embeddings.load_state_dict({'weight': weights_matrix})
        
        
        if freeze:
            self.embeddings.weight.requires_grad = False
        
        
    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        return embeds
\end{lstlisting}
 \end{frame}
 
 \begin{frame}{Limitations}
 \begin{itemize}
     \item<1-> the algorithms discussed provide very little control over the kind of similarity they include
     \begin{itemize}
         \item<2-> ``cat'' is more similar to ``dog'' than to ``tiger'' as they both are pets
         \item<3-> ``cat'' is more similar to ``tiger'' than to ``dog'' as they both as felines
     \end{itemize}
     \item<4-> many of the trivial properties of words are ignored because people are less likely to mention known information than they are to mention novel one
     \begin{itemize}
         \item<5-> when people talk about ``white sheep'', they will likely prefer to use only ``sheep''
         \item<6-> for ``black sheep'', they are very likely to use color information ``black sheep''
         \item<7-> a model trained on texts only might easily misled by this
     \end{itemize}
 \end{itemize}
 \end{frame}
 \begin{frame}{Limitations}\begin{itemize}
     \item<1-> text corpora can easily be biased for better or worse $\rightarrow$  so word embeddings can become biased too
     \begin{itemize}
         \item<2-> gender and racial biases are very common
     \end{itemize}
     \item<3-> these word vectors are context independent
     \begin{itemize}
         \item<4-> in reality, there is no such a thing to have context independent word meaning
         \item<5-> some words have multiple sense e.g., ``bank'' may refer to a financial institution or to the side of a river
     \end{itemize}
 \end{itemize}\end{frame}
 